 1/1: print(z)
 1/2:
x = 5
y = 6
z = 5 + 6
 1/3: print(z)
 3/1: runfile('/Users/josephfrasca/.spyder-py3/temp.py', wdir='/Users/josephfrasca/.spyder-py3')
 4/1: print("Hello Anaconda")
 8/1: print('Hello World')
 8/2: print('Hello World')
 8/3: print('Hello World')
 8/4: print('Hello World')
 8/5: 'Hello World'
 8/6: name = 'John'
 8/7: name
 8/8: name = 'Corey'
 8/9: name
8/10: 'Hello World'
8/11: name = 'John'
8/12: name
8/13: name = 'Corey'
8/14: !pip list
8/15: %ls magic
8/16: %lsmagic
8/17: %pwd
8/18: %pwd
8/19: %ls
8/20: %ls -la
 9/1: properties.head()
 9/2: properties.info()
 9/3: properties.describe()
 9/4: properties
 9/5: properties
 9/6:
# Let's import the pandas, numpy libraries as pd, and np respectively. 
import pandas as pd
import numpy as np

# Load the pyplot collection of functions from matplotlib, as plt 
import matplotlib.pyplot as plt
 9/7:
# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:
# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls

url_LondonHousePrices = "https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls"

# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. 
# As a result, we need to specify the sheet name in the read_excel() method.
# Put this data into a variable called properties.  
properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)
 9/8: properties
 9/9: properties.head()
9/10: properties.info()
9/11: properties.describe
9/12: properties.describe()
9/13: properties.describe
9/14: properties.describe()
9/15: properties.describe
9/16: properties.describe()
9/17: properties.index()
9/18: properties.transponse()
9/19: properties.transpose()
9/20: properties.transpose()
9/21: properties.index
9/22: properties.index()
9/23: properties.index.describe()
9/24: properties.index.info()
9/25: properties.index.info
9/26: properties.index
9/27: properties.head()
9/28: properties_t.head()
9/29: properties_t = properties.transpose()
9/30: properties_t = properties.transpose()
9/31: properties_t.head()
9/32: properties_t.head()
9/33: properties_t.columns = properties_t.iloc[0]
9/34: properties_t.columns = properties_t.iloc[0]
9/35: properties_t.head()
9/36: properties_t.head()
9/37: properties_t.index()
9/38: properties_t.index
9/39: properties_t.head
9/40: list(my_dataframe.columns.values)
9/41: properties_t.head()
9/42: list(properties_t.columns.values)
9/43: properties_t.columns.values.tolist()
9/44: properties_t.columns
9/45: properties_t.columns = properties_t.iloc[0, inplace=True]
9/46: properties_t.columns = properties_t.iloc[0]
9/47: properties_t.columns = properties_t.iloc[0]
9/48: properties_t.head()
9/49: properties_t.index.name = Borough
9/50: properties_t.columns = properties_t.iloc[0]
9/51: properties_t.head()
9/52: properties_t.index.name = Borough
9/53: properties_t.index.name = 'Borough'
9/54: properties_t.index.name = 'Borough'
9/55: properties_t.head()
9/56:
properties_t.index.name = 'Borough'
properties_t.columns.name = 'Year-Month'
9/57: properties_t.head()
9/58:
properties.head()
properties.info()
9/59:
properties.head()
properties.info()
properties.describe()
9/60:
properties.head()
properties.info()
properties.describe()
9/61:
properties.head()
properties.info()
properties.describe()
9/62: [properties.head()]
9/63: properties.head()
9/64:
properties_t = properties.transpose()
properties_t.head()
9/65:
properties_t.columns = properties_t.iloc[0]
properties_t.head()
9/66:
properties_t = properties.transpose()
properties_t.index
properties_t.head()
9/67:
properties_t = properties.transpose()
properties_t.index
9/68:
properties_t = properties.transpose()
properties_t.index
properties_t.head()
9/69:
properties_t = properties.transpose()
properties_t.index
9/70: properties_t.head()
9/71:
properties_t.drop('Unamed: 0')
properties_t.head()
9/72:
properties_t.drop('Unnamed: 0')
properties_t.head()
9/73:
properties_t.drop('Unnamed: 0')
properties_t.head()
9/74: properties_t.drop('Unnamed: 0')
9/75: properties_t.head()
9/76:
properties_t.columns = properties_t.iloc[0]
properties_t.head()
9/77: properties_t.drop('Unnamed: 0')
9/78: properties_t.drop('Unnamed: 0')
9/79:
properties_t.index.name = 'Borough'
properties_t.columns.name = 'Year-Month'
9/80: properties_t.head()
9/81: properties_t = properties_t.drop('Unnamed: 0')
9/82: properties_t = properties_t.drop('Unnamed: 0')
9/83: properties_t.drop('Unnamed: 0')
9/84: properties_t.drop('Unnamed: 0')
9/85: properties_t.drop('Unnamed: 0')
9/86: properties_t.drop(['Unnamed: 0'])
9/87:
properties_t.columns = properties_t.iloc[0]
properties_t.head()
9/88: properties_t.drop(['Unnamed: 0'])
9/89: properties_t.drop('Unnamed: 0')
9/90: properties_t.drop(properties_t.index[0])
9/91: properties_t.columns = properties_t.iloc[0]
9/92: properties_t.columns = properties_t.iloc[0]
9/93:
properties_t.columns = properties_t.iloc[0]
properties_t.columns.head()
9/94:
properties_t.columns = properties_t.iloc[0]
properties_t.head()
10/1:
properties.head()
properties.info()
properties.describe()
10/2:
# Let's import the pandas, numpy libraries as pd, and np respectively. 
import pandas as pd
import numpy as np

# Load the pyplot collection of functions from matplotlib, as plt 
import matplotlib.pyplot as plt
10/3:
# Let's import the pandas, numpy libraries as pd, and np respectively. 
import pandas as pd
import numpy as np

# Load the pyplot collection of functions from matplotlib, as plt 
import matplotlib.pyplot as plt
10/4:
# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:
# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls

url_LondonHousePrices = "https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls"

# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. 
# As a result, we need to specify the sheet name in the read_excel() method.
# Put this data into a variable called properties.  
properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)
10/5:
properties.head()
properties.info()
properties.describe()
10/6: properties.head()
10/7:
properties_t = properties.transpose()
properties_t.index
10/8: properties_t.head()
10/9:
properties_t.columns = properties_t.iloc[0]
properties_t.head()
10/10: properties_t.drop('Unnamed: 0')
10/11:
properties_t.index.name = 'Borough'
properties_t.columns.name = 'Year-Month'
10/12: properties_t.head()
10/13: properties_td = properties_t.drop('Unnamed: 0')
10/14:
properties_td.index.name = 'Borough'
properties_td.columns.name = 'Year-Month'
10/15: properties_td.head()
10/16: properties_td
10/17:
properties_t.drop('Unnamed: 0')
properties_td = properties_t.drop('1995-01-01:1999-12-01')
10/18:
properties_t.drop('Unnamed: 0')
properties_td = properties_t.drop('1995-01-01:1999-12-01', axis=1)
10/19:
properties_t.drop('Unnamed: 0')
properties_td = properties_t.drop('1995-01-01', '1999-12-01', axis=1)
10/20:
properties_t.drop('Unnamed: 0')
properties_td = properties_t.drop('1995-01-01', '1999-12-01', axis='columns')
10/21:
properties_t.drop('Unnamed: 0')
properties_td = properties_t.drop(['1995-01-01', '1999-12-01'], axis=1)
10/22:
properties_t.drop('Unnamed: 0')
properties_td = properties_t.drop(['1995-01-01', '1999-12-01'], axis='columns')
10/23:
properties_t.drop('Unnamed: 0')
properties_td = properties_t.drop(['1995-01-01', '1999-12-01'], 1)
10/24:
properties_t.drop('Unnamed: 0')
columns = ['1995-01-01':'1999-12-01']
properties_td = properties_t.drop(columns, axis=1, inplace=True)
10/25:
properties_t.drop('Unnamed: 0')
older_years = properties_t.loc[:, '1995-01-01':'1999-12-01']
properties_td = properties_t.drop(older_years, axis=1, inplace=True)
10/26:
properties_t.drop('Unnamed: 0')
older_years = properties_t.loc[:, '1995-01-01':'1999-12-01']
properties_td = properties_t.drop(older_years, axis=1, inplace=True)
10/27: properties_td.head()
10/28: properties_td.head()
10/29: properties_t.head()
10/30:
properties_t.drop('Unnamed: 0')
older_years = properties_t.loc[:, '1995-01-01':'1999-12-01']
10/31: properties_t.head()
10/32:
properties_t.drop('Unnamed: 0')
older_years = properties_t.loc[:, '1995-01-01':'2000-03-01']
10/33:
properties_t.drop('Unnamed: 0')
older_years = properties_t.loc[:, '1995-01-01':'2000-03-01']
10/34: properties_t.head()
10/35:
properties_t.drop('Unnamed: 0')
older_years = properties_t.loc[:, '1995-01-01':'2000-03-01']
10/36: properties_t.head()
10/37: older_years.head()
10/38: properties_t.head()
10/39: print(older_years)
10/40: properties_t.drop(older_years, axis=1, inplace=True)
10/41: properties_t.head()
10/42:
properties_t.drop('Unnamed: 0')
older_years = properties_t.loc[:, '1995-01-01':'2000-02-01']
10/43: properties_t.drop(older_years, axis=1, inplace=True)
10/44: properties_t.head()
10/45:
properties_t.drop('Unnamed: 0')
older_years = properties_t.loc[:, '1995-01-01':'2000-02-01']
10/46: properties_t.drop(older_years, axis=1, inplace=True)
10/47: properties_t.head()
10/48: properties_t.drop(older_years, axis=1, inplace=True)
10/49: properties_t.head()
10/50: properties_td.head()
10/51: print(properties_td)
10/52: print(properties_t)
10/53: properties_t.head()
10/54: properties_td.drop(older_years, axis=1, inplace=True)
10/55:
properties_td = properties_t.drop('Unnamed: 0')
properties_td.head()
older_years = properties_td.loc[:, '1995-01-01':'2000-02-01']
10/56:
properties_t.drop('Unnamed: 0')
properties_t.head()
properties_td.head()
older_years = properties_td.loc[:, '1995-01-01':'2000-02-01']
10/57:
properties_t.drop('Unnamed: 0')
properties_t.head()
properties_td.head()
older_years = properties_td.loc[:, '1995-01-01':'2000-02-01']
10/58:
properties_t.drop('Unnamed: 0')
properties_t.head()
10/59:
properties_t.drop(index[0])
properties_t.head()
10/60:
properties_t.drop('Unnamed: 0')
properties_t.head()
10/61:
properties_t.drop('Unnamed: 0')
properties_t.head()
10/62: properties.shape
10/63: properties.shape()
10/64: properties.shape
10/65: properties.head()
12/1: properties.shape
12/2:
# Let's import the pandas, numpy libraries as pd, and np respectively. 
import pandas as pd
import numpy as np

# Load the pyplot collection of functions from matplotlib, as plt 
import matplotlib.pyplot as plt
12/3:
# Let's import the pandas, numpy libraries as pd, and np respectively. 
import pandas as pd
import numpy as np

# Load the pyplot collection of functions from matplotlib, as plt 
import matplotlib.pyplot as plt
12/4:
# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:
# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls

url_LondonHousePrices = "https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls"

# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. 
# As a result, we need to specify the sheet name in the read_excel() method.
# Put this data into a variable called properties.  
properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)
12/5: properties.shape
12/6: properties.head()
12/7: properties_t = properties.transpose()
12/8: properties_t.head()
12/9: properties_t.head()
12/10: properties_t.index
12/11: properties_t.index
12/12: properties_t = properties_t.reset.index()
12/13: properties_t = properties_t.reset_index()
12/14: properties_t = properties_t.reset_index()
12/15: properties_t.index
12/16: properties_t.head()
12/17: properties_t.index()
12/18: properties_t.index
13/1: properties_t.index
13/2: properties_t.index
13/3:
# Let's import the pandas, numpy libraries as pd, and np respectively. 
import pandas as pd
import numpy as np

# Load the pyplot collection of functions from matplotlib, as plt 
import matplotlib.pyplot as plt
13/4:
# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:
# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls

url_LondonHousePrices = "https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls"

# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. 
# As a result, we need to specify the sheet name in the read_excel() method.
# Put this data into a variable called properties.  
properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)
13/5: properties.shape
13/6: properties.head()
13/7: properties_t = properties.transpose()
13/8: properties_t.head()
13/9: properties_t.index
13/10: properties_t = properties_t.reset_index()
13/11: properties_t.index
13/12: properties_t.index
13/13:
properties_t.index
properties_t.head()
13/14: properties_t.index
13/15: properties_t.index
13/16: properties_t.index
13/17: properties_t.head()
13/18: properties_t.head()
13/19: properties_T.columns
13/20: properties_t.columns
13/21: properties_t.iloc[[0]]
13/22: properties_t.iloc[[0]]
13/23: properties_t.columns = properties_t.iloc[0]
13/24: properties_t.columns = properties_t.iloc[0]
13/25: properties_t.head()
13/26: properties_t = properties_t.drop(0)
13/27: properties_t.head()
13/28: properties_t = properties_t.rename(columns = {'Unnamed: 0':'London_Borough', pd.NaT: 'ID'})
13/29: properties_t.head()
13/30: properties_t.columns
13/31: clean_properties = pd.melt(properties_t, id_vars=['Borough', 'ID'])
13/32: clean_properties = pd.melt(properties_t, id_vars=['London_Borough', 'ID'])
13/33: clean_properties = pd.melt(properties_t, id_vars=['London_Borough', 'ID'])
13/34: clean_properties.head()
13/35: clean_properties.shape
13/36: clean_properties.info
13/37: clean_properties.info()
13/38: clean_properties.head()
13/39: clean_properties.head()
13/40: clean_properties = clean_properties.rename(columns = {0: 'Month', 'value': 'Average_price'})
13/41: clean_properties = clean_properties.rename(columns = {0: 'Month', 'value': 'Average_price'})
13/42: clean_properties.head()
13/43: clean_properties.dtypes
15/1: clean_properties.dtypes
15/2:
# Let's import the pandas, numpy libraries as pd, and np respectively. 
import pandas as pd
import numpy as np

# Load the pyplot collection of functions from matplotlib, as plt 
import matplotlib.pyplot as plt
15/3:
# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:
# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls

url_LondonHousePrices = "https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls"

# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. 
# As a result, we need to specify the sheet name in the read_excel() method.
# Put this data into a variable called properties.  
properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)
15/4: properties.shape
15/5: properties.head()
15/6: properties_t = properties.transpose()
15/7: properties_t.head()
15/8: properties_t.index
15/9: properties_t = properties_t.reset_index()
15/10: properties_t.index
15/11: properties_t.head()
15/12: properties_t.columns
15/13: properties_t.iloc[[0]]
15/14: properties_t.columns = properties_t.iloc[0]
15/15: properties_t.head()
15/16: properties_t = properties_t.drop(0)
15/17: properties_t.head()
15/18: properties_t = properties_t.rename(columns = {'Unnamed: 0':'London_Borough', pd.NaT: 'ID'})
15/19: properties_t.head()
15/20: properties_t.columns
15/21: clean_properties = pd.melt(properties_t, id_vars=['London_Borough', 'ID'])
15/22: clean_properties.head()
15/23: clean_properties = clean_properties.rename(columns = {0: 'Month', 'value': 'Average_price'})
15/24: clean_properties.head()
15/25: clean_properties.dtypes
15/26: type(clean_properties['Average_price'])
15/27: clean_properties['Average_price'].dtypes
15/28: type(clean_properties['London_Borough'])
15/29: clean_properties['Average_price'] = pd.to_numeric(clean_properties['Average_price'])
15/30: clean_properties.dtypes
15/31: clean_properties.info()
15/32: clean_properties['London_Borough'.info()
15/33: clean_properties['London_Borough'].info()
15/34: clean_properties['Average_price'].info()
15/35: clean_properties.loc['London_Borough']
15/36: clean_properties.loc[:, 'London_Borough']
15/37: print(clean_properties.loc[:, 'London_Borough'])
15/38: clean_properties
15/39:
with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also
    print(clean_properties)
15/40: clean_properties.describe()
15/41: clean_properties.info()
15/42: clean_properties.info()
15/43: clean_properties = clean_properties.dropna(how='any',axis=0)
15/44: clean_properties = clean_properties.dropna(how='any',axis=0)
15/45: clean_properties.info()
15/46: clean_properties.last()
15/47: clean_properties.tail()
15/48: clean_properties.info()
15/49: clean_properties.iloc[4].plot
15/50: clean_properties.iloc[4].plot
15/51:
clean_properties.iloc[4].plot
plt.show()
15/52:
clean_properties.iloc[4].plot
plt.show()
15/53:
clean_properties.iloc[4].plot()
plt.show()
15/54:
clean_properties.plot()
plt.show()
15/55:
clean_properties['Month'].plot(y='Average_price')
plt.show()
15/56:
clean_properties['Month'].plot(y='Average_price').mean()
plt.show()
15/57:
clean_properties['Month'].plot
plt.show()
15/58:
clean_properties['Month'].plot
plt.show()
15/59:
clean_properties['Month'].plot
plt.show()
15/60:
clean_properties['Month'].plot(y='Average_price')
plt.show()
15/61: clean_properties['London_Borough'].unique()
15/62: clean_properties['London_Borough'].unique().count()
15/63: clean_properties['London_Borough'].unique()
15/64: clean_properties['London_Borough'].unique()
15/65: clean_properties['London_Borough'].unique()
15/66: clean_properties.dtypes
15/67: clean_properties.count()
15/68:
# Let's import the pandas, numpy libraries as pd, and np respectively. 
import pandas as pd
import numpy as np

# Load the pyplot collection of functions from matplotlib, as plt 
import matplotlib.pyplot as plt
15/69:
# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:
# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls

url_LondonHousePrices = "https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls"

# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. 
# As a result, we need to specify the sheet name in the read_excel() method.
# Put this data into a variable called properties.  
properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)
15/70: properties.shape
15/71: properties.head()
15/72: properties_t = properties.transpose()
15/73: properties_t.head()
15/74: properties_t.index
15/75: properties_t = properties_t.reset_index()
15/76: properties_t.index
15/77: properties_t.head()
15/78: properties_t.columns
15/79: properties_t.iloc[[0]]
15/80: properties_t.columns = properties_t.iloc[0]
15/81: properties_t.head()
15/82: properties_t = properties_t.drop(0)
15/83: properties_t.head()
15/84: properties_t = properties_t.rename(columns = {'Unnamed: 0':'London_Borough', pd.NaT: 'ID'})
15/85: properties_t.head()
15/86: properties_t.columns
15/87: clean_properties = pd.melt(properties_t, id_vars=['London_Borough', 'ID'])
15/88: clean_properties.head()
15/89: clean_properties = clean_properties.rename(columns = {0: 'Month', 'value': 'Average_price'})
15/90: clean_properties.head()
15/91: clean_properties.dtypes
15/92: clean_properties['Average_price'] = pd.to_numeric(clean_properties['Average_price'])
15/93: clean_properties.dtypes
15/94: clean_properties.count()
15/95: clean_properties['London_Borough'].unique()
15/96: clean_properties[loc['London_Borough'] == 'Unnamed:34'].head()
15/97: clean_properties[pd.loc['London_Borough'] == 'Unnamed:34'].head()
15/98: clean_properties[clean_properties.loc['London_Borough'] == 'Unnamed:34'].head()
15/99: clean_properties[clean_properties['London_Borough'] == 'Unnamed:34'].head()
15/100: clean_properties[clean_properties['London_Borough'] == 'Unnamed:34']
15/101: clean_properties[clean_properties['London_Borough'] == 'Unnamed:34'].head()
15/102: clean_properties[clean_properties['London_Borough'] == 'Unnamed:34'].head()
15/103: clean_properties[clean_properties['London_Borough'] == 'Unnamed:37'].head()
15/104: clean_properties[clean_properties['London_Borough'] == 'Unnamed:37'].head()
15/105: clean_properties[clean_properties['ID'].isna()]
15/106: clean_properties[clean_properties['ID'].isna()]
15/107: NaNFreeDF1 clean_properties[clean_properties['Average_price'].notna()]
15/108: NaNFreeDF1 = clean_properties[clean_properties['Average_price'].notna()]
15/109: NaNFreeDF1.count()
15/110: NaNFreeDF1.count()
15/111: NaNFreeDF2 = clean_properties.dropna()
15/112: NaNFreeDF2.head(48)
15/113: NaNFreeDF2.head(48)
15/114: NaNFreeDF2.count()
15/115: NaNFreeDF2.count()
15/116: NaNFreeDF2['London_Borough'].unique()
15/117:
print(clean_properties.shape)
print(NaNFreeDF1.shape)
print(NaNFreeDF2.shape)
15/118:
nonBoroughs = ['Inner London', 'Outer London', 
               'NORTH EAST', 'NORTH WEST', 'YORKS & THE HUMBER', 
               'EAST MIDLANDS', 'WEST MIDLANDS',
              'EAST OF ENGLAND', 'LONDON', 'SOUTH EAST', 
              'SOUTH WEST', 'England']
15/119: NaNFreeDF2[NaNFreeDF2.London_Borough.isin(nonBoroughs)]
15/120: NaNFreeDF2[~NaNFreeDF2.London_Borough.isin(nonBoroughs)]
15/121: NanFreeDF2 = NaNFreeDF2[~NaNFreeDF2.London_Borough.isin(nonBoroughs)]
15/122: NanFreeDF2.head()
15/123: NanFreeDF2.head()
15/124: df = NaNFreeDF2
15/125: df.head()
15/126: df.types
15/127: df.dtypes
15/128: camden_prices = df[df['London_Borough'] == 'Camden']
15/129: ax = camden_prices.plot(kind ='line', x = 'Month', y='Average_price')
15/130:
camden_prices = df[df['London_Borough'] == 'Camden']
ax = camden_prices.plot(kind ='line', x = 'Month', y='Average_price')
ax.set_ylabel('Price')
15/131:
df['Year'] = df['Month'].apply(lambda t: t.year)
df.tail()
15/132:
df['Year'] = df['Month'].apply(lambda t: t.year)
df.tail()
15/133: df.plot(kind ='line', x = 'Year', y='Average_price')
15/134: df.plot(kind ='line', x = 'Year', y='Average_price', subplots=True)
15/135: dfg = df.groupby(by=['Borough', 'Year']).mean()
15/136: dfg = df.groupby(by=['London_Borough', 'Year']).mean()
15/137:
dfg = df.groupby(by=['London_Borough', 'Year']).mean()
dfg.sample(10)
15/138:
dfg = df.groupby(by=['London_Borough', 'Year']).mean()
dfg.sample(10)
15/139:
dfg = df.groupby(by=['London_Borough', 'Year']).mean()
dfg.sample(10)
15/140:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head()
15/141:
def create_price_ratio(b):
    df_bfilter = df.filter(df[df['London_Borough']==b])
    price_00 = df_bfilter['Average_price']['2000']
    price_20 = df_bfilter['Average_price']['2020']
    price_ratio = price_00/price_20
    return price_ratio
15/142:
def create_price_ratio(b):
    df_bfilter = df.filter(df[df['London_Borough']==b])
    price_00 = df_bfilter['Average_price']['2000']
    price_20 = df_bfilter['Average_price']['2020']
    price_ratio = price_00/price_20
    return price_ratio
15/143: create_price_ratio['Camden']
15/144: create_price_ratio(df['Camden'])
15/145: create_price_ratio('Camden')
15/146: create_price_ratio('Camden')
15/147:
def create_price_ratio(b):
    df_bfilter = df.filter(df[df['London_Borough']==b])
    price_00 = df_bfilter['Average_price']['Year'['2000']]
    price_20 = df_bfilter['Average_price']['Year'['2020']]
    price_ratio = price_00/price_20
    return price_ratio
15/148: create_price_ratio('Camden')
15/149: create_price_ratio(1)
15/150:
def create_price_ratio(d):
    df_bfilter = df.filter(df[df['London_Borough']==b])
    price_00 = float(d['Average_price'][d['Year']==2000])
    price_20 = float(d['Average_price'][d['Year']==2020])
    price_ratio = price_00/price_20
    return price_ratio
15/151: create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham'])
15/152:
def create_price_ratio(d):
    df_bfilter = df.filter(df[df['London_Borough']==d])
    price_00 = float(d['Average_price'][d['Year']==2000])
    price_20 = float(d['Average_price'][d['Year']==2020])
    price_ratio = price_00/price_20
    return price_ratio
15/153: create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham'])
15/154: print(create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham']))
15/155: print(create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham']))
15/156: print(create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham']))
15/157: print(create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham']))
15/158: create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham'])
15/159: create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham'])
15/160: create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham'])
15/161: print(create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham']))
15/162: print(create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham']))
15/163: create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham'])
15/164:
# We want to do this for all of the London Boroughs. 
# First, let's make an empty dictionary, called final, where we'll store our ratios for each unique London_Borough.
final = {}
15/165:
# We want to do this for all of the London Boroughs. 
# First, let's make an empty dictionary, called final, where we'll store our ratios for each unique London_Borough.
final = {}
15/166:
# Now let's declare a for loop that will iterate through each of the unique elements of the 'London_Borough' column of our DataFrame dfg.
# Call the iterator variable 'b'. 
for b in dfg['London_Borough'].unique():
    # Let's make our parameter to our create_price_ratio function: i.e., we subset dfg on 'London_Borough' == b. 
    borough = dfg[dfg['London_Borough'] == b]
    # Make a new entry in the final dictionary whose value's the result of calling create_price_ratio with the argument: borough
    final[b] = create_price_ratio(borough)
# We use the function and incorporate that into a new key of the dictionary 
print(final)
15/167:
# Make a variable called df_ratios, and assign it the result of calling the DataFrame method on the dictionary final. 
df_ratios = pd.DataFrame(final)
15/168:
# All we need to do now is transpose it, and reset the index! 
df_ratios_T = transpose.T
df_ratios = df_ratios_T.reset_index()
df_ratios.head()
15/169:
# All we need to do now is transpose it, and reset the index! 
df_ratios_T = pd.transpose.T
df_ratios = df_ratios_T.reset_index()
df_ratios.head()
15/170:
# Call the head() method on this variable to check it out. 
df_ratios.head()
15/171:
# Make a variable called df_ratios, and assign it the result of calling the DataFrame method on the dictionary final. 
df_ratios = pd.DataFrame(final)
15/172:
# Let's import the pandas, numpy libraries as pd, and np respectively. 
import pandas as pd
import numpy as np

# Load the pyplot collection of functions from matplotlib, as plt 
import matplotlib.pyplot as plt
15/173:
# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:
# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls

url_LondonHousePrices = "https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls"

# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. 
# As a result, we need to specify the sheet name in the read_excel() method.
# Put this data into a variable called properties.  
properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)
15/174: properties.shape
15/175: properties.head()
15/176: properties_t = properties.transpose()
15/177: properties_t.head()
15/178: properties_t.index
15/179: properties_t = properties_t.reset_index()
15/180: properties_t.index
15/181: properties_t.head()
15/182: properties_t.columns
15/183: properties_t.iloc[[0]]
15/184: properties_t.columns = properties_t.iloc[0]
15/185: properties_t.head()
15/186: properties_t = properties_t.drop(0)
15/187: properties_t.head()
15/188: properties_t = properties_t.rename(columns = {'Unnamed: 0':'London_Borough', pd.NaT: 'ID'})
15/189: properties_t.head()
15/190: properties_t.columns
15/191: clean_properties = pd.melt(properties_t, id_vars=['London_Borough', 'ID'])
15/192: clean_properties.head()
15/193: clean_properties = clean_properties.rename(columns = {0: 'Month', 'value': 'Average_price'})
15/194: clean_properties.head()
15/195: clean_properties.dtypes
15/196: clean_properties['Average_price'] = pd.to_numeric(clean_properties['Average_price'])
15/197: clean_properties.dtypes
15/198: clean_properties.count()
15/199: clean_properties['London_Borough'].unique()
15/200: clean_properties[clean_properties['London_Borough'] == 'Unnamed:34'].head()
15/201: clean_properties[clean_properties['London_Borough'] == 'Unnamed:37'].head()
15/202: clean_properties[clean_properties['ID'].isna()]
15/203: NaNFreeDF1 = clean_properties[clean_properties['Average_price'].notna()]
15/204: NaNFreeDF1.count()
15/205: NaNFreeDF2 = clean_properties.dropna()
15/206: NaNFreeDF2.head(48)
15/207: NaNFreeDF2.count()
15/208: NaNFreeDF2['London_Borough'].unique()
15/209:
print(clean_properties.shape)
print(NaNFreeDF1.shape)
print(NaNFreeDF2.shape)
15/210:
nonBoroughs = ['Inner London', 'Outer London', 
               'NORTH EAST', 'NORTH WEST', 'YORKS & THE HUMBER', 
               'EAST MIDLANDS', 'WEST MIDLANDS',
              'EAST OF ENGLAND', 'LONDON', 'SOUTH EAST', 
              'SOUTH WEST', 'England']
15/211: NanFreeDF2 = NaNFreeDF2[~NaNFreeDF2.London_Borough.isin(nonBoroughs)]
15/212: NanFreeDF2.head()
15/213: df = NaNFreeDF2
15/214: df.head()
15/215: df.dtypes
15/216:
camden_prices = df[df['London_Borough'] == 'Camden']
ax = camden_prices.plot(kind ='line', x = 'Month', y='Average_price')
ax.set_ylabel('Price')
15/217:
df['Year'] = df['Month'].apply(lambda t: t.year)
df.tail()
15/218:
dfg = df.groupby(by=['London_Borough', 'Year']).mean()
dfg.sample(10)
15/219:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head()
15/220:
def create_price_ratio(d):
    price_00 = float(d['Average_price'][d['Year']==1998])
    price_20 = float(d['Average_price'][d['Year']==2018])
    ratio = [price_00/price_20]
    return ratio
15/221: create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham'])
15/222:
# We want to do this for all of the London Boroughs. 
# First, let's make an empty dictionary, called final, where we'll store our ratios for each unique London_Borough.
final = {}
15/223:
# Now let's declare a for loop that will iterate through each of the unique elements of the 'London_Borough' column of our DataFrame dfg.
# Call the iterator variable 'b'. 
for b in dfg['London_Borough'].unique():
    # Let's make our parameter to our create_price_ratio function: i.e., we subset dfg on 'London_Borough' == b. 
    borough = dfg[dfg['London_Borough'] == b]
    # Make a new entry in the final dictionary whose value's the result of calling create_price_ratio with the argument: borough
    final[b] = create_price_ratio(borough)
# We use the function and incorporate that into a new key of the dictionary 
print(final)
15/224:
# Make a variable called df_ratios, and assign it the result of calling the DataFrame method on the dictionary final. 
df_ratios = pd.DataFrame(final)
15/225:
# Call the head() method on this variable to check it out. 
df_ratios.head()
15/226:
# All we need to do now is transpose it, and reset the index! 
df_ratios_T = df_ratios.T
df_ratios = df_ratios_T.reset_index()
df_ratios.head()
15/227:
# Let's just rename the 'index' column as 'London_Borough', and the '0' column to '2018'.
df_ratios.index(columns={'index':'Borough', 0:'2018'}, inplace=True)
df_ratios.head()
15/228:
# Let's just rename the 'index' column as 'London_Borough', and the '0' column to '2018'.
df_ratios.index(columns={'index':'London_Borough', 0:'2018'}, inplace=True)
df_ratios.head()
15/229:
# Let's just rename the 'index' column as 'London_Borough', and the '0' column to '2018'.
df_ratios.rename(columns={'index':'London_Borough', 0:'2018'}, inplace=True)
df_ratios.head()
15/230:
# Let's just rename the 'index' column as 'London_Borough', and the '0' column to '2018'.
df_ratios.rename(columns={'index':'London_Borough', 0:'2018'}, inplace=True)
df_ratios.head()
15/231:
# Let's sort in descending order and select the top 15 boroughs.
# Make a variable called top15, and assign it the result of calling sort_values() on df_ratios. 
top15 = df_ratios.sort_values(by='2018',ascending=False).head(15)
print(top15)
15/232:
# Let's sort in descending order and select the top 15 boroughs.
# Make a variable called top15, and assign it the result of calling sort_values() on df_ratios. 
top15 = df_ratios.sort_values(by='2018',ascending=False).head(15)
print(top15)
15/233:
# Let's plot the boroughs that have seen the greatest changes in price.
# Make a variable called ax. Assign it the result of filtering top15 on 'Borough' and '2018', then calling plot(), with
# the parameter kind = 'bar'. 
ax = top15[['Borough','2018']].plot(kind='bar')

ax.set_xticklabels(top15.Borough)
15/234:
# Let's plot the boroughs that have seen the greatest changes in price.
# Make a variable called ax. Assign it the result of filtering top15 on 'Borough' and '2018', then calling plot(), with
# the parameter kind = 'bar'. 
ax = top15[['London_Borough','2018']].plot(kind='bar')

ax.set_xticklabels(top15.Borough)
15/235:
# Let's plot the boroughs that have seen the greatest changes in price.
# Make a variable called ax. Assign it the result of filtering top15 on 'Borough' and '2018', then calling plot(), with
# the parameter kind = 'bar'. 
ax = top15[['London_Borough','2018']].plot(kind='bar')

ax.set_xticklabels(top15.London_Borough)
15/236: ##It seems that the borough with the
15/237:
# Let's sort in descending order and select the top 15 boroughs.
# Make a variable called top15, and assign it the result of calling sort_values() on df_ratios. 
top15 = df_ratios.sort_values(by='2018',ascending=False).head(15)
print(top15)
15/238: df_ratios.sort_values(by='2018',ascending=False)
15/239:
def create_price_ratio(d):
    y1998 = float(d['Average_price'][d['Year']==1998])
    y2018 = float(d['Average_price'][d['Year']==2018])
    ratio = [y1998/y2018]
    return ratio
15/240:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head(20)
15/241:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head(32)
15/242:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head(25)
15/243:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head(25)
15/244:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head()
15/245:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head()
15/246:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head()
15/247:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head()
17/1:
# Let's import the pandas, numpy libraries as pd, and np respectively. 
import pandas as pd
import numpy as np

# Load the pyplot collection of functions from matplotlib, as plt 
import matplotlib.pyplot as plt
17/2:
# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:
# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls

url_LondonHousePrices = "https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls"

# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. 
# As a result, we need to specify the sheet name in the read_excel() method.
# Put this data into a variable called properties.  
properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)
17/3: properties.shape
17/4: properties.head()
17/5: properties_t = properties.transpose()
17/6: properties_t.head()
17/7: properties_t.index
17/8: properties_t = properties_t.reset_index()
17/9: properties_t.index
17/10: properties_t.head()
17/11: properties_t.columns
17/12: properties_t.iloc[[0]]
17/13: properties_t.columns = properties_t.iloc[0]
17/14: properties_t.head()
17/15: properties_t = properties_t.drop(0)
17/16: properties_t.head()
17/17: properties_t = properties_t.rename(columns = {'Unnamed: 0':'London_Borough', pd.NaT: 'ID'})
17/18: properties_t.head()
17/19: properties_t.columns
17/20: clean_properties = pd.melt(properties_t, id_vars=['London_Borough', 'ID'])
17/21: clean_properties.head()
17/22: clean_properties = clean_properties.rename(columns = {0: 'Month', 'value': 'Average_price'})
17/23: clean_properties.head()
17/24: clean_properties.dtypes
17/25: clean_properties['Average_price'] = pd.to_numeric(clean_properties['Average_price'])
17/26: clean_properties.dtypes
17/27: clean_properties.count()
17/28: clean_properties['London_Borough'].unique()
17/29: clean_properties[clean_properties['London_Borough'] == 'Unnamed:34'].head()
17/30: clean_properties[clean_properties['London_Borough'] == 'Unnamed:37'].head()
17/31: clean_properties[clean_properties['ID'].isna()]
17/32: NaNFreeDF1 = clean_properties[clean_properties['Average_price'].notna()]
17/33: NaNFreeDF1.count()
17/34: NaNFreeDF2 = clean_properties.dropna()
17/35: NaNFreeDF2.head(48)
17/36: NaNFreeDF2.count()
17/37: NaNFreeDF2['London_Borough'].unique()
17/38:
print(clean_properties.shape)
print(NaNFreeDF1.shape)
print(NaNFreeDF2.shape)
17/39:
nonBoroughs = ['Inner London', 'Outer London', 
               'NORTH EAST', 'NORTH WEST', 'YORKS & THE HUMBER', 
               'EAST MIDLANDS', 'WEST MIDLANDS',
              'EAST OF ENGLAND', 'LONDON', 'SOUTH EAST', 
              'SOUTH WEST', 'England']
17/40: NanFreeDF2 = NaNFreeDF2[~NaNFreeDF2.London_Borough.isin(nonBoroughs)]
17/41: NanFreeDF2.head()
17/42: df = NaNFreeDF2
17/43: df.head()
17/44: df.dtypes
17/45:
camden_prices = df[df['London_Borough'] == 'Camden']
ax = camden_prices.plot(kind ='line', x = 'Month', y='Average_price')
ax.set_ylabel('Price')
17/46:
df['Year'] = df['Month'].apply(lambda t: t.year)
df.tail()
17/47:
dfg = df.groupby(by=['London_Borough', 'Year']).mean()
dfg.sample(10)
17/48:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head()
17/49:
def create_price_ratio(d):
    y1998 = float(d['Average_price'][d['Year']==1998])
    y2018 = float(d['Average_price'][d['Year']==2018])
    ratio = [y1998/y2018]
    return ratio
17/50: create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham'])
17/51:
# We want to do this for all of the London Boroughs. 
# First, let's make an empty dictionary, called final, where we'll store our ratios for each unique London_Borough.
final = {}
17/52:
# Now let's declare a for loop that will iterate through each of the unique elements of the 'London_Borough' column of our DataFrame dfg.
# Call the iterator variable 'b'. 
for b in dfg['London_Borough'].unique():
    # Let's make our parameter to our create_price_ratio function: i.e., we subset dfg on 'London_Borough' == b. 
    borough = dfg[dfg['London_Borough'] == b]
    # Make a new entry in the final dictionary whose value's the result of calling create_price_ratio with the argument: borough
    final[b] = create_price_ratio(borough)
# We use the function and incorporate that into a new key of the dictionary 
print(final)
17/53:
# Make a variable called df_ratios, and assign it the result of calling the DataFrame method on the dictionary final. 
df_ratios = pd.DataFrame(final)
17/54:
# Call the head() method on this variable to check it out. 
df_ratios.head()
17/55:
# All we need to do now is transpose it, and reset the index! 
df_ratios_T = df_ratios.T
df_ratios = df_ratios_T.reset_index()
df_ratios.head()
17/56:
# Let's just rename the 'index' column as 'London_Borough', and the '0' column to '2018'.
df_ratios.rename(columns={'index':'London_Borough', 0:'2018'}, inplace=True)
df_ratios.head()
17/57:
# Let's sort in descending order and select the top 15 boroughs.
# Make a variable called top15, and assign it the result of calling sort_values() on df_ratios. 
top15 = df_ratios.sort_values(by='2018',ascending=False).head(15)
print(top15)
17/58: df_ratios.sort_values(by='2018',ascending=False)
17/59:
# Let's plot the boroughs that have seen the greatest changes in price.
# Make a variable called ax. Assign it the result of filtering top15 on 'Borough' and '2018', then calling plot(), with
# the parameter kind = 'bar'. 
ax = top15[['London_Borough','2018']].plot(kind='bar')

ax.set_xticklabels(top15.London_Borough)
17/60: #It seems that the borough with the highest growth from 1998-2018 was North East
17/61:
def create_price_ratio(d):
    y1998 = float(d['Average_price'][d['Year']==1998])
    y2018 = float(d['Average_price'][d['Year']==2018])
    ratio = [y2018/y1998]
    return ratio
17/62:
# Let's import the pandas, numpy libraries as pd, and np respectively. 
import pandas as pd
import numpy as np

# Load the pyplot collection of functions from matplotlib, as plt 
import matplotlib.pyplot as plt
17/63:
# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:
# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls

url_LondonHousePrices = "https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls"

# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. 
# As a result, we need to specify the sheet name in the read_excel() method.
# Put this data into a variable called properties.  
properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)
17/64: properties.shape
17/65: properties.head()
17/66: properties_t = properties.transpose()
17/67: properties_t.head()
17/68: properties_t.index
17/69: properties_t = properties_t.reset_index()
17/70: properties_t.index
17/71: properties_t.head()
17/72: properties_t.columns
17/73: properties_t.iloc[[0]]
17/74: properties_t.columns = properties_t.iloc[0]
17/75: properties_t.head()
17/76: properties_t = properties_t.drop(0)
17/77: properties_t.head()
17/78: properties_t = properties_t.rename(columns = {'Unnamed: 0':'London_Borough', pd.NaT: 'ID'})
17/79: properties_t.head()
17/80: properties_t.columns
17/81: clean_properties = pd.melt(properties_t, id_vars=['London_Borough', 'ID'])
17/82: clean_properties.head()
17/83: clean_properties = clean_properties.rename(columns = {0: 'Month', 'value': 'Average_price'})
17/84: clean_properties.head()
17/85: clean_properties.dtypes
17/86: clean_properties['Average_price'] = pd.to_numeric(clean_properties['Average_price'])
17/87: clean_properties.dtypes
17/88: clean_properties.count()
17/89: clean_properties['London_Borough'].unique()
17/90: clean_properties[clean_properties['London_Borough'] == 'Unnamed:34'].head()
17/91: clean_properties[clean_properties['London_Borough'] == 'Unnamed:37'].head()
17/92: clean_properties[clean_properties['ID'].isna()]
17/93: NaNFreeDF1 = clean_properties[clean_properties['Average_price'].notna()]
17/94: NaNFreeDF1.count()
17/95: NaNFreeDF2 = clean_properties.dropna()
17/96: NaNFreeDF2.head(48)
17/97: NaNFreeDF2.count()
17/98: NaNFreeDF2['London_Borough'].unique()
17/99:
print(clean_properties.shape)
print(NaNFreeDF1.shape)
print(NaNFreeDF2.shape)
17/100:
nonBoroughs = ['Inner London', 'Outer London', 
               'NORTH EAST', 'NORTH WEST', 'YORKS & THE HUMBER', 
               'EAST MIDLANDS', 'WEST MIDLANDS',
              'EAST OF ENGLAND', 'LONDON', 'SOUTH EAST', 
              'SOUTH WEST', 'England']
17/101: NanFreeDF2 = NaNFreeDF2[~NaNFreeDF2.London_Borough.isin(nonBoroughs)]
17/102: NanFreeDF2.head()
17/103: df = NaNFreeDF2
17/104: df.head()
17/105: df.dtypes
17/106:
camden_prices = df[df['London_Borough'] == 'Camden']
ax = camden_prices.plot(kind ='line', x = 'Month', y='Average_price')
ax.set_ylabel('Price')
17/107:
df['Year'] = df['Month'].apply(lambda t: t.year)
df.tail()
17/108:
dfg = df.groupby(by=['London_Borough', 'Year']).mean()
dfg.sample(10)
17/109:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head()
17/110:
def create_price_ratio(d):
    y1998 = float(d['Average_price'][d['Year']==1998])
    y2018 = float(d['Average_price'][d['Year']==2018])
    ratio = [y2018/y1998]
    return ratio
17/111: create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham'])
17/112:
# We want to do this for all of the London Boroughs. 
# First, let's make an empty dictionary, called final, where we'll store our ratios for each unique London_Borough.
final = {}
17/113:
# Now let's declare a for loop that will iterate through each of the unique elements of the 'London_Borough' column of our DataFrame dfg.
# Call the iterator variable 'b'. 
for b in dfg['London_Borough'].unique():
    # Let's make our parameter to our create_price_ratio function: i.e., we subset dfg on 'London_Borough' == b. 
    borough = dfg[dfg['London_Borough'] == b]
    # Make a new entry in the final dictionary whose value's the result of calling create_price_ratio with the argument: borough
    final[b] = create_price_ratio(borough)
# We use the function and incorporate that into a new key of the dictionary 
print(final)
17/114:
# Make a variable called df_ratios, and assign it the result of calling the DataFrame method on the dictionary final. 
df_ratios = pd.DataFrame(final)
17/115:
# Call the head() method on this variable to check it out. 
df_ratios.head()
17/116:
# All we need to do now is transpose it, and reset the index! 
df_ratios_T = df_ratios.T
df_ratios = df_ratios_T.reset_index()
df_ratios.head()
17/117:
# Let's just rename the 'index' column as 'London_Borough', and the '0' column to '2018'.
df_ratios.rename(columns={'index':'London_Borough', 0:'2018'}, inplace=True)
df_ratios.head()
17/118:
# Let's sort in descending order and select the top 15 boroughs.
# Make a variable called top15, and assign it the result of calling sort_values() on df_ratios. 
top15 = df_ratios.sort_values(by='2018',ascending=False).head(15)
print(top15)
17/119: df_ratios.sort_values(by='2018',ascending=False)
17/120:
# Let's plot the boroughs that have seen the greatest changes in price.
# Make a variable called ax. Assign it the result of filtering top15 on 'Borough' and '2018', then calling plot(), with
# the parameter kind = 'bar'. 
ax = top15[['London_Borough','2018']].plot(kind='bar')

ax.set_xticklabels(top15.London_Borough)
17/121:
def create_price_ratio(d):
    y1998 = float(d['Average_price'][d['Year']==1998])
    y2018 = float(d['Average_price'][d['Year']==2018])
    ratio = [y1998/y2018]
    return ratio
17/122:
# Let's import the pandas, numpy libraries as pd, and np respectively. 
import pandas as pd
import numpy as np

# Load the pyplot collection of functions from matplotlib, as plt 
import matplotlib.pyplot as plt
17/123:
# First, make a variable called url_LondonHousePrices, and assign it the following link, enclosed in quotation-marks as a string:
# https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls

url_LondonHousePrices = "https://data.london.gov.uk/download/uk-house-price-index/70ac0766-8902-4eb5-aab5-01951aaed773/UK%20House%20price%20index.xls"

# The dataset we're interested in contains the Average prices of the houses, and is actually on a particular sheet of the Excel file. 
# As a result, we need to specify the sheet name in the read_excel() method.
# Put this data into a variable called properties.  
properties = pd.read_excel(url_LondonHousePrices, sheet_name='Average price', index_col= None)
17/124: properties.shape
17/125: properties.head()
17/126: properties_t = properties.transpose()
17/127: properties_t.head()
17/128: properties_t.index
17/129: properties_t = properties_t.reset_index()
17/130: properties_t.index
17/131: properties_t.head()
17/132: properties_t.columns
17/133: properties_t.iloc[[0]]
17/134: properties_t.columns = properties_t.iloc[0]
17/135: properties_t.head()
17/136: properties_t = properties_t.drop(0)
17/137: properties_t.head()
17/138: properties_t = properties_t.rename(columns = {'Unnamed: 0':'London_Borough', pd.NaT: 'ID'})
17/139: properties_t.head()
17/140: properties_t.columns
17/141: clean_properties = pd.melt(properties_t, id_vars=['London_Borough', 'ID'])
17/142: clean_properties.head()
17/143: clean_properties = clean_properties.rename(columns = {0: 'Month', 'value': 'Average_price'})
17/144: clean_properties.head()
17/145: clean_properties.dtypes
17/146: clean_properties['Average_price'] = pd.to_numeric(clean_properties['Average_price'])
17/147: clean_properties.dtypes
17/148: clean_properties.count()
17/149: clean_properties['London_Borough'].unique()
17/150: clean_properties[clean_properties['London_Borough'] == 'Unnamed:34'].head()
17/151: clean_properties[clean_properties['London_Borough'] == 'Unnamed:37'].head()
17/152: clean_properties[clean_properties['ID'].isna()]
17/153: NaNFreeDF1 = clean_properties[clean_properties['Average_price'].notna()]
17/154: NaNFreeDF1.count()
17/155: NaNFreeDF2 = clean_properties.dropna()
17/156: NaNFreeDF2.head(48)
17/157: NaNFreeDF2.count()
17/158: NaNFreeDF2['London_Borough'].unique()
17/159:
print(clean_properties.shape)
print(NaNFreeDF1.shape)
print(NaNFreeDF2.shape)
17/160:
nonBoroughs = ['Inner London', 'Outer London', 
               'NORTH EAST', 'NORTH WEST', 'YORKS & THE HUMBER', 
               'EAST MIDLANDS', 'WEST MIDLANDS',
              'EAST OF ENGLAND', 'LONDON', 'SOUTH EAST', 
              'SOUTH WEST', 'England']
17/161: NanFreeDF2 = NaNFreeDF2[~NaNFreeDF2.London_Borough.isin(nonBoroughs)]
17/162: NanFreeDF2.head()
17/163: df = NaNFreeDF2
17/164: df.head()
17/165: df.dtypes
17/166:
camden_prices = df[df['London_Borough'] == 'Camden']
ax = camden_prices.plot(kind ='line', x = 'Month', y='Average_price')
ax.set_ylabel('Price')
17/167:
df['Year'] = df['Month'].apply(lambda t: t.year)
df.tail()
17/168:
dfg = df.groupby(by=['London_Borough', 'Year']).mean()
dfg.sample(10)
17/169:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head()
17/170:
def create_price_ratio(d):
    y1998 = float(d['Average_price'][d['Year']==1998])
    y2018 = float(d['Average_price'][d['Year']==2018])
    ratio = [y1998/y2018]
    return ratio
17/171: create_price_ratio(dfg[dfg['London_Borough']=='Barking & Dagenham'])
17/172:
# We want to do this for all of the London Boroughs. 
# First, let's make an empty dictionary, called final, where we'll store our ratios for each unique London_Borough.
final = {}
17/173:
# Now let's declare a for loop that will iterate through each of the unique elements of the 'London_Borough' column of our DataFrame dfg.
# Call the iterator variable 'b'. 
for b in dfg['London_Borough'].unique():
    # Let's make our parameter to our create_price_ratio function: i.e., we subset dfg on 'London_Borough' == b. 
    borough = dfg[dfg['London_Borough'] == b]
    # Make a new entry in the final dictionary whose value's the result of calling create_price_ratio with the argument: borough
    final[b] = create_price_ratio(borough)
# We use the function and incorporate that into a new key of the dictionary 
print(final)
17/174:
# Make a variable called df_ratios, and assign it the result of calling the DataFrame method on the dictionary final. 
df_ratios = pd.DataFrame(final)
17/175:
# Call the head() method on this variable to check it out. 
df_ratios.head()
17/176:
# All we need to do now is transpose it, and reset the index! 
df_ratios_T = df_ratios.T
df_ratios = df_ratios_T.reset_index()
df_ratios.head()
17/177:
# Let's just rename the 'index' column as 'London_Borough', and the '0' column to '2018'.
df_ratios.rename(columns={'index':'London_Borough', 0:'2018'}, inplace=True)
df_ratios.head()
17/178:
# Let's sort in descending order and select the top 15 boroughs.
# Make a variable called top15, and assign it the result of calling sort_values() on df_ratios. 
top15 = df_ratios.sort_values(by='2018',ascending=False).head(15)
print(top15)
17/179: df_ratios.sort_values(by='2018',ascending=False)
17/180:
# Let's plot the boroughs that have seen the greatest changes in price.
# Make a variable called ax. Assign it the result of filtering top15 on 'Borough' and '2018', then calling plot(), with
# the parameter kind = 'bar'. 
ax = top15[['London_Borough','2018']].plot(kind='bar')

ax.set_xticklabels(top15.London_Borough)
17/181: df_ratios.sort_values(by='2018',ascending=False).mean()
17/182:
# Let's reset the index for our new DataFrame dfg, and call the head() method on it. 
dfg = dfg.reset_index()
dfg.head()
17/183: dfg.loc['2018']
17/184: dfg.loc('Year'[2018])
17/185: dfg.loc['Year'[2018]]
17/186: dfg.loc['Year'[2018]]
17/187: dfg[dfg['Year'] ==2018]
17/188: dfg[dfg['Year'] ==2018]
17/189:
18_only = dfg[dfg['Year'] ==2018]
18_only.sort_values('Average_price')
17/190:
only = dfg[dfg['Year'] ==2018]
only.sort_values('Average_price')
17/191:
only = dfg[dfg['Year'] ==2018]
only.describe()
17/192:
only = dfg[dfg['Year'] ==2018]
only.describe('Average_price')
17/193:
only = dfg[dfg['Year'] ==2018]
only.describe(['Average_price'])
17/194:
only = dfg[dfg['Year'] ==2018]
only.describe()
17/195:
only = dfg[dfg['Year'] ==2018]
only.describe(3)
17/196:
only = dfg[dfg['Year'] ==2018]
only.describe('3')
17/197:
only = dfg[dfg['Year'] ==2018]
only.describe()
17/198:
only = dfg[dfg['Year']==2018]['Average_price']
only.describe()
17/199:
'2018' = dfg[dfg['Year']==2018]['Average_price']
'2018'.describe()
17/200:
only = dfg[dfg['Year']==2018]['Average_price']
only.describe()
17/201:
first = dfg[dfg['Year']==1998]['Average_price']
first.describe()
17/202:
first = dfg[dfg['Year']==2020]['Average_price']
first.describe()
17/203:
camden_prices = df[df['London_Borough'] == 'Camden']
ax = camden_prices.plot(kind ='line', x = 'Month', y='Average_price')
ax.set_ylabel('Price')
17/204:
camden_prices = df['London_Borough'].median()
ax = camden_prices.plot(kind ='line', x = 'Month', y='Average_price')
ax.set_ylabel('Price')
17/205:
camden_prices = df[df['London_Borough'] == 'Camden']
ax = camden_prices.plot(kind ='line', x = 'Month', y='Average_price')
ax.set_ylabel('Price')
17/206:
prices = dfg['London_Borough']
ax = dfg.plot(kind ='line', x = 'Month', y='Average_price')
ax.set_ylabel('Price')
17/207:
prices = dfg['London_Borough']
ax = dfg.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/208:
prices = dfg['London_Borough']
ax = dfg.plot(kind ='line', x = 'Year', y='Average_price', subplots=True)
ax.set_ylabel('Price')
17/209:
prices = dfg['London_Borough'].median()
ax = dfg.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/210:
prices = dfg.median()
ax = dfg.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/211:
prices = dfg['Average_price']median()
ax = dfg.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/212:
prices = dfg['Average_price'].median()
ax = dfg.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/213:
prices = dfg['Average_price'].std()
ax = dfg.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/214:
prices = dfg.std()
ax = dfg.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/215:
prices = dfg.std()
ax = prices.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/216:
prices = dfg.median()
ax = prices.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/217:
prices = dfg['Average_price'.median()
ax = prices.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/218:
prices = dfg['Average_price'].median()
ax = prices.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/219:
prices = dfg['Average_price'].median()
ax = dfg.plot(kind ='line', x = 'Year', y='Average_price').median()
ax.set_ylabel('Price')
17/220:
prices = dfg['Average_price'].median()
ax = dfg.plot(kind ='line', x = 'Year', y='Average_price').mean()
ax.set_ylabel('Price')
17/221:
prices = dfg['Average_price'].median()
ax = dfg.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/222:
prices = dfg['Average_price'].median()
ax = dfg.mean()plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/223:
prices = dfg['Average_price'].median()
ax = dfg.mean().plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/224:
prices = dfg['Average_price'].median()
ax = dfg.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/225:
prices = dfg.loc['Average_price'].median()
ax = prices.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/226:
p = dfg.loc['Average_price'].median()
ax = p.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/227:
p = dfg['London_Borough']['Average_price'].median()
ax = p.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/228:
p = dfg.Average_price.median()
ax = p.plot(kind ='line', x = 'Year', y='Average_price')
ax.set_ylabel('Price')
17/229: dfg.plot()
17/230: dfg.plot(subplots=true)
17/231: dfg.plot(subplots=True)
17/232: dfg.plot(['Average_price']. subplots=True)
17/233: dfg.plot([Average_price'], subplots=True)
17/234: dfg.plot(Average_price, subplots=True)
17/235: dfg.plot('Average_price', subplots=True)
17/236: dfg['Average_price'].plot()
17/237: dfg['Average_price'].plot(x = 'Year')
17/238: dfg['Average_price'].plot(kind='line', x = 'Year')
17/239: dfg['Average_price'].plot(kind='line', x = 'Year', y='Average_price')
17/240: dfg.plot(kind='line', x = 'Year', y='Average_price')
17/241: dfg.['Average_price].median().plot(kind='line', x = 'Year', y='Average_price')
17/242: dfg.['Average_price'].median().plot(kind='line', x = 'Year', y='Average_price')
17/243: dfg['Average_price'].median().plot(kind='line', x = 'Year', y='Average_price')
17/244: dfg['Average_price'].median().plot(kind='hist', x = 'Year', y='Average_price')
17/245: dfg['Average_price'].plot(kind='hist', x = 'Year', y='Average_price')
19/1:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
19/2: os.getcwd()
19/3: os.listdir()
20/1:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
20/2: os.getcwd()
20/3: os.listdir()
20/4: os.listdir()
20/5: ski_data = pd.read_csv('updated_ski_data.csv')
20/6:
ski_data = pd.read_csv('updated_ski_data.csv')
ski_data.head()
21/1: path = '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone'
21/2: print ("The current working directory is %s" % path)
21/3: os.getcwd()
21/4:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
21/5: os.getcwd()
21/6: os.listdir()
21/7:
ski_data = pd.read_csv('updated_ski_data.csv')
ski_data.head()
21/8: path = '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone'
21/9: print ("The current working directory is %s" % path)
21/10: os.makedirs('data')
21/11: os.makedirs('figures')
21/12: os.makedirs('models')
21/13: print(os.listdir(path))
21/14: ski_data.columns
21/15: ski_data.dtypes
21/16: ski_data.dtypes()
21/17: ski_data.dtypes
21/18: df.info()
21/19: ski_data.info()
21/20: ski_data.nunique()
21/21: ski_data.unique()
21/22: ski_data.nunique()
21/23: ski_data.nunique()/330
21/24: ski_data.nunique()/ski_data.size
21/25: ski_data.size
21/26: ski_data.size()
21/27: ski_data.size
21/28: ski_data.nunique()/ski_data.size
22/1:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
22/2: os.getcwd()
22/3: os.listdir()
22/4:
#path=""
#os.chdir(path)
22/5:
ski_data = pd.read_csv('updated_ski_data.csv')
ski_data.head()
22/6: path = '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone'
22/7: print ("The current working directory is %s" % path)
22/8: os.makedirs('data')
22/9: ski_data.nunique()/ski_data.size
22/10: ski_data.nunique()
22/11: os.makedirs('data')
22/12:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
22/13: os.getcwd()
22/14: os.listdir()
22/15:
#path=""
#os.chdir(path)
22/16:
ski_data = pd.read_csv('updated_ski_data.csv')
ski_data.head()
22/17: path = '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone'
22/18: print ("The current working directory is %s" % path)
22/19: os.makedirs('data')
22/20: ski_data.nunique()
22/21: ski_data.nunique()/ski_data.size
22/22: ski_data.value_count()
22/23: ski_data.value_counts()
22/24: ski_data['Region'].value_counts()
22/25: ski_data['state'].value_counts()
22/26: ski_data['Name'].value_counts()
22/27: ski_data['state'].value_counts()
22/28: ski_data.info()
22/29: ski_data.dtypes
22/30: print(os.listdir(path))
22/31: agg(ski_data[min, max].T)
22/32: ski_data.agg([min, max])
22/33: ski_data.agg([min, max]).T
22/34: ski_data.info()
22/35: ski_data.describe()
22/36: ski_data.describe()
22/37: ski_data['Name'].value_counts()
22/38: ski_data['Big Mountain Resort']
22/39: ski_data['Big Mountain Resorts']
22/40: ski_data['Big']
22/41: ski_data['Crystal Resort']
22/42: ski_data['Crystal Mountain']
22/43: ski_data.loc['Crystal Mountain']
22/44: ski_data.loc['Shawnee Peak']
22/45: ski_data.loc['Name'][]'Shawnee Peak']
22/46: ski_data.loc['Name']['Shawnee Peak']
22/47: ski_data.loc[['Shawnee Peak']]
22/48:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
22/49:
ski_data = pd.read_csv('updated_ski_data.csv')
ski_data.head()
22/50: ski_data.columns
22/51: ski_data.loc['Name']
22/52: ski_data.head()
22/53: ski_data.loc['Alaska']
22/54: ski_data.index
22/55: ski_data.loc[ski_data['Name'] == 'Shawnee Peak', :]
22/56: ski_data.loc[ski_data['Name'] == 'Big Mountain Resort', :]
22/57: ski_data.loc[ski_data['Name'] == 'Big Mountain Resorts', :]
22/58: ski_data.loc[ski_data['Name'] == 'Big Mountain', :]
22/59: ski_data.loc[ski_data['Name'] == 'Shawnee Peak', :]
22/60: ski_data.loc[ski_data['Name']['Shawnee Peak'], :]
22/61: ski_data.nunique()/len(ski_data)
22/62: ski_data.nunique()/len(ski_data)
22/63: len(ski_data)
22/64:
nas=ski_data.DataFrame(ski_data.isnull().sum().sort_values(ascending=False)/len(ski_data),columns = ['percent'])
pos = nas['percent'] > 0
nas[pos]
22/65:
nas=pd.DataFrame(ski_data.isnull().sum().sort_values(ascending=False)/len(ski_data),columns = ['percent'])
pos = nas['percent'] > 0
nas[pos]
22/66:
nas=pd.DataFrame(ski_data.isnull().sum().sort_values(ascending=False)/len(ski_data),columns = ['percent'])
pos = nas['percent'] > 0
nas[pos]
22/67: ski_data['fastEigh']
22/68: ski_data['fastEight']
22/69: ski_data['fastEight']
22/70: ski_data['Name']
22/71: ski_data['Alyeska Resort']
22/72: ski_data['Name']['Alyeska Resort']
22/73: ski_data['Name']
22/74: ski_data['Name']['Arizona Snowbowl']
22/75: ski_data['Name']
22/76: ski_data['NightSkiing_ac']
22/77: ski_data['AdultWeekday']
22/78: ski_data
22/79: ski_data['Alaska']
22/80: ski_data['yearsOpen']
22/81: ski_data['SkiableTerrain_ac']
22/82: ski_data['total_chairs']-sum(ski_data['trams', 'fastEight', 'fastSixes', 'fastQuads', 'quad', 'triple', 'double', 'surface'])
22/83: ski_data['total_chairs']-ski_data['trams', 'fastEight', 'fastSixes', 'fastQuads', 'quad', 'triple', 'double', 'surface'].sum()
22/84: ski_data['total_chairs']-ski_data[['trams', 'fastEight', 'fastSixes', 'fastQuads', 'quad', 'triple', 'double', 'surface']].sum()
22/85: ski_data[['trams', 'fastEight', 'fastSixes', 'fastQuads', 'quad', 'triple', 'double', 'surface']].sum()
22/86: sum(ski_data[['trams', 'fastEight', 'fastSixes', 'fastQuads', 'quad', 'triple', 'double', 'surface']])
22/87: ski_data[['trams', 'fastEight', 'fastSixes', 'fastQuads', 'quad', 'triple', 'double', 'surface']].sum()
22/88: ski_data['trams' + 'fastEight']
22/89: ski_data[['trams' + 'fastEight']]
22/90: ski_data['trams'] + ski_data['fastEight']
22/91: ski_data['trams'] + ski_data['fastEight'] + ski_data['fastSixes'], #'fastQuads', 'quad', 'triple', 'double', 'surface']
22/92: ski_data['trams'] + ski_data['fastEight'] + ski_data['fastSixes'] #'fastQuads', 'quad', 'triple', 'double', 'surface']
22/93: ski_data['trams'] + ski_data['fastEight'] #'fastQuads', 'quad', 'triple', 'double', 'surface']
22/94: ski_data['trams'] + ski_data['fastEight'] + ski_data['fastSixes'] #'fastQuads', 'quad', 'triple', 'double', 'surface']
22/95: ski_data['trams'] + ski_data['fastEight'] + ski_data['fastSixes'] + ski_data['fastQuads'] + ski_data['quad'] + ski_data['triple'] + ski_data['double'] + ski_data['surface']
22/96: ski_data['total_chairs']-(ski_data['trams'] + ski_data['fastEight'] + ski_data['fastSixes'] + ski_data['fastQuads'] + ski_data['quad'] + ski_data['triple'] + ski_data['double'] + ski_data['surface'])
22/97: print(ski_data['total_chairs']-(ski_data['trams'] + ski_data['fastEight'] + ski_data['fastSixes'] + ski_data['fastQuads'] + ski_data['quad'] + ski_data['triple'] + ski_data['double'] + ski_data['surface']))
22/98: ski_data['total_chairs']-(ski_data['trams'] + ski_data['fastEight'] + ski_data['fastSixes'] + ski_data['fastQuads'] + ski_data['quad'] + ski_data['triple'] + ski_data['double'] + ski_data['surface'])
22/99: ski_data['total_chairs']-(ski_data['trams'] + ski_data['fastEight'] + ski_data['fastSixes'] + ski_data['fastQuads'] + ski_data['quad'] + ski_data['triple'] + ski_data['double'] + ski_data['surface'])
22/100: ski_data[['trams', 'fastEight']].sum()
22/101: ski_data[['trams', 'fastEight']].sum()
22/102: ski_data['trams']+ski_data['fastEight']
22/103: ski_data['trams']+ski_data['fastSixes']
22/104: ski_data['trams']+ski_data['fastSixes']
22/105: ski_data['trams']
22/106: ski_data['fastSixes']
22/107: ski_data.head()
22/108: sum(ski_data[['trams', 'fastSixes']])
22/109: ski_data[['trams', 'fastSixes']].sum()
22/110: ski_data['total_chairs']-(ski_data[['trams', 'fastSixes']].sum())
22/111: ski_data['total_chairs']-(ski_data[['trams', 'fastSixes']].sum(skipna=True))
22/112: ski_data['total_chairs']-(ski_data[['trams', 'fastSixes']].sum(axis=1, skipna=True))
22/113: ski_data[['trams', 'fastSixes']].sum(axis=1, skipna=True)
22/114: ski_data['total_chairs']-(ski_data[['trams', 'fastSixes']].sum(axis=1, skipna=True))
22/115: ski_data['total_chairs']-(ski_data[['trams', 'fastEight', 'fastSixes', 'fastQuads', 'quad', 'triple', 'double', 'surface']].sum(axis=1, skipna=True))
22/116: ski_data.fillna(ski_data['fastEight']:0)
22/117: ski_data.fillna('fastEight':0)
22/118: ski_data.fillna('fastEight': 0)
22/119:
values = {'fastEight': 0}
ski_data.fillna(value=values)
22/120:
values = {'fastEight': 0, 'NightSkiing_ac': 0}
ski_data.fillna(value=values)
22/121: df['AdultWeekday'].fillna((df['AdultWeekday'].mean()), inplace=True)
22/122: ski_data['AdultWeekday'].fillna((ski_data['AdultWeekday'].mean()), inplace=True)
22/123: ski_data
22/124: ski_data[['Name']['yearsOpen']]
22/125: ski_data[['Name', 'yearsOpen']]
22/126: ski_data[ski_data['yearsOpen']==np.nan]
22/127: ski_data['yearsOpen']
22/128: ski_data.loc[ski_data['yearsOpen' is na]
22/129: ski_data.loc[ski_data['yearsOpen'] is na]
22/130: ski_data.loc[ski_data['yearsOpen'] is np.nan]
22/131: ski_data.loc[ski_data['yearsOpen'] == np.nan]
22/132: ski_data['yearsOpen']
22/133: pd.set_option('display.max_columns, ski_data['yearsOpen'])
22/134: pd.set_option('display.max_columns', ski_data['yearsOpen'])
22/135: pd.set_option('display.max_columns', None)
22/136:
pd.set_option('display.max_columns', None)
ski_data['yearsOpen']
22/137:
pd.set_option('display.max_columns', None)
ski_data['yearsOpen']
22/138:
values = {'fastEight': 0, 'NightSkiing_ac': 0, 'TerrainParks': 0, 'yearsOpen': 0}
ski_data.fillna(value=values)
22/139:
ski_data['AdultWeekday'].fillna((ski_data['AdultWeekday'].mean()), inplace=True)
ski_data['AdultWeekend'].fillna((ski_data['AdultWeekend'].mean()), inplace=True)
ski_data['daysOpenLastYear'].fillna((ski_data['daysOpenLastYear'].mean()), inplace=True)
ski_data['projectedDaysOpen'].fillna((ski_data['projectedDaysOpen'].mean()), inplace=True)
ski_data['Snow Making_ac'].fillna((ski_data['Snow Making_ac'].mean()), inplace=True)
ski_data['averageSnowfall'].fillna((ski_data['averageSnowfall'].mean()), inplace=True)
ski_data['LongestRun_mi'].fillna((ski_data['LongestRun_mi'].mean()), inplace=True)
ski_data['Runs'].fillna((ski_data['Runs'].mean()), inplace=True)
ski_data['SkiableTerrain_ac'].fillna((ski_data['SkiableTerrain_ac'].mean()), inplace=True)SkiableTerrain_ac
22/140:
ski_data['AdultWeekday'].fillna((ski_data['AdultWeekday'].mean()), inplace=True)
ski_data['AdultWeekend'].fillna((ski_data['AdultWeekend'].mean()), inplace=True)
ski_data['daysOpenLastYear'].fillna((ski_data['daysOpenLastYear'].mean()), inplace=True)
ski_data['projectedDaysOpen'].fillna((ski_data['projectedDaysOpen'].mean()), inplace=True)
ski_data['Snow Making_ac'].fillna((ski_data['Snow Making_ac'].mean()), inplace=True)
ski_data['averageSnowfall'].fillna((ski_data['averageSnowfall'].mean()), inplace=True)
ski_data['LongestRun_mi'].fillna((ski_data['LongestRun_mi'].mean()), inplace=True)
ski_data['Runs'].fillna((ski_data['Runs'].mean()), inplace=True)
ski_data['SkiableTerrain_ac'].fillna((ski_data['SkiableTerrain_ac'].mean()), inplace=True
22/141:
ski_data['AdultWeekday'].fillna((ski_data['AdultWeekday'].mean()), inplace=True)
ski_data['AdultWeekend'].fillna((ski_data['AdultWeekend'].mean()), inplace=True)
ski_data['daysOpenLastYear'].fillna((ski_data['daysOpenLastYear'].mean()), inplace=True)
ski_data['projectedDaysOpen'].fillna((ski_data['projectedDaysOpen'].mean()), inplace=True)
ski_data['Snow Making_ac'].fillna((ski_data['Snow Making_ac'].mean()), inplace=True)
ski_data['averageSnowfall'].fillna((ski_data['averageSnowfall'].mean()), inplace=True)
ski_data['LongestRun_mi'].fillna((ski_data['LongestRun_mi'].mean()), inplace=True)
ski_data['Runs'].fillna((ski_data['Runs'].mean()), inplace=True)
ski_data['SkiableTerrain_ac'].fillna((ski_data['SkiableTerrain_ac'].mean()), inplace=True)
22/142: ski_data
22/143:
values = {'fastEight': 0, 'NightSkiing_ac': 0, 'TerrainParks': 0, 'yearsOpen': 0}
ski_data = ski_data.fillna(value=values)
22/144: ski_data
22/145:
ski_data['AdultWeekday'].fillna((ski_data['AdultWeekday'].mean()), inplace=True)
ski_data['AdultWeekend'].fillna((ski_data['AdultWeekend'].mean()), inplace=True)
ski_data['daysOpenLastYear'].fillna((ski_data['daysOpenLastYear'].mean()), inplace=True)
ski_data['projectedDaysOpen'].fillna((ski_data['projectedDaysOpen'].mean()), inplace=True)
ski_data['Snow Making_ac'].fillna((ski_data['Snow Making_ac'].mean()), inplace=True)
ski_data['averageSnowfall'].fillna((ski_data['averageSnowfall'].mean()), inplace=True)
ski_data['LongestRun_mi'].fillna((ski_data['LongestRun_mi'].mean()), inplace=True)
ski_data['Runs'].fillna((ski_data['Runs'].mean()), inplace=True)
ski_data['SkiableTerrain_ac'].fillna((ski_data['SkiableTerrain_ac'].mean()), inplace=True)
22/146:
#see above two coded cells fill NaN with either zero or the mean
ski_data
22/147:
#see above two coded cells fill NaN with either zero or the mean
ski_data
22/148: ski_data.info()
22/149:
duplicateRowsDF = ski_data[ski_data.duplicated()]
duplicateRowsDF
22/150: ski_data.to_csv(step2_output.csv)
22/151: ski_data.to_csv('step2_output.csv')
25/1:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
25/2: os.getcwd()
25/3: os.listdir()
25/4:
ski_data = pd.read_csv('updated_ski_data.csv')
ski_data.head()
25/5:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
25/6:
ski_data = pd.read_csv('updated_ski_data.csv')
ski_data.head()
24/1:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
24/2:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
24/3:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
24/4: os.getcwd()
24/5:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
24/6:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
24/7: os.getcwd()
24/8: os.getcwd()
24/9: os.listdir()
24/10:
df = pd.read_csv('step2_output.csv')
df.head()
24/11: df.describe()
26/1:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
26/2: os.getcwd()
26/3: os.listdir()
26/4:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
26/5:
ski_data = pd.read_csv('updated_ski_data.csv')
ski_data.head()
26/6: path = '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone'
26/7: print ("The current working directory is %s" % path)
26/8: os.makedirs('data')
26/9: os.makedirs('figures')
26/10: os.makedirs('models')
26/11: print(os.listdir(path))
26/12: ski_data.columns
26/13: ski_data.dtypes
26/14: ski_data.info()
26/15: ski_data.nunique()
26/16: ski_data.nunique()/len(ski_data)
26/17: ski_data['Name'].value_counts()
26/18: ski_data.agg([min, max]).T
26/19: ski_data.describe()
26/20:
nas=pd.DataFrame(ski_data.isnull().sum().sort_values(ascending=False)/len(ski_data),columns = ['percent'])
pos = nas['percent'] > 0
nas[pos]
26/21: ski_data['total_chairs']-(ski_data[['trams', 'fastEight', 'fastSixes', 'fastQuads', 'quad', 'triple', 'double', 'surface']].sum(axis=1, skipna=True))
26/22:
values = {'fastEight': 0, 'NightSkiing_ac': 0, 'TerrainParks': 0, 'yearsOpen': 0}
ski_data = ski_data.fillna(value=values)
26/23:
ski_data['AdultWeekday'].fillna((ski_data['AdultWeekday'].mean()), inplace=True)
ski_data['AdultWeekend'].fillna((ski_data['AdultWeekend'].mean()), inplace=True)
ski_data['daysOpenLastYear'].fillna((ski_data['daysOpenLastYear'].mean()), inplace=True)
ski_data['projectedDaysOpen'].fillna((ski_data['projectedDaysOpen'].mean()), inplace=True)
ski_data['Snow Making_ac'].fillna((ski_data['Snow Making_ac'].mean()), inplace=True)
ski_data['averageSnowfall'].fillna((ski_data['averageSnowfall'].mean()), inplace=True)
ski_data['LongestRun_mi'].fillna((ski_data['LongestRun_mi'].mean()), inplace=True)
ski_data['Runs'].fillna((ski_data['Runs'].mean()), inplace=True)
ski_data['SkiableTerrain_ac'].fillna((ski_data['SkiableTerrain_ac'].mean()), inplace=True)
26/24:
#see above two coded cells fill NaN with either zero or the mean
ski_data
26/25: ski_data.info()
26/26:
duplicateRowsDF = ski_data[ski_data.duplicated()]
duplicateRowsDF
26/27: ski_data.to_csv('step2_output.csv')
24/12:
df = pd.read_csv('step2_output.csv')
df.head()
24/13: df.describe().transpose()
24/14: df.plot.hist(bins=25)
24/15: describe.plot.hist(bins=25)
24/16: describe = df.describe().transpose()
24/17: describe = df.describe().transpose()
24/18: describe.plot.hist(bins=25)
24/19: df.describe().transpose()
24/20: describe.plot.hist(bins=25)
24/21: df.plot.hist(bins=25)
24/22: df.describe()
24/23: df.plot.hist(bins=25)
24/24: df.describe().transponse()
24/25: df.describe().transpose()
24/26: df.plot.hist(bins=25)
24/27: df['AdultsWeekday'].plot.hist(bins=25)
24/28: df['AdultWeekday'].plot.hist(bins=25)
24/29:
for x in df.columns:
    x.plot.hist(bins=25)
24/30:
x = df.columns
x.plot.hist(bins=25)
24/31: df.hist(bins=25)
24/32:
fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df.hist(ax = ax)
24/33:
fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df.hist(ax = ax, bins = 25)
24/34:
# Uncomment the following code to get your visualization started 
f, ax = plt.subplots(figsize=(10, 10))

# In the following brackets, we want the value_counts() of the states 
x = pd.DataFrame(df.state.value_counts())

# Get the state names by calling list() on the x.index
names = list(x.index)

# Get the values by plugging x.state into the list() function
values = list(x.state)

# We're now going to call the barplot() method on our sns seaborn object. 
# If you don't have a searborn object yet, make sure you've imported seaborn as sns in your imports above. 
sns.barplot(x=values, y=names, palette="RdBu_r")
24/35:
# Now do the same for regions! 
# Uncomment the following code to get your visualization started 
f, ax = plt.subplots(figsize=(10, 10))

# In the following brackets, we want the value_counts() of the states 
x = pd.DataFrame(df.Region.value_counts())

# Get the state names by calling list() on the x.index
names = list(x.index)

# Get the values by plugging x.state into the list() function
values = list(x.Region)

# We're now going to call the barplot() method on our sns seaborn object. 
# If you don't have a searborn object yet, make sure you've imported seaborn as sns in your imports above. 
sns.barplot(x=values, y=names, palette="RdBu_r")
24/36: df.drop(columns=['Region'])
24/37: df.drop(columns=['Region'])
24/38: df.head()
24/39: df = df.drop(columns=['Region'])
24/40: df.head()
24/41: df.drop(columns=['Region'])
24/42: df.drop(columns=['Region'])
24/43: df.head()
24/44:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
24/45: os.getcwd()
24/46:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
24/47: os.getcwd()
24/48: os.listdir()
24/49:
df = pd.read_csv('step2_output.csv')
df.head()
24/50: df.describe().transpose()
24/51:
fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df.hist(ax = ax, bins = 25)
24/52:
'''
total chairs, summit_elev, vertical_drop, base elev all similar
projected days open and daysOpenLastYear are similar
quad, double, triple, fast quads, terrain parks have similar features - may be related to avg. snowfall and surface which are also similar
yearsOpen?, Skiable terrain, snowmaking seems to have an outlier around 2000, 20000, and 3000 respectively. Runs has oulier too
Adult Weekend and Adult Weekday similar
'''
24/53:
# Uncomment the following code to get your visualization started 
f, ax = plt.subplots(figsize=(10, 10))

# In the following brackets, we want the value_counts() of the states 
x = pd.DataFrame(df.state.value_counts())

# Get the state names by calling list() on the x.index
names = list(x.index)

# Get the values by plugging x.state into the list() function
values = list(x.state)

# We're now going to call the barplot() method on our sns seaborn object. 
# If you don't have a searborn object yet, make sure you've imported seaborn as sns in your imports above. 
sns.barplot(x=values, y=names, palette="RdBu_r")
24/54:
# Now do the same for regions! 
# Uncomment the following code to get your visualization started 
f, ax = plt.subplots(figsize=(10, 10))

# In the following brackets, we want the value_counts() of the states 
x = pd.DataFrame(df.Region.value_counts())

# Get the state names by calling list() on the x.index
names = list(x.index)

# Get the values by plugging x.state into the list() function
values = list(x.Region)

# We're now going to call the barplot() method on our sns seaborn object. 
# If you don't have a searborn object yet, make sure you've imported seaborn as sns in your imports above. 
sns.barplot(x=values, y=names, palette="RdBu_r")
24/55: df.drop(columns=['Region'])
24/56: df.head()
24/57: df.head()
24/58: df = df.drop(columns=['Region'])
24/59: df.head()
24/60:
for column in df:
    plt.figure()
    df.boxplot([column])
24/61: df.plot(kind='box')
24/62:
import matplotlib.pyplot as plt

for column in df:
    plt.figure()
    df.boxplot([column])
24/63:
for column in df:
    plt.figure()
    df.boxplot([column])
24/64:
for column in df:
    plt.figure()
    df.boxplot([column])
24/65: df.plot(kind='box')
24/66: df.boxplot()
24/67: df.columns
24/68: columns = df.columns
24/69:
columns = df.columns

for column in columns:
    plt.figure()
    df.boxplot([column])
24/70:
columns = df.columns

for column in columns:
    df.boxplot([column])
24/71:
columns = df.columns

for column in columns:
    columns.boxplot([column])
24/72:
columns = df.columns

for column in columns:
    columns.boxplot()
24/73:
columns = df.columns

for column in columns:
    column.boxplot()
24/74:
columns = df.columns

for column in columns:
    df.boxplot(columns)
24/75:
columns = df.columns

for column in columns:
    df.boxplot(column)
24/76:
columns = df.columns

for column in columns:
    df.boxplot([column])
24/77: df.boxplot()
24/78: df.describe.boxplot()
24/79: df.describe().boxplot()
24/80:
columns = df.columns

for column in columns:
    df.boxplot([column])
24/81: boxplot = df.boxplot(grid=False, vert=False,fontsize=15)
24/82: boxplot = df.boxplot(grid=False, vert=False)
24/83:
# Let's get the Interquartile range, or IQR. This is equal to Q3 - Q1. 
# First, let's use the quantile() method to get the first quartile, and store it in a variable called Q1.
# We'll want to plug 0.25 into the quantile method. 
Q1 = df.quantile(0.25)

# Now get Q3 and store in a variable called Q3. 
Q3 = df.quantile(0.75)

# Now calculate the IQR, storing it in a variable called IQR.
IQR = Q3 - Q1

# Make a variable called `dfno`, and assign it the value: df[~((df < (Q1 - 1.5 * IQR)) |(df> (Q3 + 1.5 * IQR))).any(axis=1)]. 
# This filters on our existing dataframe, picking out just those observations that are NOT outliers. 
dfno = df[~((df < (Q1 - 1.5 * IQR)) |(df> (Q3 + 1.5 * IQR))).any(axis=1)]

# We now want to make a boxplot of this new dataframe dfno. 
dfno.boxplot(grid=False, vert=False,fontsize=15, figsize=(12,15))
24/84:
# Let's get the Interquartile range, or IQR. This is equal to Q3 - Q1. 
# First, let's use the quantile() method to get the first quartile, and store it in a variable called Q1.
# We'll want to plug 0.25 into the quantile method. 
Q1 = df.quantile(0.25)

# Now get Q3 and store in a variable called Q3. 
Q3 = df.quantile(0.75)

# Now calculate the IQR, storing it in a variable called IQR.
IQR = Q3 - Q1

# Make a variable called `dfno`, and assign it the value: df[~((df < (Q1 - 1.5 * IQR)) |(df> (Q3 + 1.5 * IQR))).any(axis=1)]. 
# This filters on our existing dataframe, picking out just those observations that are NOT outliers. 
dfno = df[~((df < (Q1 - 1.5 * IQR)) |(df> (Q3 + 1.5 * IQR))).any(axis=1)]

# We now want to make a boxplot of this new dataframe dfno. 
boxplotno = dfno.boxplot(grid=False, vert=False,fontsize=15, figsize=(12,15))
24/85:
# Print the shapes of our dataframes df and dfno to compare the number of observations in each. 
df.shape
24/86:
# Print the shapes of our dataframes df and dfno to compare the number of observations in each. 
df.shape
dfno.shape
24/87:
# Print the shapes of our dataframes df and dfno to compare the number of observations in each. 
df.shape
24/88: dfno.shape
24/89: boxplot = df.boxplot(grid=False, vert=False, fontsize=15, figsize=(12,15))
24/90:
# Make a histogram of the 'AdultWeekday' column of the dfno dataframe. 
# You'll want to call hist() on that column 
dfno['AdultWeekday'].hist()
24/91:
# Do the same but with the AdultWeekend column 
dfno['AdultWeekend'].hist()
24/92:
# Make a histogram of the 'AdultWeekday' column of the dfno dataframe. 
# You'll want to call hist() on that column 
dfno['AdultWeekday'].hist()
24/93: df['AdultWeekday'].hist()
24/94:
# Do the same as above! You got this :) 
dfno['daysOpenLastYear'].hist()
24/95: dfno['projecteDaysOpen'].hist()
24/96: dfno['projectedDaysOpen'].hist()
24/97: g = sns.pairplot(df)
24/98: g = sns.pairplot(df)
24/99: g
24/100: print(g)
24/101: g = sns.pairplot(df)
24/102: g = sns.pairplot(dfno)
24/103:
#Calculate the correlation coefficients
corr = dfno.corr()
#plot it in the next line
corr.round(2).style.background_gradient(cmap='coolwarm')
24/104:
#Calculate the correlation coefficients
corr = dfno.corr()
#plot it in the next line
corr.round(2).style.background_gradient(cmap='coolwarm')
24/105: dfno.head()
24/106:
# Step 1. Call the variable corr_matrix
 corr_matrix = dfno.corr(['AdultWeekday','AdultWeekend','daysOpenLastYear','projectedDaysOpen'], axis=1).corr().abs()

# Step 2. Uncomment the following code to get the upper triangle of the correlation matrix 
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Step 3. This code iterates through our columns and gets the index of any that have a correlation > 0.95
# Call the variable to_drop, get the columns of our 'upper' variable, make sure the threshold is 0.95.
to_drop = [column for column in corr_matrix.columns if any(upper[column] > .95)]
24/107:
# Step 1. Call the variable corr_matrix
corr_matrix = dfno.corr(['AdultWeekday','AdultWeekend','daysOpenLastYear','projectedDaysOpen'], axis=1).corr().abs()

# Step 2. Uncomment the following code to get the upper triangle of the correlation matrix 
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Step 3. This code iterates through our columns and gets the index of any that have a correlation > 0.95
# Call the variable to_drop, get the columns of our 'upper' variable, make sure the threshold is 0.95.
to_drop = [column for column in corr_matrix.columns if any(upper[column] > .95)]
24/108:
# Step 1. Call the variable corr_matrix
corr_matrix = dfno.columns(['AdultWeekday','AdultWeekend','daysOpenLastYear','projectedDaysOpen'], axis=1).corr().abs()

# Step 2. Uncomment the following code to get the upper triangle of the correlation matrix 
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Step 3. This code iterates through our columns and gets the index of any that have a correlation > 0.95
# Call the variable to_drop, get the columns of our 'upper' variable, make sure the threshold is 0.95.
to_drop = [column for column in corr_matrix.columns if any(upper[column] > .95)]
24/109:
# Step 1. Call the variable corr_matrix
corr_matrix = dfno.corr(['AdultWeekday','AdultWeekend','daysOpenLastYear','projectedDaysOpen'], axis=1).corr().abs()

# Step 2. Uncomment the following code to get the upper triangle of the correlation matrix 
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Step 3. This code iterates through our columns and gets the index of any that have a correlation > 0.95
# Call the variable to_drop, get the columns of our 'upper' variable, make sure the threshold is 0.95.
to_drop = [column for column in corr_matrix.columns if any(upper[column] > .95)]
24/110:
# Step 1. Call the variable corr_matrix
corr_matrix = dfno.drop(['AdultWeekday','AdultWeekend','daysOpenLastYear','projectedDaysOpen'], axis=1).corr().abs()

# Step 2. Uncomment the following code to get the upper triangle of the correlation matrix 
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Step 3. This code iterates through our columns and gets the index of any that have a correlation > 0.95
# Call the variable to_drop, get the columns of our 'upper' variable, make sure the threshold is 0.95.
to_drop = [column for column in corr_matrix.columns if any(upper[column] > .95)]
24/111:
# Step 1. Call the variable corr_matrix
corr_matrix = dfno.drop(['AdultWeekday','AdultWeekend','daysOpenLastYear','projectedDaysOpen'], axis=1).corr().abs()

# Step 2. Uncomment the following code to get the upper triangle of the correlation matrix 
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Step 3. This code iterates through our columns and gets the index of any that have a correlation > 0.95
# Call the variable to_drop, get the columns of our 'upper' variable, make sure the threshold is 0.95.
to_drop = [column for column in corr_matrix.columns if any(upper[column] > .95)]
24/112:
# Step 1. Call the variable corr_matrix
corr_matrix = dfno.drop(['AdultWeekday','AdultWeekend','daysOpenLastYear','projectedDaysOpen'], axis=1).corr().abs()

# Step 2. Uncomment the following code to get the upper triangle of the correlation matrix 
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Step 3. This code iterates through our columns and gets the index of any that have a correlation > 0.95
# Call the variable to_drop, get the columns of our 'upper' variable, make sure the threshold is 0.95.
to_drop = [column for column in corr_matrix.columns if any(upper[column] > .95)]
24/113:
# Let's see those features! 
print('Features selected to drop include:',to_drop)
24/114: print('Reduced dataframe size: ',dfno.drop(dfno[to_drop], axis=1).shape)
24/115: print('Reduced dataframe size: ',dfno.drop(dfno[to_drop], axis=1).shape)
24/116: corr_matrix
24/117:
# Now replace dfno by the result of dropping the columns in the to_drop variable from it
dfno = dfno.drop(dfno[to_drop], axis=1)
30/1:
from sklearn.cluster import KMeans
x = dfno.drop(['Name','state'], axis =1).values
30/2:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
30/3: os.getcwd()
30/4:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
30/5: os.getcwd()
30/6: os.listdir()
30/7:
df = pd.read_csv('step2_output.csv')
df.head()
30/8: df.describe().transpose()
30/9:
fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df.hist(ax = ax, bins = 25)
30/10:
'''
total chairs, summit_elev, vertical_drop, base elev all similar
projected days open and daysOpenLastYear are similar
quad, double, triple, fast quads, terrain parks have similar features - may be related to avg. snowfall and surface which are also similar
yearsOpen?, Skiable terrain, snowmaking seems to have an outlier around 2000, 20000, and 3000 respectively. Runs has oulier too
Adult Weekend and Adult Weekday similar
'''
30/11:
# Uncomment the following code to get your visualization started 
f, ax = plt.subplots(figsize=(10, 10))

# In the following brackets, we want the value_counts() of the states 
x = pd.DataFrame(df.state.value_counts())

# Get the state names by calling list() on the x.index
names = list(x.index)

# Get the values by plugging x.state into the list() function
values = list(x.state)

# We're now going to call the barplot() method on our sns seaborn object. 
# If you don't have a searborn object yet, make sure you've imported seaborn as sns in your imports above. 
sns.barplot(x=values, y=names, palette="RdBu_r")
30/12:
# Now do the same for regions! 
# Uncomment the following code to get your visualization started 
f, ax = plt.subplots(figsize=(10, 10))

# In the following brackets, we want the value_counts() of the states 
x = pd.DataFrame(df.Region.value_counts())

# Get the state names by calling list() on the x.index
names = list(x.index)

# Get the values by plugging x.state into the list() function
values = list(x.Region)

# We're now going to call the barplot() method on our sns seaborn object. 
# If you don't have a searborn object yet, make sure you've imported seaborn as sns in your imports above. 
sns.barplot(x=values, y=names, palette="RdBu_r")
30/13: df = df.drop(columns=['Region'])
30/14: df.head()
30/15: boxplot = df.boxplot(grid=False, vert=False, fontsize=15, figsize=(12,15))
30/16:
# Let's get the Interquartile range, or IQR. This is equal to Q3 - Q1. 
# First, let's use the quantile() method to get the first quartile, and store it in a variable called Q1.
# We'll want to plug 0.25 into the quantile method. 
Q1 = df.quantile(0.25)

# Now get Q3 and store in a variable called Q3. 
Q3 = df.quantile(0.75)

# Now calculate the IQR, storing it in a variable called IQR.
IQR = Q3 - Q1

# Make a variable called `dfno`, and assign it the value: df[~((df < (Q1 - 1.5 * IQR)) |(df> (Q3 + 1.5 * IQR))).any(axis=1)]. 
# This filters on our existing dataframe, picking out just those observations that are NOT outliers. 
dfno = df[~((df < (Q1 - 1.5 * IQR)) |(df> (Q3 + 1.5 * IQR))).any(axis=1)]

# We now want to make a boxplot of this new dataframe dfno. 
boxplotno = dfno.boxplot(grid=False, vert=False,fontsize=15, figsize=(12,15))
30/17:
# Print the shapes of our dataframes df and dfno to compare the number of observations in each. 
df.shape
30/18: dfno.shape
30/19:
# Make a histogram of the 'AdultWeekday' column of the dfno dataframe. 
# You'll want to call hist() on that column 
dfno['AdultWeekday'].hist()
30/20:
# Do the same but with the AdultWeekend column 
dfno['AdultWeekend'].hist()
30/21:
# Do the same as above! You got this :) 
dfno['daysOpenLastYear'].hist()
30/22: dfno['projectedDaysOpen'].hist()
30/23: g = sns.pairplot(dfno)
30/24:
#Calculate the correlation coefficients
corr = dfno.corr()
#plot it in the next line
corr.round(2).style.background_gradient(cmap='coolwarm')
30/25:
# Step 1. Call the variable corr_matrix
corr_matrix = dfno.drop(['AdultWeekday','AdultWeekend','daysOpenLastYear','projectedDaysOpen'], axis=1).corr().abs()

# Step 2. Uncomment the following code to get the upper triangle of the correlation matrix 
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Step 3. This code iterates through our columns and gets the index of any that have a correlation > 0.95
# Call the variable to_drop, get the columns of our 'upper' variable, make sure the threshold is 0.95.
to_drop = [column for column in corr_matrix.columns if any(upper[column] > .95)]
30/26:
# Let's see those features! 
print('Features selected to drop include:',to_drop)
30/27: print('Reduced dataframe size: ',dfno.drop(dfno[to_drop], axis=1).shape)
30/28:
# Now replace dfno by the result of dropping the columns in the to_drop variable from it
dfno = dfno.drop(dfno[to_drop], axis=1)
30/29:
from sklearn.cluster import KMeans
x = dfno.drop(['Name','state'], axis =1).values
30/30:
Error =[]
for i in range(1, 11):
   kmeans = KMeans(n_clusters = i).fit(x)
    kmeans.fit(x)
    Error.append(kmeans.inertia_)
import matplotlib.pyplot as plt
plt.plot(range(1, 11), Error)
plt.title('Elbow method')
plt.xlabel('No of clusters')
plt.ylabel('Error')
plt.show()
30/31:
Error =[]
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i).fit(x)
    kmeans.fit(x)
    Error.append(kmeans.inertia_)
import matplotlib.pyplot as plt
plt.plot(range(1, 11), Error)
plt.title('Elbow method')
plt.xlabel('No of clusters')
plt.ylabel('Error')
plt.show()
30/32:
# This code will fit the k-means algorithm with our k parameter set to three, and plot the results. Cool, huh? 
kmeans3 = KMeans(n_clusters=3)
y_kmeans3 = kmeans3.fit_predict(x)
plt.scatter(x[:, 0], x[:, 1], c=y_kmeans3, s=50, cmap='viridis')

centers = kmeans3.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);
30/33:
# Make a new column in your dfno dataframe called 'clusters', and assign it the variable: y_kmeans3
dfno['clusters'] = y_kmeans3
30/34:
# Make a new column in your dfno dataframe called 'clusters', and assign it the variable: y_kmeans3
dfno['clusters'] = y_kmeans3
30/35: dfno.head()
30/36: dfno
30/37:
# Write your dataframe to csv 
dfno.to_csv('step3_output.csv')
34/1:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
34/2:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
34/3: os.getcwd()
34/4:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
34/5: os.getcwd()
34/6: os.listdir()
34/7:
df = pd.read_csv('step3_output.csv')
df.head()
34/8: dfo=df.select_dtypes(include=['state']) # select object type columns
34/9:
dfo=df.select_dtypes(include=['state']) # select object type columns
df = pd.concat([df.drop(dfo, axis=1), pd.get_dummies(dfo)], axis=1)
34/10:
dfo=df.select_dtypes(include=['object']) # select object type columns
df = pd.concat([df.drop(dfo, axis=1), pd.get_dummies(dfo)], axis=1)
34/11:
dfo=df.select_dtypes(include=['object']) # select object type columns
df = pd.concat([df.drop(dfo, axis=1), pd.get_dummies(dfo)], axis=1)
34/12: dfo.head()
34/13: dfo.head()
34/14:
dfo=df.select_dtypes(include=['object']) # select object type columns
df = pd.concat([df.drop(dfo, axis=1), pd.get_dummies(dfo)], axis=1)
34/15:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
34/16: os.getcwd()
34/17:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
34/18: os.getcwd()
34/19: os.listdir()
34/20:
df = pd.read_csv('step3_output.csv')
df.head()
34/21:
dfo=df.select_dtypes(include=['object']) # select object type columns
df = pd.concat([df.drop(dfo, axis=1), pd.get_dummies(dfo)], axis=1)
34/22: dfo
34/23: df.head()
34/24: dfo.head()
34/25: df.head()
34/26: df.head().T
34/27: df.T
34/28: df
34/29:
dfo=df['state']
df = pd.concat([df.drop(dfo, axis=1), pd.get_dummies(dfo)], axis=1)
34/30:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
34/31: os.getcwd()
34/32:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
34/33: os.getcwd()
34/34: os.listdir()
34/35:
df = pd.read_csv('step3_output.csv')
df.head()
34/36:
dfo=df['state']
df = pd.concat([df.drop(dfo, axis=1), pd.get_dummies(dfo)], axis=1)
34/37: df
34/38: dfo
34/39:
dfo=df.state
df = pd.concat([df.drop(dfo, axis=1), pd.get_dummies(dfo)], axis=1)
34/40: dfo
34/41: df
34/42:
dfo=df.state
df = pd.concat([df.drop(dfo), pd.get_dummies(dfo)], axis=1)
34/43:
dummy = pd.get_dummies(df['state'])
dummy.head()
34/44: df = pd.concat([df, dummy], axis=1)
34/45:
df = pd.concat([df, dummy], axis=1)
df.head()
34/46:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.tranform(X)
34/47:
dummy = pd.get_dummies(df['state'])
dummy.head()
34/48:
df = pd.concat([df, dummy], axis=1)
df.head()
34/49:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.tranform(X)
34/50:
df = pd.concat([df, dummy], axis=1)
df.head()
34/51:
df = pd.concat([df, dummy], axis=1)
df.head()
34/52: df = drop(df['state'])
34/53: df = df.drop(df['state'])
34/54:
state = df['state']
df = df.drop(state)
34/55:
state = df['state']
df = df.drop(state, axis=1)
34/56: df.head()
34/57:
state = df['state']
df = df.drop(state, axis=1)
34/58: df.head()
34/59:
state = df.state
df = df.drop(state, axis=1)
34/60: df.head()
34/61: df.drop(columns=['state'])
34/62: df.head()
34/63: df = df.drop(columns=['state'])
34/64: df.head()
34/65:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.tranform(X)
34/66:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
34/67:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
36/1:
all first model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X_train,y_train)
36/2:
#all first model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X_train,y_train)
36/3:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
36/4: os.getcwd()
36/5:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
36/6: os.getcwd()
36/7: os.listdir()
36/8:
df = pd.read_csv('step3_output.csv')
df.head()
36/9:
dummy = pd.get_dummies(df['state'])
dummy.head()
36/10:
df = pd.concat([df, dummy], axis=1)
df.head()
36/11: df = df.drop(columns=['state'])
36/12: df.head()
36/13:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
36/14:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
36/15:
#all first model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X_train,y_train)
36/16:
#all first model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X_train,y_train)
36/17:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y_pred = model.predict(X_test)
36/18:
from sklearn.metrics import explained_variance_score()
from sklearn.metrics import mean_absolute_error()
36/19:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error()
36/20:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
36/21:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
explained_variance_score(y_true, y_pred)
mean_absolute_error(y_true, y_pred)
36/22:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
explained_variance_score(y_test, y_pred)
mean_absolute_error(y_test, y_pred)
36/23:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
explained_variance_score(y_test, y_pred)
36/24: mean_absolute_error(y_test, y_pred)
36/25: print(lm.intercept_)
36/26:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
36/27:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
sort(pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient']), reverse=True)
36/28:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
list.sort(pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient']), reverse=True)
36/29:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
36/30:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values()
36/31:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(axis=1)
36/32:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
36/33:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(by columns)
36/34:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(columns)
36/35:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(by=columns)
36/36:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(by='Coefficient')
36/37:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(by='Coefficient', ascending=False)
36/38:
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X_train,y_train)
36/39: y_pred = model.predict(X_test)
37/1:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
37/2: os.getcwd()
37/3:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
37/4: os.getcwd()
37/5: os.listdir()
37/6:
df = pd.read_csv('step2_output.csv')
df.head()
37/7: df.describe().transpose()
37/8:
fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df.hist(ax = ax, bins = 25)
37/9:
'''
total chairs, summit_elev, vertical_drop, base elev all similar
projected days open and daysOpenLastYear are similar
quad, double, triple, fast quads, terrain parks have similar features - may be related to avg. snowfall and surface which are also similar
yearsOpen?, Skiable terrain, snowmaking seems to have an outlier around 2000, 20000, and 3000 respectively. Runs has oulier too
Adult Weekend and Adult Weekday similar
'''
37/10:
# Uncomment the following code to get your visualization started 
f, ax = plt.subplots(figsize=(10, 10))

# In the following brackets, we want the value_counts() of the states 
x = pd.DataFrame(df.state.value_counts())

# Get the state names by calling list() on the x.index
names = list(x.index)

# Get the values by plugging x.state into the list() function
values = list(x.state)

# We're now going to call the barplot() method on our sns seaborn object. 
# If you don't have a searborn object yet, make sure you've imported seaborn as sns in your imports above. 
sns.barplot(x=values, y=names, palette="RdBu_r")
37/11:
# Now do the same for regions! 
# Uncomment the following code to get your visualization started 
f, ax = plt.subplots(figsize=(10, 10))

# In the following brackets, we want the value_counts() of the states 
x = pd.DataFrame(df.Region.value_counts())

# Get the state names by calling list() on the x.index
names = list(x.index)

# Get the values by plugging x.state into the list() function
values = list(x.Region)

# We're now going to call the barplot() method on our sns seaborn object. 
# If you don't have a searborn object yet, make sure you've imported seaborn as sns in your imports above. 
sns.barplot(x=values, y=names, palette="RdBu_r")
37/12: df = df.drop(columns=['Region'])
37/13: df.head()
37/14: boxplot = df.boxplot(grid=False, vert=False, fontsize=15, figsize=(12,15))
37/15:
# Let's get the Interquartile range, or IQR. This is equal to Q3 - Q1. 
# First, let's use the quantile() method to get the first quartile, and store it in a variable called Q1.
# We'll want to plug 0.25 into the quantile method. 
Q1 = df.quantile(0.25)

# Now get Q3 and store in a variable called Q3. 
Q3 = df.quantile(0.75)

# Now calculate the IQR, storing it in a variable called IQR.
IQR = Q3 - Q1

# Make a variable called `dfno`, and assign it the value: df[~((df < (Q1 - 1.5 * IQR)) |(df> (Q3 + 1.5 * IQR))).any(axis=1)]. 
# This filters on our existing dataframe, picking out just those observations that are NOT outliers. 
dfno = df[~((df < (Q1 - 1.5 * IQR)) |(df> (Q3 + 1.5 * IQR))).any(axis=1)]

# We now want to make a boxplot of this new dataframe dfno. 
boxplotno = dfno.boxplot(grid=False, vert=False,fontsize=15, figsize=(12,15))
37/16:
# Print the shapes of our dataframes df and dfno to compare the number of observations in each. 
df.shape
37/17: dfno.shape
37/18:
# Make a histogram of the 'AdultWeekday' column of the dfno dataframe. 
# You'll want to call hist() on that column 
dfno['AdultWeekday'].hist()
37/19:
# Do the same but with the AdultWeekend column 
dfno['AdultWeekend'].hist()
37/20:
# Do the same as above! You got this :) 
dfno['daysOpenLastYear'].hist()
37/21: dfno['projectedDaysOpen'].hist()
37/22: g = sns.pairplot(dfno)
37/23:
#Calculate the correlation coefficients
corr = dfno.corr()
#plot it in the next line
corr.round(2).style.background_gradient(cmap='coolwarm')
37/24:
# Step 1. Call the variable corr_matrix
corr_matrix = dfno.drop(['AdultWeekday','AdultWeekend','daysOpenLastYear','projectedDaysOpen'], axis=1).corr().abs()

# Step 2. Uncomment the following code to get the upper triangle of the correlation matrix 
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Step 3. This code iterates through our columns and gets the index of any that have a correlation > 0.95
# Call the variable to_drop, get the columns of our 'upper' variable, make sure the threshold is 0.95.
to_drop = [column for column in corr_matrix.columns if any(upper[column] > .95)]
37/25:
# Let's see those features! 
print('Features selected to drop include:',to_drop)
37/26: print('Reduced dataframe size: ',dfno.drop(dfno[to_drop], axis=1).shape)
37/27:
# Now replace dfno by the result of dropping the columns in the to_drop variable from it
dfno = dfno.drop(dfno[to_drop], axis=1)
37/28:
from sklearn.cluster import KMeans
x = df.drop(['Name','state'], axis =1).values
37/29:
Error =[]
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i).fit(x)
    kmeans.fit(x)
    Error.append(kmeans.inertia_)
import matplotlib.pyplot as plt
plt.plot(range(1, 11), Error)
plt.title('Elbow method')
plt.xlabel('No of clusters')
plt.ylabel('Error')
plt.show()
37/30:
# This code will fit the k-means algorithm with our k parameter set to three, and plot the results. Cool, huh? 
kmeans3 = KMeans(n_clusters=3)
y_kmeans3 = kmeans3.fit_predict(x)
plt.scatter(x[:, 0], x[:, 1], c=y_kmeans3, s=50, cmap='viridis')

centers = kmeans3.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);
37/31:
# Make a new column in your dfno dataframe called 'clusters', and assign it the variable: y_kmeans3
df['clusters'] = y_kmeans3
37/32:
# Write your dataframe to csv 
df.to_csv('step3_output.csv', index=False)
36/40:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
36/41: os.getcwd()
36/42:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
36/43: os.getcwd()
36/44: os.listdir()
36/45:
df = pd.read_csv('step3_output.csv')
df.head()
36/46:
dummy = pd.get_dummies(df['state'])
dummy.head()
36/47:
df = pd.concat([df, dummy], axis=1)
df.head()
36/48: df = df.drop(columns=['state'])
36/49: df.head()
36/50:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
36/51:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
36/52:
#all first model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X_train,y_train)
36/53:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y_pred = model.predict(X_test)
36/54:
# You might want to use the explained_variance_score() and mean_absolute_error() metrics.
# To do so, you will need to import them from sklearn.metrics. 
# You can plug y_test and y_pred into the functions to evaluate the model
36/55:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
explained_variance_score(y_test, y_pred)
36/56: mean_absolute_error(y_test, y_pred)
36/57: print(lm.intercept_)
36/58:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(by='Coefficient', ascending=False)
38/1: ski_data.to_csv('step2_output.csv', index=False)
38/2:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
38/3: os.getcwd()
38/4: os.listdir()
38/5:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
38/6:
ski_data = pd.read_csv('updated_ski_data.csv')
ski_data.head()
38/7: path = '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone'
38/8: print ("The current working directory is %s" % path)
38/9: os.makedirs('data')
37/33:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
37/34: os.getcwd()
37/35:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
37/36: os.getcwd()
37/37: os.listdir()
37/38:
df = pd.read_csv('step2_output.csv')
df.head()
37/39: df.describe().transpose()
37/40:
fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df.hist(ax = ax, bins = 25)
37/41:
'''
total chairs, summit_elev, vertical_drop, base elev all similar
projected days open and daysOpenLastYear are similar
quad, double, triple, fast quads, terrain parks have similar features - may be related to avg. snowfall and surface which are also similar
yearsOpen?, Skiable terrain, snowmaking seems to have an outlier around 2000, 20000, and 3000 respectively. Runs has oulier too
Adult Weekend and Adult Weekday similar
'''
37/42:
# Uncomment the following code to get your visualization started 
f, ax = plt.subplots(figsize=(10, 10))

# In the following brackets, we want the value_counts() of the states 
x = pd.DataFrame(df.state.value_counts())

# Get the state names by calling list() on the x.index
names = list(x.index)

# Get the values by plugging x.state into the list() function
values = list(x.state)

# We're now going to call the barplot() method on our sns seaborn object. 
# If you don't have a searborn object yet, make sure you've imported seaborn as sns in your imports above. 
sns.barplot(x=values, y=names, palette="RdBu_r")
37/43:
# Now do the same for regions! 
# Uncomment the following code to get your visualization started 
f, ax = plt.subplots(figsize=(10, 10))

# In the following brackets, we want the value_counts() of the states 
x = pd.DataFrame(df.Region.value_counts())

# Get the state names by calling list() on the x.index
names = list(x.index)

# Get the values by plugging x.state into the list() function
values = list(x.Region)

# We're now going to call the barplot() method on our sns seaborn object. 
# If you don't have a searborn object yet, make sure you've imported seaborn as sns in your imports above. 
sns.barplot(x=values, y=names, palette="RdBu_r")
37/44: df = df.drop(columns=['Region'])
37/45: df.head()
37/46: boxplot = df.boxplot(grid=False, vert=False, fontsize=15, figsize=(12,15))
37/47:
# Let's get the Interquartile range, or IQR. This is equal to Q3 - Q1. 
# First, let's use the quantile() method to get the first quartile, and store it in a variable called Q1.
# We'll want to plug 0.25 into the quantile method. 
Q1 = df.quantile(0.25)

# Now get Q3 and store in a variable called Q3. 
Q3 = df.quantile(0.75)

# Now calculate the IQR, storing it in a variable called IQR.
IQR = Q3 - Q1

# Make a variable called `dfno`, and assign it the value: df[~((df < (Q1 - 1.5 * IQR)) |(df> (Q3 + 1.5 * IQR))).any(axis=1)]. 
# This filters on our existing dataframe, picking out just those observations that are NOT outliers. 
dfno = df[~((df < (Q1 - 1.5 * IQR)) |(df> (Q3 + 1.5 * IQR))).any(axis=1)]

# We now want to make a boxplot of this new dataframe dfno. 
boxplotno = dfno.boxplot(grid=False, vert=False,fontsize=15, figsize=(12,15))
37/48:
# Print the shapes of our dataframes df and dfno to compare the number of observations in each. 
df.shape
37/49: dfno.shape
37/50:
# Make a histogram of the 'AdultWeekday' column of the dfno dataframe. 
# You'll want to call hist() on that column 
dfno['AdultWeekday'].hist()
37/51:
# Do the same but with the AdultWeekend column 
dfno['AdultWeekend'].hist()
37/52:
# Do the same as above! You got this :) 
dfno['daysOpenLastYear'].hist()
37/53: dfno['projectedDaysOpen'].hist()
37/54: g = sns.pairplot(dfno)
36/59:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend', 'state'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
37/55:
#Calculate the correlation coefficients
corr = dfno.corr()
#plot it in the next line
corr.round(2).style.background_gradient(cmap='coolwarm')
37/56:
# Step 1. Call the variable corr_matrix
corr_matrix = dfno.drop(['AdultWeekday','AdultWeekend','daysOpenLastYear','projectedDaysOpen'], axis=1).corr().abs()

# Step 2. Uncomment the following code to get the upper triangle of the correlation matrix 
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Step 3. This code iterates through our columns and gets the index of any that have a correlation > 0.95
# Call the variable to_drop, get the columns of our 'upper' variable, make sure the threshold is 0.95.
to_drop = [column for column in corr_matrix.columns if any(upper[column] > .95)]
37/57:
# Let's see those features! 
print('Features selected to drop include:',to_drop)
37/58: print('Reduced dataframe size: ',dfno.drop(dfno[to_drop], axis=1).shape)
37/59:
# Now replace dfno by the result of dropping the columns in the to_drop variable from it
dfno = dfno.drop(dfno[to_drop], axis=1)
37/60:
from sklearn.cluster import KMeans
x = df.drop(['Name','state'], axis =1).values
37/61:
Error =[]
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i).fit(x)
    kmeans.fit(x)
    Error.append(kmeans.inertia_)
import matplotlib.pyplot as plt
plt.plot(range(1, 11), Error)
plt.title('Elbow method')
plt.xlabel('No of clusters')
plt.ylabel('Error')
plt.show()
37/62:
# This code will fit the k-means algorithm with our k parameter set to three, and plot the results. Cool, huh? 
kmeans3 = KMeans(n_clusters=3)
y_kmeans3 = kmeans3.fit_predict(x)
plt.scatter(x[:, 0], x[:, 1], c=y_kmeans3, s=50, cmap='viridis')

centers = kmeans3.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);
37/63:
# Make a new column in your dfno dataframe called 'clusters', and assign it the variable: y_kmeans3
df['clusters'] = y_kmeans3
37/64:
# Write your dataframe to csv 
df.to_csv('step3_output.csv', index=False)
36/60:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend', dummy], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
39/1:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
39/2: os.getcwd()
39/3:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
39/4: os.getcwd()
39/5: os.listdir()
39/6:
df = pd.read_csv('step3_output.csv')
df.head()
39/7:
dummy = pd.get_dummies(df['state'])
dummy.head()
39/8:
df = pd.concat([df, dummy], axis=1)
df.head()
39/9: df = df.drop(columns=['state'])
39/10: df.head()
39/11:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
39/12:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
39/13:
#all first model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X_train,y_train)
39/14:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y_pred = model.predict(X_test)
39/15:
# You might want to use the explained_variance_score() and mean_absolute_error() metrics.
# To do so, you will need to import them from sklearn.metrics. 
# You can plug y_test and y_pred into the functions to evaluate the model
39/16:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
explained_variance_score(y_test, y_pred)
39/17: mean_absolute_error(y_test, y_pred)
39/18: print(lm.intercept_)
39/19:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(by='Coefficient', ascending=False)
39/20:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend', dummy], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
39/21:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop([['Name','AdultWeekend', dummy]], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
39/22:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend', 'state'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
39/23:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend', dummy.columns], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
39/24: dummy.columns
39/25: type(df['Name'])
39/26: type(dummy)
39/27:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
39/28: os.getcwd()
39/29:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
39/30: os.getcwd()
39/31: os.listdir()
39/32:
df1 = pd.read_csv('step3_output.csv')
df1.head()
39/33:
dummy = pd1.get_dummies(df['state'])
dummy.head()
39/34:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df1.drop(['Name','AdultWeekend', 'state'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
39/35:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
39/36:
#all first model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X_train,y_train)
39/37: mean_absolute_error(y_test, y_pred)
39/38: print(lm.intercept_)
39/39:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df1.drop(['Name','AdultWeekend', 'state'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
39/40:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
39/41:
#all second model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X_train,y_train)
39/42:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y_pred = model.predict(X_test)
39/43:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
explained_variance_score(y_test, y_pred)
39/44: mean_absolute_error(y_test, y_pred)
39/45: print(lm.intercept_)
39/46:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(by='Coefficient', ascending=False)
40/1:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
40/2: os.getcwd()
40/3:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
40/4: os.getcwd()
40/5: os.listdir()
40/6:
df1 = pd.read_csv('step3_output.csv')
df1.head()
40/7:
dummy = pd1.get_dummies(df['state'])
dummy.head()
40/8:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
40/9: os.getcwd()
40/10:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
40/11: os.getcwd()
40/12: os.listdir()
40/13:
df1 = pd.read_csv('step3_output.csv')
df1.head()
40/14:
dummy = pd1.get_dummies(df['state'])
dummy.head()
40/15:
dummy = pd.get_dummies(df['state'])
dummy.head()
40/16:
dummy = pd.get_dummies(df1['state'])
dummy.head()
40/17:
df = pd.concat([df1, dummy], axis=1)
df.head()
40/18: df = df.drop(columns=['state'])
40/19: df.head()
40/20:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
40/21:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
40/22:
#all first model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X_train,y_train)
40/23:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y_pred = model.predict(X_test)
40/24:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
explained_variance_score(y_test, y_pred)
40/25: mean_absolute_error(y_test, y_pred)
40/26: print(lm.intercept_)
40/27:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(by='Coefficient', ascending=False)
40/28:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df1.drop(['Name','AdultWeekend', 'state'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
40/29:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
40/30:
#all second model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X_train,y_train)
40/31:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y_pred = model.predict(X_test)
40/32:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
explained_variance_score(y_test, y_pred)
40/33: mean_absolute_error(y_test, y_pred)
40/34: print(lm.intercept_)
40/35:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(by='Coefficient', ascending=False)
40/36:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df1.drop(['Name','AdultWeekend', 'state'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
40/37:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
40/38:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
40/39:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df1.drop(['Name','AdultWeekend', 'state', 'summit_elev', 'without base_elev'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
40/40:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df1.drop(['Name','AdultWeekend', 'state', 'summit_elev', 'base_elev'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
40/41:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
40/42:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
40/43:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y_pred = model.predict(X_test)
40/44:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y3_pred = model.predict(X_test)
40/45:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X3 = df1.drop(['Name','AdultWeekend', 'state', 'summit_elev', 'base_elev'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y3 = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X3)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X3)
40/46:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y3 = y3.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X3_train, X3_test, y3_train, y3_test = train_test_split(X3_scaled, y3, test_size=0.25, random_state=1)
40/47:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X3 = df1.drop(['Name','AdultWeekend', 'state', 'summit_elev', 'base_elev'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y3 = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X3)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X3_scaled=scaler.transform(X3)
40/48:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y3 = y3.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X3_train, X3_test, y3_train, y3_test = train_test_split(X3_scaled, y3, test_size=0.25, random_state=1)
40/49:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y3 = y3.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X3_train, X3_test, y3_train, y3_test = train_test_split(X3_scaled, y3, test_size=0.25, random_state=1)
40/50:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y3_pred = model.predict(X_test3)
40/51:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y3_pred = model.predict(X3_test)
40/52:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X2 = df1.drop(['Name','AdultWeekend', 'state'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y2 = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X2)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X2_scaled=scaler.transform(X2)
40/53:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y2 = y2.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X2_train, X2_test, y2_train, y2_test = train_test_split(X2_scaled, y2, test_size=0.25, random_state=1)
40/54:
#all second model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X2_train,y2_train)
40/55:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y2_pred = model.predict(X2_test)
40/56:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
explained_variance_score(y2_test, y2_pred)
40/57: mean_absolute_error(y2_test, y2_pred)
40/58: print(lm.intercept_)
40/59:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X2.columns, columns=['Coefficient'])
coef.sort_values(by='Coefficient', ascending=False)
40/60:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X3 = df1.drop(['Name','AdultWeekend', 'state', 'summit_elev', 'base_elev'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y3 = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X3)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X3_scaled=scaler.transform(X3)
40/61:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y3 = y3.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X3_train, X3_test, y3_train, y3_test = train_test_split(X3_scaled, y3, test_size=0.25, random_state=1)
40/62:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y3 = y3.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X3_train, X3_test, y3_train, y3_test = train_test_split(X3_scaled, y3, test_size=0.25, random_state=1)
40/63:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y3_pred = model.predict(X3_test)
40/64:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
explained_variance_score(y3_test, y3_pred)
40/65:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y3_pred = model.predict(X3_test)
40/66: mean_absolute_error(y3_test, y3_pred)
40/67:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(by='Coefficient', ascending=False)
40/68:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y3_pred = model.predict(X3_test)
40/69:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y3 = y3.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X3_train, X3_test, y3_train, y3_test = train_test_split(X3_scaled, y3, test_size=0.25, random_state=1)
40/70:
#all second model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X3_train,y3_train)
40/71:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y3_pred = model.predict(X3_test)
40/72:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
explained_variance_score(y3_test, y3_pred)
40/73: mean_absolute_error(y3_test, y3_pred)
40/74: print(lm.intercept_)
40/75:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(by='Coefficient', ascending=False)
41/1:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
41/2: os.getcwd()
43/1:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
43/2:
df = pd.read_csv('step3_output.csv')
df.head()
43/3:
#load python packages
import os
import pandas as pd
import datetime
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
43/4: os.getcwd()
43/5:
df = pd.read_csv('step3_output.csv')
df.head()
43/6:
path= '/Users/josephfrasca/Coding Stuff/Springboard/GuidedCapstone/data'
os.chdir(path)
43/7: os.getcwd()
43/8: os.listdir()
43/9:
df = pd.read_csv('step3_output.csv')
df.head()
43/10:
dummy = pd.get_dummies(df1['state'])
dummy.head()
43/11:
dummy = pd.get_dummies(df['state'])
dummy.head()
43/12:
df = pd.concat([df, dummy], axis=1)
df.head()
43/13: df = df.drop(columns=['state'])
43/14: df.head()
43/15:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend',  'state', 'summit_elev', 'base_elev'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
43/16:
df1 = pd.read_csv('step3_output.csv')
df.head()
43/17:
dummy = pd.get_dummies(df1['state'])
dummy.head()
43/18:
df = pd.concat([df1, dummy], axis=1)
df.head()
43/19:
df = pd.concat([df1, dummy], axis=1)
df.head()
43/20: df = df.drop(columns=['state'])
43/21: df.head()
43/22:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df.drop(['Name','AdultWeekend',  'state', 'summit_elev', 'base_elev'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
43/23:
# first we import the preprocessing package from the sklearn library
from sklearn import preprocessing

# Declare an explanatory variable, called X,and assign it the result of dropping 'Name' and 'AdultWeekend' from the df
X = df1.drop(['Name','AdultWeekend',  'state', 'summit_elev', 'base_elev'], axis=1)

# Declare a response variable, called y, and assign it the AdultWeekend column of the df 
y = df.AdultWeekend 

# Here we use the StandardScaler() method of the preprocessing package, and then call the fit() method with parameter X 
scaler = preprocessing.StandardScaler().fit(X)

# Declare a variable called X_scaled, and assign it the result of calling the transform() method with parameter X 
X_scaled=scaler.transform(X)
43/24:
# Import the train_test_split function from the sklearn.model_selection utility.  
from sklearn.model_selection import train_test_split

# Get the 1-dimensional flattened array of our response variable y by calling the ravel() function on y
y = y.ravel()

# Call the train_test_split() function with the first two parameters set to X_scaled and y 
# Declare four variables, X_train, X_test, y_train and y_test separated by commas 
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=1)
43/25:
#all third model set
from sklearn import linear_model
from sklearn.metrics import explained_variance_score,mean_absolute_error
lm = linear_model.LinearRegression()
model = lm.fit(X_train,y_train)
43/26:
# Make a variable called y_pred and assign it the result of calling predict() on our model variable with parameter X_test
y_pred = model.predict(X_test)
43/27:
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_absolute_error
explained_variance_score(y_test, y_pred)
43/28: mean_absolute_error(y_test, y_pred)
43/29: print(lm.intercept_)
43/30:
# You might want to make a pandas DataFrame displaying the coefficients for each state like so: 
coef = pd.DataFrame(abs(lm.coef_), X.columns, columns=['Coefficient'])
coef.sort_values(by='Coefficient', ascending=False)
43/31: df[df['Name'].str.contains('Big Mountain')]
45/1:
#Code task 1#
#Import pandas, matplotlib.pyplot, and seaborn in the correct lines below
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
45/2:
# the supplied CSV data file is the raw_data directory
ski_data = pd.read_csv('raw_data/ski_resort_data.csv')
47/1:
# the supplied CSV data file is the raw_data directory
ski_data = pd.read_csv('raw_data/ski_resort_data.csv')
47/2:
#Code task 1#
#Import pandas, matplotlib.pyplot, and seaborn in the correct lines below
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
47/3:
# the supplied CSV data file is the raw_data directory
ski_data = pd.read_csv('raw_data/ski_resort_data.csv')
47/4:
#change directory to get raw_data directory
os.getcwd()
47/5: os.listdir()
47/6: os.listdir()
47/7:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data'
os.chdir(path)
47/8:
# the supplied CSV data file is the raw_data directory
ski_data = pd.read_csv('raw_data/ski_resort_data.csv')
47/9:
# the supplied CSV data file is the raw_data directory
ski_data = pd.read_csv('raw_data/ski_resort_data.csv')
47/10:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data'
os.chdir(path)
47/11: os.getcwd()
47/12: os.getcwd()
47/13: os.listdir()
47/14:
# the supplied CSV data file is the raw_data directory
ski_data = pd.read_csv('ski_resort_data.csv')
47/15:
#Code task 2#
#Call the info method on ski_data to see a summary of the data
ski_data.info()
47/16:
#Code task 3#
#Call the head method on ski_data to print the first several rows of the data
ski_data.head()
47/17:
#Code task 4#
#Filter the ski_data dataframe to display just the row for our resort with the name 'Big Mountain Resort'
#Hint: you will find that the transpose of the row will give a nicer output. DataFrame's do have a
#transpose method, but you can access this conveniently with the `T` property.
ski_data[ski_data.Name == 'Big Mountain Resort'].T
47/18:
#Code task 5#
#Count (using `.sum()`) the number of missing values (`.isnull()`) in each column of 
#ski_data as well as the percentages (using `.mean()` instead of `.sum()`).
#Order them (increasing or decreasing) using sort_values
#Call `pd.concat` to present these in a single table (DataFrame) with the helpful column names 'count' and '%'
missing = ski_data([ski_data.sum().isnull(), 100 * ski_data.mean().isnull()], axis=1)
missing.columns=[count, %]
missing.pd.concat(by=___)
47/19:
#Code task 5#
#Count (using `.sum()`) the number of missing values (`.isnull()`) in each column of 
#ski_data as well as the percentages (using `.mean()` instead of `.sum()`).
#Order them (increasing or decreasing) using sort_values
#Call `pd.concat` to present these in a single table (DataFrame) with the helpful column names 'count' and '%'
missing = pd.concat([ski_data.isnull().sum(), 100 * ski_data.isnull().mean()], axis=1)
missing.columns=['count', '%']
missing.sort_values(by=___)
47/20:
#Code task 3#
#Call the head method on ski_data to print the first several rows of the data
ski_data.head()
47/21:
#Code task 4#
#Filter the ski_data dataframe to display just the row for our resort with the name 'Big Mountain Resort'
#Hint: you will find that the transpose of the row will give a nicer output. DataFrame's do have a
#transpose method, but you can access this conveniently with the `T` property.
ski_data[ski_data.Name == 'Big Mountain Resort'].T
47/22:
#Code task 5#
#Count (using `.sum()`) the number of missing values (`.isnull()`) in each column of 
#ski_data as well as the percentages (using `.mean()` instead of `.sum()`).
#Order them (increasing or decreasing) using sort_values
#Call `pd.concat` to present these in a single table (DataFrame) with the helpful column names 'count' and '%'
missing = pd.concat([ski_data.isnull().sum(), 100 * ski_data.isnull().mean()], axis=1)
missing.columns=['count', '%']
missing.sort_values(by='count')
47/23:
#Code task 5#
#Count (using `.sum()`) the number of missing values (`.isnull()`) in each column of 
#ski_data as well as the percentages (using `.mean()` instead of `.sum()`).
#Order them (increasing or decreasing) using sort_values
#Call `pd.concat` to present these in a single table (DataFrame) with the helpful column names 'count' and '%'
missing = pd.concat([ski_data.isnull().sum(), 100 * ski_data.isnull().mean()], axis=1)
missing.columns=['count', '%']
missing.sort_values(by='count', ascending=False)
47/24:
#Code task 6#
#Use ski_data's `select_dtypes` method to select columns of dtype 'object'
ski_data.select_dtypes('object')
47/25:
#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].value_counts.head()
47/26:
#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].value_counts
47/27:
#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].value_counts.count()
47/28:
#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].value_count
47/29:
#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].value_counts
47/30:
#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].value_counts.head()
47/31:
#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].value_counts
47/32:
#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].value_counts()
47/33:
#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].value_counts().head()
47/34:
#Code task 8#
#Concatenate the string columns 'Name' and 'Region' and count the values again (as above)
(ski_data['Name'] + ', ' + ski_data['Region']).value_counts().head()
47/35:
#Code task 9#
#Concatenate 'Name' and 'state' and count the values again (as above)
(ski_data['Name'] + ', ' + ski_data['State']).value_counts().head()
47/36:
#Code task 9#
#Concatenate 'Name' and 'state' and count the values again (as above)
(ski_data['Name'] + ', ' + ski_data['state']).value_counts().head()
47/37: **NB** because you know `value_counts()` sorts descending, you can use the `head()` method and know the rest of the counts must be 1.
47/38: ski_data[ski_data['Name'] == 'Crystal Mountain']
47/39:
#Code task 10#
#Calculate the number of times Region does not equal state
(ski_data.Region == ski_data.state).sum()
47/40:
#Code task 10#
#Calculate the number of times Region does not equal state
(ski_data.Region == ski_data.state).count()
47/41:
#Code task 10#
#Calculate the number of times Region does not equal state
(ski_data.Region != ski_data.state).count()
47/42:
#Code task 10#
#Calculate the number of times Region does not equal state
(ski_data.Region != ski_data.state).sum()
47/43: ski_data['Region'].value_counts()
47/44:
#Code task 11#
#Filter the ski_data dataframe for rows where 'Region' and 'state' are different,
#group that by 'state' and perform `value_counts` on the 'Region'
(ski_data[ski_data.Region != ski_data.state]
 .groupby('state')['Region']
 .value_counts())
47/45:
#Code task 12#
#Select the 'Region' and 'state' columns from ski_data and use the `nunique` method to calculate
#the number of unique values in each
ski_data[['Region', 'state']].nunique()
47/46:
#Code task 13#
#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)
fig, ax = plt.subplots(1, 2, figsize=(12, 8))
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
ax[0].set_title('Region')
#Label the xaxis 'Count'
ax[0].set_xlabel('Count')
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.state.value_counts().plot(kind=barh, ax=ax[1])
#Give the plot a helpful title of 'state'
ax[1].set_title('state')
#Label the xaxis 'Count'
ax[1].set_xlabel('Count')
#Give the subplots a little "breathing room" with a wspace of 0.5
plt.subplots_adjust(wspace=0.5);
#You're encouraged to explore a few different figure sizes, orientations, and spacing here
# as the importance of easy-to-read and informative figures is frequently understated
# and you will find the ability to tweak figures invaluable later on
47/47:
#Code task 13#
#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)
fig, ax = plt.subplots(1, 2, figsize=(12, 8))
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
ax[0].set_title('Region')
#Label the xaxis 'Count'
ax[0].set_xlabel('Count')
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.state.value_counts().plot(kind='barh', ax=ax[1])
#Give the plot a helpful title of 'state'
ax[1].set_title('state')
#Label the xaxis 'Count'
ax[1].set_xlabel('Count')
#Give the subplots a little "breathing room" with a wspace of 0.5
plt.subplots_adjust(wspace=0.5);
#You're encouraged to explore a few different figure sizes, orientations, and spacing here
# as the importance of easy-to-read and informative figures is frequently understated
# and you will find the ability to tweak figures invaluable later on
47/48:
#Code task 13#
#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)
fig, ax = plt.subplots(1, 2, figsize=(12, 8))
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
ax[0].set_title('Region')
#Label the xaxis 'Count'
ax[0].set_xlabel('Count')
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.state.value_counts().plot(kind='barh', ax=ax[1])
#Give the plot a helpful title of 'state'
ax[1].set_title('state')
#Label the xaxis 'Count'
ax[1].set_xlabel('Count')
#Give the subplots a little "breathing room" with a wspace of 0.5
plt.subplots_adjust(wspace=0.7);
#You're encouraged to explore a few different figure sizes, orientations, and spacing here
# as the importance of easy-to-read and informative figures is frequently understated
# and you will find the ability to tweak figures invaluable later on
47/49:
#Code task 13#
#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)
fig, ax = plt.subplots(1, 2, figsize=(12, 8))
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
ax[0].set_title('Region')
#Label the xaxis 'Count'
ax[0].set_xlabel('Count')
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.state.value_counts().plot(kind='barh', ax=ax[1])
#Give the plot a helpful title of 'state'
ax[1].set_title('state')
#Label the xaxis 'Count'
ax[1].set_xlabel('Count')
#Give the subplots a little "breathing room" with a wspace of 0.5
plt.subplots_adjust(wspace=0.1);
#You're encouraged to explore a few different figure sizes, orientations, and spacing here
# as the importance of easy-to-read and informative figures is frequently understated
# and you will find the ability to tweak figures invaluable later on
47/50:
#Code task 13#
#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)
fig, ax = plt.subplots(1, 2, figsize=(12, 8))
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
ax[0].set_title('Region')
#Label the xaxis 'Count'
ax[0].set_xlabel('Count')
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.state.value_counts().plot(kind='barh', ax=ax[1])
#Give the plot a helpful title of 'state'
ax[1].set_title('state')
#Label the xaxis 'Count'
ax[1].set_xlabel('Count')
#Give the subplots a little "breathing room" with a wspace of 0.5
plt.subplots_adjust(wspace=0.5);
#You're encouraged to explore a few different figure sizes, orientations, and spacing here
# as the importance of easy-to-read and informative figures is frequently understated
# and you will find the ability to tweak figures invaluable later on
47/51:
#Code task 13#
#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)
fig, ax = plt.subplots(1, 2, figsize=(12, 8))
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
ax[0].set_title('Region')
#Label the xaxis 'Count'
ax[0].set_xlabel('Count')
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.state.value_counts().plot(kind='barh', ax=ax[1])
#Give the plot a helpful title of 'state'
ax[1].set_title('state')
#Label the xaxis 'Count'
ax[1].set_xlabel('Count')
#Give the subplots a little "breathing room" with a wspace of 0.5
plt.subplots_adjust(wspace=0.9);
#You're encouraged to explore a few different figure sizes, orientations, and spacing here
# as the importance of easy-to-read and informative figures is frequently understated
# and you will find the ability to tweak figures invaluable later on
47/52:
#Code task 13#
#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)
fig, ax = plt.subplots(1, 2, figsize=(12, 8))
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
ax[0].set_title('Region')
#Label the xaxis 'Count'
ax[0].set_xlabel('Count')
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.state.value_counts().plot(kind='barh', ax=ax[1])
#Give the plot a helpful title of 'state'
ax[1].set_title('state')
#Label the xaxis 'Count'
ax[1].set_xlabel('Count')
#Give the subplots a little "breathing room" with a wspace of 0.5
plt.subplots_adjust(wspace=0.5);
#You're encouraged to explore a few different figure sizes, orientations, and spacing here
# as the importance of easy-to-read and informative figures is frequently understated
# and you will find the ability to tweak figures invaluable later on
47/53:
#Code task 13#
#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)
fig, ax = plt.subplots(1, 2, figsize=(1, 8))
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
ax[0].set_title('Region')
#Label the xaxis 'Count'
ax[0].set_xlabel('Count')
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.state.value_counts().plot(kind='barh', ax=ax[1])
#Give the plot a helpful title of 'state'
ax[1].set_title('state')
#Label the xaxis 'Count'
ax[1].set_xlabel('Count')
#Give the subplots a little "breathing room" with a wspace of 0.5
plt.subplots_adjust(wspace=0.5);
#You're encouraged to explore a few different figure sizes, orientations, and spacing here
# as the importance of easy-to-read and informative figures is frequently understated
# and you will find the ability to tweak figures invaluable later on
47/54:
#Code task 13#
#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)
fig, ax = plt.subplots(1, 2, figsize=(12, 1))
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
ax[0].set_title('Region')
#Label the xaxis 'Count'
ax[0].set_xlabel('Count')
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.state.value_counts().plot(kind='barh', ax=ax[1])
#Give the plot a helpful title of 'state'
ax[1].set_title('state')
#Label the xaxis 'Count'
ax[1].set_xlabel('Count')
#Give the subplots a little "breathing room" with a wspace of 0.5
plt.subplots_adjust(wspace=0.5);
#You're encouraged to explore a few different figure sizes, orientations, and spacing here
# as the importance of easy-to-read and informative figures is frequently understated
# and you will find the ability to tweak figures invaluable later on
47/55:
#Code task 13#
#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)
fig, ax = plt.subplots(1, 2, figsize=(12, 8))
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
ax[0].set_title('Region')
#Label the xaxis 'Count'
ax[0].set_xlabel('Count')
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.state.value_counts().plot(kind='barh', ax=ax[1])
#Give the plot a helpful title of 'state'
ax[1].set_title('state')
#Label the xaxis 'Count'
ax[1].set_xlabel('Count')
#Give the subplots a little "breathing room" with a wspace of 0.5
plt.subplots_adjust(wspace=0.5);
#You're encouraged to explore a few different figure sizes, orientations, and spacing here
# as the importance of easy-to-read and informative figures is frequently understated
# and you will find the ability to tweak figures invaluable later on
47/56:
#Code task 14#
# Calculate average weekday and weekend price by state and sort by the average of the two
# Hint: use the pattern dataframe.groupby(<grouping variable>)[<list of columns>].mean()
state_price_means = ski_data.groupby('state')[['AdultWeekend', 'AdultWeekday']].mean()
state_price_means.head()
47/57:
# The next bit simply reorders the index by increasing average of weekday and weekend prices
# Compare the index order you get from
# state_price_means.index
# with
# state_price_means.mean(axis=1).sort_values(ascending=False).index
# See how this expression simply sits within the reindex()
(state_price_means.reindex(index=state_price_means.mean(axis=1)
    .sort_values(ascending=False)
    .index)
    .plot(kind='barh', figsize=(10, 10), title='Average ticket price by State'))
plt.xlabel('Price ($)');
47/58:
#Code task 15#
#Use the pd.melt function, pass in the ski_data columns 'state', 'AdultWeekday', and 'Adultweekend' only,
#specify 'state' for `id_vars`
#gather the ticket prices from the 'Adultweekday' and 'AdultWeekend' columns using the `value_vars` argument,
#call the resultant price column 'Price' via the `value_name` argument,
#name the weekday/weekend indicator column 'Ticket' via the `var_name` argument
ticket_prices = pd.melt(ski_data[['state', 'AdultWeekday', 'Adultweekend']], 
                        id_vars='state', 
                        var_name='Ticket', 
                        value_vars=['AdultWeekday', 'Adultweekend'], 
                        value_name='Price')
47/59:
#Code task 15#
#Use the pd.melt function, pass in the ski_data columns 'state', 'AdultWeekday', and 'Adultweekend' only,
#specify 'state' for `id_vars`
#gather the ticket prices from the 'Adultweekday' and 'AdultWeekend' columns using the `value_vars` argument,
#call the resultant price column 'Price' via the `value_name` argument,
#name the weekday/weekend indicator column 'Ticket' via the `var_name` argument
ticket_prices = pd.melt(ski_data[['state', 'AdultWeekday', 'AdultWeekend']], 
                        id_vars='state', 
                        var_name='Ticket', 
                        value_vars=['AdultWeekday', 'AdultWeekend'], 
                        value_name='Price')
47/60: ticket_prices.head()
47/61:
#Code task 16#
#Create a seaborn boxplot of the ticket price dataframe we created above,
#with 'state' on the x-axis, 'Price' as the y-value, and a hue that indicates 'Ticket'
#This will use boxplot's x, y, hue, and data arguments.
plt.subplots(figsize=(12, 8))
sns.boxplot(x='state', y='Price', hue='Ticket', data=ticket_prices)
plt.xticks(rotation='vertical')
plt.ylabel('Price ($)')
plt.xlabel('State');
47/62:
#Code task 16#
#Create a seaborn boxplot of the ticket price dataframe we created above,
#with 'state' on the x-axis, 'Price' as the y-value, and a hue that indicates 'Ticket'
#This will use boxplot's x, y, hue, and data arguments.
plt.subplots(figsize=(12, 8))
sns.boxplot(x='state', y='Price', hue='Ticket', data=ticket_prices)
plt.xticks(rotation='vertical')
plt.ylabel('Price ($)')
plt.xlabel('State');
48/1:
#Code task 1#
#Import pandas, matplotlib.pyplot, and seaborn in the correct lines below
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
48/2:
#change directory to get raw_data directory
os.getcwd()
48/3: os.listdir()
48/4:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data'
os.chdir(path)
48/5: os.listdir()
48/6:
# the supplied CSV data file is the raw_data directory
ski_data = pd.read_csv('ski_resort_data.csv')
48/7:
#Code task 2#
#Call the info method on ski_data to see a summary of the data
ski_data.info()
48/8:
#Code task 3#
#Call the head method on ski_data to print the first several rows of the data
ski_data.head()
48/9:
#Code task 4#
#Filter the ski_data dataframe to display just the row for our resort with the name 'Big Mountain Resort'
#Hint: you will find that the transpose of the row will give a nicer output. DataFrame's do have a
#transpose method, but you can access this conveniently with the `T` property.
ski_data[ski_data.Name == 'Big Mountain Resort'].T
48/10:
#Code task 5#
#Count (using `.sum()`) the number of missing values (`.isnull()`) in each column of 
#ski_data as well as the percentages (using `.mean()` instead of `.sum()`).
#Order them (increasing or decreasing) using sort_values
#Call `pd.concat` to present these in a single table (DataFrame) with the helpful column names 'count' and '%'
missing = pd.concat([ski_data.isnull().sum(), 100 * ski_data.isnull().mean()], axis=1)
missing.columns=['count', '%']
missing.sort_values(by='count', ascending=False)
48/11:
#Code task 6#
#Use ski_data's `select_dtypes` method to select columns of dtype 'object'
ski_data.select_dtypes('object')
48/12:
#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].value_counts().head()
48/13:
#Code task 8#
#Concatenate the string columns 'Name' and 'Region' and count the values again (as above)
(ski_data['Name'] + ', ' + ski_data['Region']).value_counts().head()
48/14:
#Code task 9#
#Concatenate 'Name' and 'state' and count the values again (as above)
(ski_data['Name'] + ', ' + ski_data['state']).value_counts().head()
48/15: **NB** because you know `value_counts()` sorts descending, you can use the `head()` method and know the rest of the counts must be 1.
48/16:
#Code task 17#
#Call ski_data's `describe` method for a statistical summary of the numerical columns
#Hint: there are fewer summary stat columns than features, so displaying the transpose
#will be useful again
ski_data.describe.T
48/17:
#Code task 17#
#Call ski_data's `describe` method for a statistical summary of the numerical columns
#Hint: there are fewer summary stat columns than features, so displaying the transpose
#will be useful again
ski_data.describe().T
48/18:
missing_price = ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)
missing_price.value_counts()/len(missing_price) * 100
48/19:
#Code task 18#
#Call ski_data's `hist` method to plot histograms of each of the numeric features
#Try passing it an argument figsize=(15,10)
#Try calling plt.subplots_adjust() with an argument hspace=0.5 to adjust the spacing
#It's important you create legible and easy-to-read plots
ski_data.hist(figsize=(15,10))
#plt.subplots_adjust(hspace=___);
#Hint: notice how the terminating ';' "swallows" some messy output and leads to a tidier notebook
48/20:
#Code task 18#
#Call ski_data's `hist` method to plot histograms of each of the numeric features
#Try passing it an argument figsize=(15,10)
#Try calling plt.subplots_adjust() with an argument hspace=0.5 to adjust the spacing
#It's important you create legible and easy-to-read plots
ski_data.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
#Hint: notice how the terminating ';' "swallows" some messy output and leads to a tidier notebook
48/21:
#Code task 18#
#Call ski_data's `hist` method to plot histograms of each of the numeric features
#Try passing it an argument figsize=(15,10)
#Try calling plt.subplots_adjust() with an argument hspace=0.5 to adjust the spacing
#It's important you create legible and easy-to-read plots
ski_data.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5)
#Hint: notice how the terminating ';' "swallows" some messy output and leads to a tidier notebook
48/22:
#Code task 18#
#Call ski_data's `hist` method to plot histograms of each of the numeric features
#Try passing it an argument figsize=(15,10)
#Try calling plt.subplots_adjust() with an argument hspace=0.5 to adjust the spacing
#It's important you create legible and easy-to-read plots
ski_data.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
#Hint: notice how the terminating ';' "swallows" some messy output and leads to a tidier notebook
48/23:
#Code task 19#
#Filter the 'SkiableTerrain_ac' column to print the values greater than 10000
ski_data.'SkiableTerrain_ac'[ski_data.SkiableTerrain_ac > 10000]
48/24:
#Code task 19#
#Filter the 'SkiableTerrain_ac' column to print the values greater than 10000
ski_data.loc[ski_data.SkiableTerrain_ac > 10000]
48/25:
#Code task 19#
#Filter the 'SkiableTerrain_ac' column to print the values greater than 10000
ski_data.SkiableTerrain_ac[ski_data.SkiableTerrain_ac > 10000]
48/26:
#Code task 20#
#Now you know there's only one, print the whole row to investigate all values, including seeing the resort name
#Hint: don't forget the transpose will be helpful here
ski_data[ski_data.SkiableTerrain_ac > 10000].T
48/27:
#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].value_counts().head()
48/28:
#Code task 8#
#Concatenate the string columns 'Name' and 'Region' and count the values again (as above)
(ski_data['Name'] + ', ' + ski_data['Region']).value_counts().head()
48/29:
#Code task 9#
#Concatenate 'Name' and 'state' and count the values again (as above)
(ski_data['Name'] + ', ' + ski_data['state']).value_counts().head()
48/30:
#Code task 21#
#Use the .loc accessor to print the 'SkiableTerrain_ac' value only for this resort
ski_data.loc[39, 'SkiableTerrain_ac']
48/31:
#Code task 22#
#Use the .loc accessor again to modify this value with the correct value of 1819
ski_data.loc[39, 'SkiableTerrain_ac'] = 1819
48/32:
#Code task 23#
#Use the .loc accessor a final time to verify that the value has been modified
ski_data.loc[39, 'SkiableTerrain_ac']
48/33:
ski_data.SkiableTerrain_ac.hist(bins=30)
plt.xlabel('SkiableTerrain_ac')
plt.ylabel('Count')
plt.title('Distribution of skiable area (acres) after replacing erroneous value');
48/34: ski_data['Snow Making_ac'][ski_data['Snow Making_ac'] > 1000]
48/35: ski_data[ski_data['Snow Making_ac'] > 3000].T
48/36: ski_data.fastEight.value_counts()
48/37:
#Code task 24#
#Drop the 'fastEight' column from ski_data. Use inplace=True
ski_data.drop(columns='fastEight', inplace=True)
48/38:
#Code task 25#
#Filter the 'yearsOpen' column for values greater than 100
ski_data.loc[ski_data.yearsOpen > 100]
48/39:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.loc[ski_data.yearsOpen < 1000].hist(bins=300)
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
48/40:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.loc[ski_data.yearsOpen < 1000].hist(bins=30)
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
48/41:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.hist[ski_data.yearsOpen < 1000].hist(bins=30)
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
48/42:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.loc[ski_data.yearsOpen < 1000].hist(bins=30)
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
48/43:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.filter[ski_data.yearsOpen < 1000].hist(bins=30)
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
48/44:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.loc[ski_data.yearsOpen < 1000].hist(bins=30)
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
48/45:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.loc[ski_data.yearsOpen < 1000].hist(bins=30, figsize=(15,10))
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
48/46:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.loc[ski_data.yearsOpen < 1000].hist(bins=30, figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
48/47: ski_data.yearsOpen[ski_data.yearsOpen < 1000].describe()
48/48:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.yearsOpen[ski_data.yearsOpen < 1000].hist(bins=30, figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
48/49:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.yearsOpen[ski_data.yearsOpen < 1000].hist(bins=30)
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
48/50:
#Code task 25#
#Filter the 'yearsOpen' column for values greater than 100
ski_data.yearsOpen[ski_data.yearsOpen > 100]
48/51:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.yearsOpen[ski_data.yearsOpen < 1000].hist(bins=300)
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
48/52:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.yearsOpen[ski_data.yearsOpen < 1000].hist(bins=30)
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
48/53: ski_data.yearsOpen[ski_data.yearsOpen < 1000].describe()
48/54: ski_data = ski_data[ski_data.yearsOpen < 1000]
48/55:
#Code task 27#
#Add named aggregations for the sum of 'daysOpenLastYear', 'TerrainParks', and 'NightSkiing_ac'
#call them 'state_total_days_open', 'state_total_terrain_parks', and 'state_total_nightskiing_ac',
#respectively
#Finally, add a call to the reset_index() method (we recommend you experiment with and without this to see
#what it does)
state_summary = ski_data.groupby('state').agg(
    resorts_per_state=pd.NamedAgg(column='Name', aggfunc='size'), #could pick any column here
    state_total_skiable_area_ac=pd.NamedAgg(column='SkiableTerrain_ac', aggfunc='sum'),
    state_total_days_open=pd.NamedAgg(column='daysOpenLastYear', aggfunc='sum'),
    'state_total_terrain_parks'=pd.NamedAgg(column='TerrainParks', aggfunc='sum'),
    'state_total_nightskiing_ac'=pd.NamedAgg(column='NightSkiing_ac', aggfunc='sum')
)
state_summary.head()
48/56:
#Code task 27#
#Add named aggregations for the sum of 'daysOpenLastYear', 'TerrainParks', and 'NightSkiing_ac'
#call them 'state_total_days_open', 'state_total_terrain_parks', and 'state_total_nightskiing_ac',
#respectively
#Finally, add a call to the reset_index() method (we recommend you experiment with and without this to see
#what it does)
state_summary = ski_data.groupby('state').agg(
    resorts_per_state=pd.NamedAgg(column='Name', aggfunc='size'), #could pick any column here
    state_total_skiable_area_ac=pd.NamedAgg(column='SkiableTerrain_ac', aggfunc='sum'),
    state_total_days_open=pd.NamedAgg(column='daysOpenLastYear', aggfunc='sum'),
    state_total_terrain_parks=pd.NamedAgg(column='TerrainParks', aggfunc='sum'),
    state_total_nightskiing_ac=pd.NamedAgg(column='NightSkiing_ac', aggfunc='sum')
)
state_summary.head()
48/57:
#Code task 27#
#Add named aggregations for the sum of 'daysOpenLastYear', 'TerrainParks', and 'NightSkiing_ac'
#call them 'state_total_days_open', 'state_total_terrain_parks', and 'state_total_nightskiing_ac',
#respectively
#Finally, add a call to the reset_index() method (we recommend you experiment with and without this to see
#what it does)
state_summary = ski_data.groupby('state').agg(
    resorts_per_state=pd.NamedAgg(column='Name', aggfunc='size'), #could pick any column here
    state_total_skiable_area_ac=pd.NamedAgg(column='SkiableTerrain_ac', aggfunc='sum'),
    state_total_days_open=pd.NamedAgg(column='daysOpenLastYear', aggfunc='sum'),
    state_total_terrain_parks=pd.NamedAgg(column='TerrainParks', aggfunc='sum'),
    state_total_nightskiing_ac=pd.NamedAgg(column='NightSkiing_ac', aggfunc='sum')
).reset_index()
state_summary.head()
48/58:
missing_price = ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)
missing_price.value_counts()/len(missing_price) * 100
48/59:
#Code task 28#
#Use `missing_price` to remove rows from ski_data where both price values are missing
ski_data = ski_data[ski_data.missing_price != 2]
48/60:
#Code task 28#
#Use `missing_price` to remove rows from ski_data where both price values are missing
ski_data = ski_data[missing_price != 2]
48/61:
ski_data.hist(figsize=(15, 10))
plt.subplots_adjust(hspace=0.5);
48/62:
#Code task 29#
#Use pandas' `read_html` method to read the table from the URL below
states_url = 'https://simple.wikipedia.org/wiki/List_of_U.S._states'
usa_states = pd.read_html(states_url)
48/63: type(usa_states)
48/64: len(usa_states)
48/65:
usa_states = usa_states[0]
usa_states.head()
48/66:
#Code task 30#
#Use the iloc accessor to get the pandas Series for column number 4 from `usa_states`
#It should be a column of dates
established = usa_sates.iloc[:, 4]
48/67:
#Code task 30#
#Use the iloc accessor to get the pandas Series for column number 4 from `usa_states`
#It should be a column of dates
established = usa_states.iloc[:, 4]
48/68:
#Code task 30#
#Use the iloc accessor to get the pandas Series for column number 4 from `usa_states`
#It should be a column of dates
established = usa_states.iloc[:, 4]
48/69: established
48/70:
#Code task 31#
#Now use the iloc accessor again to extract columns 0, 5, and 6 and the dataframe's `copy()` method
#Set the names of these extracted columns to 'state', 'state_population', and 'state_area_sq_miles',
#respectively.
usa_states_sub = usa_states.iloc[:, [:, 0, 5, 6]].copy()
usa_states_sub.columns = ['state', 'state_population', 'state_area_sq_miles']
usa_states_sub.head()
48/71:
#Code task 31#
#Now use the iloc accessor again to extract columns 0, 5, and 6 and the dataframe's `copy()` method
#Set the names of these extracted columns to 'state', 'state_population', and 'state_area_sq_miles',
#respectively.
usa_states_sub = usa_states.iloc[:, [0, 5, 6]].copy()
usa_states_sub.columns = ['state', 'state_population', 'state_area_sq_miles']
usa_states_sub.head()
48/72:
#Code task 32#
#Find the states in `state_summary` that are not in `usa_states_sub`
#Hint: set(list1) - set(list2) is an easy way to get items in list1 that are not in list2
missing_states = set(state_summary.state) - set(usa_states_sub.state)
missing_states
48/73: usa_states_sub.state[usa_states_sub.state.str.contains('Massachusetts|Pennsylvania|Rhode Island|Virginia')]
48/74:
#Code task 33#
#Use pandas' Series' `replace()` method to replace anything within square brackets (including the brackets)
#with the empty string. Do this inplace, so you need to specify the arguments:
#to_replace='\[.*\]' #literal square bracket followed by anything or nothing followed by literal closing bracket
#value='' #empty string as replacement
#regex=True #we used a regex in our `to_replace` argument
#inplace=True #Do this "in place"
usa_states_sub.state.replace(to_replace='\[.*\]', value='', regex=True, inplace=True)
usa_states_sub.state[usa_states_sub.state.str.contains('Massachusetts|Pennsylvania|Rhode Island|Virginia')]
48/75:
#Code task 34#
#And now verify none of our states are missing by checking that there are no states in
#state_summary that are not in usa_states_sub (as earlier using `set()`)
missing_states = set(state_summary.state) - set(usa_states_sub.state)
missing_states
48/76:
#Code task 35#
#Use 'state_summary's `merge()` method to combine our new data in 'usa_states_sub'
#specify the arguments how='left' and on='state'
state_summary = state_summary.merge(usa_states_sub, how='left', on='state')
state_summary.head()
48/77:
#Code task 36#
#Use ski_data's `plot()` method to create a scatterplot (kind='scatter') with 'AdultWeekday' on the x-axis and
#'AdultWeekend' on the y-axis
ski_data.plot(x='AdultWeekday', y='AdultWeekend', kind='scatter');
48/78:
#Code task 37#
#Use the loc accessor on ski_data to print the 'AdultWeekend' and 'AdultWeekday' columns for Montana only
ski_data..loc[ski_data.state == 'Montana', ['AdultWeekend', 'AdultWeekday']]
48/79:
#Code task 37#
#Use the loc accessor on ski_data to print the 'AdultWeekend' and 'AdultWeekday' columns for Montana only
ski_data.loc[ski_data.state == 'Montana', ['AdultWeekend', 'AdultWeekday']]
48/80: ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum()
48/81:
ski_data.drop(columns='AdultWeekday', inplace=True)
ski_data.dropna(subset=['AdultWeekend'], inplace=True)
48/82: ski_data.shape
48/83: ski_data.shape
48/84: ski_data.head()
48/85:
missing = pd.concat([ski_data.isnull().sum(axis=1), 100 * ski_data.isnull().mean(axis=1)], axis=1)
missing.columns=['count', '%']
missing.sort_values(by='count', ascending=False).head(10)
48/86: missing['%'].unique()
48/87: missing['%'].value_counts()
48/88: ski_data.info()
48/89: ski_data.shape
48/90:
datapath = 'data'
# renaming the output data directory and re-running this notebook, for example,
# will recreate this (empty) directory and resave the data files.
# NB this is not a substitute for a modern data pipeline, for which there are
# various tools. However, for our purposes here, and often in a "one off" analysis,
# this is useful because we have to deliberately move/delete our data in order
# to overwrite it.
if not os.path.exists(datapath):
    os.mkdir(datapath)
48/91:
datapath_skidata = os.path.join(datapath, 'ski_data_cleaned.csv')
if not os.path.exists(datapath_skidata):
    ski_data.to_csv(datapath_skidata, index=False)
48/92:
datapath_states = os.path.join(datapath, 'state_summary.csv')
if not os.path.exists(datapath_states):
    state_summary.to_csv(datapath_states, index=False)
49/1:
#Code task 1#
#Import pandas, matplotlib.pyplot, and seaborn in the correct lines below
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
49/2:
#change directory to get raw_data directory
os.getcwd()
49/3: os.listdir()
49/4:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data'
os.chdir(path)
49/5: os.listdir()
49/6:
# the supplied CSV data file is the raw_data directory
ski_data = pd.read_csv('ski_resort_data.csv')
49/7:
#Code task 2#
#Call the info method on ski_data to see a summary of the data
ski_data.info()
49/8:
#Code task 2#
#Call the info method on ski_data to see a summary of the data
ski_data.info()
49/9:
#Code task 2#
#Call the info method on ski_data to see a summary of the data
ski_data.info()
49/10:
#Code task 3#
#Call the head method on ski_data to print the first several rows of the data
ski_data.head()
49/11:
#Code task 4#
#Filter the ski_data dataframe to display just the row for our resort with the name 'Big Mountain Resort'
#Hint: you will find that the transpose of the row will give a nicer output. DataFrame's do have a
#transpose method, but you can access this conveniently with the `T` property.
ski_data[ski_data.Name == 'Big Mountain Resort'].T
49/12:
#Code task 5#
#Count (using `.sum()`) the number of missing values (`.isnull()`) in each column of 
#ski_data as well as the percentages (using `.mean()` instead of `.sum()`).
#Order them (increasing or decreasing) using sort_values
#Call `pd.concat` to present these in a single table (DataFrame) with the helpful column names 'count' and '%'
missing = pd.concat([ski_data.isnull().sum(), 100 * ski_data.isnull().mean()], axis=1)
missing.columns=['count', '%']
missing.sort_values(by='count', ascending=False)
49/13:
#Code task 6#
#Use ski_data's `select_dtypes` method to select columns of dtype 'object'
ski_data.select_dtypes('object')
49/14:
#Code task 7#
#Use pandas' Series method `value_counts` to find any duplicated resort names
ski_data['Name'].value_counts().head()
49/15:
#Code task 8#
#Concatenate the string columns 'Name' and 'Region' and count the values again (as above)
(ski_data['Name'] + ', ' + ski_data['Region']).value_counts().head()
49/16:
#Code task 9#
#Concatenate 'Name' and 'state' and count the values again (as above)
(ski_data['Name'] + ', ' + ski_data['state']).value_counts().head()
49/17: ski_data[ski_data['Name'] == 'Crystal Mountain']
49/18:
#Code task 10#
#Calculate the number of times Region does not equal state
(ski_data.Region != ski_data.state).sum()
49/19: ski_data['Region'].value_counts()
49/20:
#Code task 11#
#Filter the ski_data dataframe for rows where 'Region' and 'state' are different,
#group that by 'state' and perform `value_counts` on the 'Region'
(ski_data[ski_data.Region != ski_data.state]
 .groupby('state')['Region']
 .value_counts())
49/21:
#Code task 12#
#Select the 'Region' and 'state' columns from ski_data and use the `nunique` method to calculate
#the number of unique values in each
ski_data[['Region', 'state']].nunique()
49/22:
#Code task 13#
#Create two subplots on 1 row and 2 columns with a figsize of (12, 8)
fig, ax = plt.subplots(1, 2, figsize=(12, 8))
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.Region.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
ax[0].set_title('Region')
#Label the xaxis 'Count'
ax[0].set_xlabel('Count')
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
ski_data.state.value_counts().plot(kind='barh', ax=ax[1])
#Give the plot a helpful title of 'state'
ax[1].set_title('state')
#Label the xaxis 'Count'
ax[1].set_xlabel('Count')
#Give the subplots a little "breathing room" with a wspace of 0.5
plt.subplots_adjust(wspace=0.5);
#You're encouraged to explore a few different figure sizes, orientations, and spacing here
# as the importance of easy-to-read and informative figures is frequently understated
# and you will find the ability to tweak figures invaluable later on
49/23:
#Code task 14#
# Calculate average weekday and weekend price by state and sort by the average of the two
# Hint: use the pattern dataframe.groupby(<grouping variable>)[<list of columns>].mean()
state_price_means = ski_data.groupby('state')[['AdultWeekend', 'AdultWeekday']].mean()
state_price_means.head()
49/24:
# The next bit simply reorders the index by increasing average of weekday and weekend prices
# Compare the index order you get from
# state_price_means.index
# with
# state_price_means.mean(axis=1).sort_values(ascending=False).index
# See how this expression simply sits within the reindex()
(state_price_means.reindex(index=state_price_means.mean(axis=1)
    .sort_values(ascending=False)
    .index)
    .plot(kind='barh', figsize=(10, 10), title='Average ticket price by State'))
plt.xlabel('Price ($)');
49/25:
#Code task 15#
#Use the pd.melt function, pass in the ski_data columns 'state', 'AdultWeekday', and 'Adultweekend' only,
#specify 'state' for `id_vars`
#gather the ticket prices from the 'Adultweekday' and 'AdultWeekend' columns using the `value_vars` argument,
#call the resultant price column 'Price' via the `value_name` argument,
#name the weekday/weekend indicator column 'Ticket' via the `var_name` argument
ticket_prices = pd.melt(ski_data[['state', 'AdultWeekday', 'AdultWeekend']], 
                        id_vars='state', 
                        var_name='Ticket', 
                        value_vars=['AdultWeekday', 'AdultWeekend'], 
                        value_name='Price')
49/26: ticket_prices.head()
49/27:
#Code task 16#
#Create a seaborn boxplot of the ticket price dataframe we created above,
#with 'state' on the x-axis, 'Price' as the y-value, and a hue that indicates 'Ticket'
#This will use boxplot's x, y, hue, and data arguments.
plt.subplots(figsize=(12, 8))
sns.boxplot(x='state', y='Price', hue='Ticket', data=ticket_prices)
plt.xticks(rotation='vertical')
plt.ylabel('Price ($)')
plt.xlabel('State');
49/28:
#Code task 17#
#Call ski_data's `describe` method for a statistical summary of the numerical columns
#Hint: there are fewer summary stat columns than features, so displaying the transpose
#will be useful again
ski_data.describe().T
49/29:
missing_price = ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)
missing_price.value_counts()/len(missing_price) * 100
49/30:
#Code task 18#
#Call ski_data's `hist` method to plot histograms of each of the numeric features
#Try passing it an argument figsize=(15,10)
#Try calling plt.subplots_adjust() with an argument hspace=0.5 to adjust the spacing
#It's important you create legible and easy-to-read plots
ski_data.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
#Hint: notice how the terminating ';' "swallows" some messy output and leads to a tidier notebook
49/31:
#Code task 19#
#Filter the 'SkiableTerrain_ac' column to print the values greater than 10000
ski_data.SkiableTerrain_ac[ski_data.SkiableTerrain_ac > 10000]
49/32:
#Code task 20#
#Now you know there's only one, print the whole row to investigate all values, including seeing the resort name
#Hint: don't forget the transpose will be helpful here
ski_data[ski_data.SkiableTerrain_ac > 10000].T
49/33:
#Code task 21#
#Use the .loc accessor to print the 'SkiableTerrain_ac' value only for this resort
ski_data.loc[39, 'SkiableTerrain_ac']
49/34:
#Code task 22#
#Use the .loc accessor again to modify this value with the correct value of 1819
ski_data.loc[39, 'SkiableTerrain_ac'] = 1819
49/35:
#Code task 23#
#Use the .loc accessor a final time to verify that the value has been modified
ski_data.loc[39, 'SkiableTerrain_ac']
49/36:
ski_data.SkiableTerrain_ac.hist(bins=30)
plt.xlabel('SkiableTerrain_ac')
plt.ylabel('Count')
plt.title('Distribution of skiable area (acres) after replacing erroneous value');
49/37: ski_data['Snow Making_ac'][ski_data['Snow Making_ac'] > 1000]
49/38: ski_data[ski_data['Snow Making_ac'] > 3000].T
49/39: .6 * 4800
49/40: ski_data.fastEight.value_counts()
49/41:
#Code task 24#
#Drop the 'fastEight' column from ski_data. Use inplace=True
ski_data.drop(columns='fastEight', inplace=True)
49/42:
#Code task 24#
#Drop the 'fastEight' column from ski_data. Use inplace=True
ski_data.drop(columns='fastEight', inplace=True)
49/43:
#Code task 25#
#Filter the 'yearsOpen' column for values greater than 100
ski_data.yearsOpen[ski_data.yearsOpen > 100]
49/44:
#Code task 26#
#Call the hist method on 'yearsOpen' after filtering for values under 1000
#Pass the argument bins=30 to hist(), but feel free to explore other values
ski_data.yearsOpen[ski_data.yearsOpen < 1000].hist(bins=30)
plt.xlabel('Years open')
plt.ylabel('Count')
plt.title('Distribution of years open excluding 2019');
49/45: ski_data.yearsOpen[ski_data.yearsOpen < 1000].describe()
49/46: ski_data = ski_data[ski_data.yearsOpen < 1000]
49/47:
#Code task 27#
#Add named aggregations for the sum of 'daysOpenLastYear', 'TerrainParks', and 'NightSkiing_ac'
#call them 'state_total_days_open', 'state_total_terrain_parks', and 'state_total_nightskiing_ac',
#respectively
#Finally, add a call to the reset_index() method (we recommend you experiment with and without this to see
#what it does)
state_summary = ski_data.groupby('state').agg(
    resorts_per_state=pd.NamedAgg(column='Name', aggfunc='size'), #could pick any column here
    state_total_skiable_area_ac=pd.NamedAgg(column='SkiableTerrain_ac', aggfunc='sum'),
    state_total_days_open=pd.NamedAgg(column='daysOpenLastYear', aggfunc='sum'),
    state_total_terrain_parks=pd.NamedAgg(column='TerrainParks', aggfunc='sum'),
    state_total_nightskiing_ac=pd.NamedAgg(column='NightSkiing_ac', aggfunc='sum')
).reset_index()
state_summary.head()
49/48:
missing_price = ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)
missing_price.value_counts()/len(missing_price) * 100
49/49:
#Code task 28#
#Use `missing_price` to remove rows from ski_data where both price values are missing
ski_data = ski_data[missing_price != 2]
49/50:
missing_price = ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)
missing_price.value_counts()/len(missing_price) * 100
49/51:
missing_price = ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum(axis=1)
missing_price.value_counts()/len(missing_price) * 100
49/52:
#Code task 28#
#Use `missing_price` to remove rows from ski_data where both price values are missing
ski_data = ski_data[missing_price != 2]
49/53:
ski_data.hist(figsize=(15, 10))
plt.subplots_adjust(hspace=0.5);
49/54:
#Code task 29#
#Use pandas' `read_html` method to read the table from the URL below
states_url = 'https://simple.wikipedia.org/wiki/List_of_U.S._states'
usa_states = pd.read_html(states_url)
49/55: type(usa_states)
49/56: len(usa_states)
49/57:
usa_states = usa_states[0]
usa_states.head()
49/58:
#Code task 30#
#Use the iloc accessor to get the pandas Series for column number 4 from `usa_states`
#It should be a column of dates
established = usa_states.iloc[:, 4]
49/59: established
49/60:
#Code task 31#
#Now use the iloc accessor again to extract columns 0, 5, and 6 and the dataframe's `copy()` method
#Set the names of these extracted columns to 'state', 'state_population', and 'state_area_sq_miles',
#respectively.
usa_states_sub = usa_states.iloc[:, [0, 5, 6]].copy()
usa_states_sub.columns = ['state', 'state_population', 'state_area_sq_miles']
usa_states_sub.head()
49/61:
#Code task 32#
#Find the states in `state_summary` that are not in `usa_states_sub`
#Hint: set(list1) - set(list2) is an easy way to get items in list1 that are not in list2
missing_states = set(state_summary.state) - set(usa_states_sub.state)
missing_states
49/62: usa_states_sub.state[usa_states_sub.state.str.contains('Massachusetts|Pennsylvania|Rhode Island|Virginia')]
49/63:
#Code task 33#
#Use pandas' Series' `replace()` method to replace anything within square brackets (including the brackets)
#with the empty string. Do this inplace, so you need to specify the arguments:
#to_replace='\[.*\]' #literal square bracket followed by anything or nothing followed by literal closing bracket
#value='' #empty string as replacement
#regex=True #we used a regex in our `to_replace` argument
#inplace=True #Do this "in place"
usa_states_sub.state.replace(to_replace='\[.*\]', value='', regex=True, inplace=True)
usa_states_sub.state[usa_states_sub.state.str.contains('Massachusetts|Pennsylvania|Rhode Island|Virginia')]
49/64:
#Code task 34#
#And now verify none of our states are missing by checking that there are no states in
#state_summary that are not in usa_states_sub (as earlier using `set()`)
missing_states = set(state_summary.state) - set(usa_states_sub.state)
missing_states
49/65:
#Code task 35#
#Use 'state_summary's `merge()` method to combine our new data in 'usa_states_sub'
#specify the arguments how='left' and on='state'
state_summary = state_summary.merge(usa_states_sub, how='left', on='state')
state_summary.head()
49/66:
#Code task 36#
#Use ski_data's `plot()` method to create a scatterplot (kind='scatter') with 'AdultWeekday' on the x-axis and
#'AdultWeekend' on the y-axis
ski_data.plot(x='AdultWeekday', y='AdultWeekend', kind='scatter');
49/67:
#Code task 37#
#Use the loc accessor on ski_data to print the 'AdultWeekend' and 'AdultWeekday' columns for Montana only
ski_data.loc[ski_data.state == 'Montana', ['AdultWeekend', 'AdultWeekday']]
49/68: ski_data[['AdultWeekend', 'AdultWeekday']].isnull().sum()
49/69:
ski_data.drop(columns='AdultWeekday', inplace=True)
ski_data.dropna(subset=['AdultWeekend'], inplace=True)
49/70: ski_data.shape
49/71: ski_data.head()
49/72:
missing = pd.concat([ski_data.isnull().sum(axis=1), 100 * ski_data.isnull().mean(axis=1)], axis=1)
missing.columns=['count', '%']
missing.sort_values(by='count', ascending=False).head(10)
49/73: missing['%'].unique()
49/74: missing['%'].value_counts()
49/75: ski_data.info()
49/76: ski_data.info()
49/77: ski_data.shape
49/78: ski_data.shape
49/79:
datapath = 'data'
# renaming the output data directory and re-running this notebook, for example,
# will recreate this (empty) directory and resave the data files.
# NB this is not a substitute for a modern data pipeline, for which there are
# various tools. However, for our purposes here, and often in a "one off" analysis,
# this is useful because we have to deliberately move/delete our data in order
# to overwrite it.
if not os.path.exists(datapath):
    os.mkdir(datapath)
49/80:
datapath_skidata = os.path.join(datapath, 'ski_data_cleaned.csv')
if not os.path.exists(datapath_skidata):
    ski_data.to_csv(datapath_skidata, index=False)
49/81:
datapath_states = os.path.join(datapath, 'state_summary.csv')
if not os.path.exists(datapath_states):
    state_summary.to_csv(datapath_states, index=False)
50/1:
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale
50/2: ski_data = pd.read_csv('data/ski_data_cleaned.csv')
50/3: os.getcwd()
50/4:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data'
os.chdir(path)
50/5:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data'
os.chdir(path)
50/6: os.listdir()
50/7: ski_data = pd.read_csv('data/ski_data_cleaned.csv')
50/8: ski_data.info()
50/9: ski_data.head()
50/10: state_summary = pd.read_csv('data/state_summary.csv')
50/11: state_summary.info()
50/12: state_summary.head()
50/13: state_summary_newind = state_summary.set_index('state')
50/14: state_summary_newind.state_area_sq_miles.sort_values(ascending=False).head()
50/15: state_summary_newind.state_population.sort_values(ascending=False).head()
50/16: state_summary_newind.resorts_per_state.sort_values(ascending=False).head()
50/17: state_summary_newind.state_total_skiable_area_ac.sort_values(ascending=False).head()
50/18: state_summary_newind.state_total_nightskiing_ac.sort_values(ascending=False).head()
50/19: state_summary_newind.state_total_nightskiing_ac.sort_values(ascending=False).head()
50/20: state_summary_newind.state_total_days_open.sort_values(ascending=False).head()
50/21:
#Code task 1#
#Create a new dataframe, `state_summary_scale` from `state_summary` whilst setting the index to 'state'
state_summary_scale = state_summary.set_index('state')
#Save the state labels (using the index attribute of `state_summary_scale`) into the variable 'state_summary_index'
state_summary_index = state_summary_scale.index
#Save the column names (using the `columns` attribute) of `state_summary_scale` into the variable 'state_summary_columns'
state_summary_columns = state_summary_scale.columns
state_summary_scale.head()
50/22: state_summary_scale = scale(state_summary_scale)
50/23:
#Code task 2#
#Create a new dataframe from `state_summary_scale` using the column names we saved in `state_summary_columns`
state_summary_scaled_df = pd.DataFrame(state_summary_scale, columns=state_summary_columns)
state_summary_scaled_df.head()
50/24:
#Code task 3#
#Call `state_summary_scaled_df`'s `mean()` method
state_summary_scaled_df.mean()
50/25:
#Code task 4#
#Call `state_summary_scaled_df`'s `std()` method
state_summary_scaled_df.std()
50/26:
#Code task 5#
#Repeat the previous call to `std()` but pass in ddof=0 
state_summary_scaled_df.std(ddof=0)
50/27:
#Code task 5#
#Repeat the previous call to `std()` but pass in ddof=0 
state_summary_scaled_df.std(ddof=0)
50/28: state_pca = PCA().fit(state_summary_scale)
50/29:
#Code task 6#
#Call the `cumsum()` method on the 'explained_variance_ratio_' attribute of `state_pca` and
#create a line plot to visualize the cumulative explained variance ratio with number of components
#Set the xlabel to 'Component #', the ylabel to 'Cumulative ratio variance', and the
#title to 'Cumulative variance ratio explained by PCA components for state/resort summary statistics'
#Hint: remember the handy ';' at the end of the last plot call to suppress that untidy output
plt.subplots(figsize=(10, 6))
plt.plot(state_pca.explained_variance_ratio_.cumsum())
plt.xlabel('Component #')
plt.ylabel('Cumulative ratio variance')
plt.title('Cumulative variance ratio explained by PCA components for state/resort summary statistics');
50/30:
#Code task 7#
#Call `state_pca`'s `transform()` method, passing in `state_summary_scale` as its argument
state_pca_x = state_pca.transform(state_summary_scale)
50/31: state_pca_x.shape
50/32:
x = state_pca_x[:, 0]
y = state_pca_x[:, 1]
state = state_summary_index
pc_var = 100 * state_pca.explained_variance_ratio_.cumsum()[1]
plt.subplots(figsize=(10,8))
plt.scatter(x=x, y=y)
plt.xlabel('First component')
plt.ylabel('Second component')
plt.title(f'Ski states summary PCA, {pc_var:.1f}% variance explained')
for s, x, y in zip(state, x, y):
    plt.annotate(s, (x, y))
50/33:
#Code task 8#
#Calculate the average 'AdultWeekend' ticket price by state
state_avg_price = ski_data.groupby('state')['AdultWeekend'].mean()
state_avg_price.head()
50/34:
state_avg_price.hist(bins=30)
plt.title('Distribution of state averaged prices')
plt.xlabel('Mean state adult weekend ticket price')
plt.ylabel('count');
50/35:
#Code task 9#
#Create a dataframe containing the values of the first two PCA components
#Remember the first component was given by state_pca_x[:, 0],
#and the second by state_pca_x[:, 1]
#Call these 'PC1' and 'PC2', respectively and set the dataframe index to `state_summary_index`
pca_df = pd.DataFrame({'PC1': state_pca_x[:, 0], 'PC2': state_pca_x[:, 1]}, index=state_summary_index)
pca_df.head()
50/36:
# our average state prices also have state as an index
state_avg_price.head()
50/37:
# we can also cast it to a dataframe using Series' to_frame() method:
state_avg_price.to_frame().head()
50/38:
#Code task 10#
#Use pd.concat to concatenate `pca_df` and `state_avg_price` along axis 1
# remember, pd.concat will align on index
pca_df = pd.concat([pca_df, state_avg_price], axis=1)
pca_df.head()
50/39:
pca_df['Quartile'] = pd.qcut(pca_df.AdultWeekend, q=4, precision=1)
pca_df.head()
50/40:
# Note that Quartile is a new data type: category
# This will affect how we handle it later on
pca_df.dtypes
50/41: pca_df[pca_df.isnull().any(axis=1)]
50/42: pca_df[pca_df.isnull().any(axis=1)]
50/43:
pca_df['AdultWeekend'].fillna(pca_df.AdultWeekend.mean(), inplace=True)
pca_df['Quartile'] = pca_df['Quartile'].cat.add_categories('NA')
pca_df['Quartile'].fillna('NA', inplace=True)
pca_df.loc['Rhode Island']
50/44:
x = pca_df.PC1
y = pca_df.PC2
price = pca_df.AdultWeekend
quartiles = pca_df.Quartile
state = pca_df.index
pc_var = 100 * state_pca.explained_variance_ratio_.cumsum()[1]
fig, ax = plt.subplots(figsize=(10,8))
for q in quartiles.cat.categories:
    im = quartiles == q
    ax.scatter(x=x[im], y=y[im], s=price[im], label=q)
ax.set_xlabel('First component')
ax.set_ylabel('Second component')
plt.legend()
ax.set_title(f'Ski states summary PCA, {pc_var:.1f}% variance explained')
for s, x, y in zip(state, x, y):
    plt.annotate(s, (x, y))
50/45:
#Code task 11#
#Create a seaborn scatterplot by calling `sns.scatterplot`
#Specify the dataframe pca_df as the source of the data,
#specify 'PC1' for x and 'PC2' for y,
#specify 'AdultWeekend' for the pointsize (scatterplot's `size` argument),
#specify 'Quartile' for `hue`
#specify pca_df.Quartile.cat.categories for `hue_order` - what happens with/without this?
x = pca_df.PC1
y = pca_df.PC2
state = pca_df.index
plt.subplots(figsize=(12, 10))
# Note the argument below to make sure we get the colours in the ascending
# order we intuitively expect!
sns.scatterplot(x=PC1, y=PC2, size='AdultWeekend', hue='Quartile', 
                hue_order=pca_df.Quartile.cat.categories, data=pca_df)
#and we can still annotate with the state labels
for s, x, y in zip(state, x, y):
    plt.annotate(s, (x, y))   
plt.title(f'Ski states summary PCA, {pc_var:.1f}% variance explained');
50/46:
#Code task 11#
#Create a seaborn scatterplot by calling `sns.scatterplot`
#Specify the dataframe pca_df as the source of the data,
#specify 'PC1' for x and 'PC2' for y,
#specify 'AdultWeekend' for the pointsize (scatterplot's `size` argument),
#specify 'Quartile' for `hue`
#specify pca_df.Quartile.cat.categories for `hue_order` - what happens with/without this?
x = pca_df.PC1
y = pca_df.PC2
state = pca_df.index
plt.subplots(figsize=(12, 10))
# Note the argument below to make sure we get the colours in the ascending
# order we intuitively expect!
sns.scatterplot(x='PC1', y='PC2', size='AdultWeekend', hue='Quartile', 
                hue_order=pca_df.Quartile.cat.categories, data=pca_df)
#and we can still annotate with the state labels
for s, x, y in zip(state, x, y):
    plt.annotate(s, (x, y))   
plt.title(f'Ski states summary PCA, {pc_var:.1f}% variance explained');
50/47:
#Code task 11#
#Create a seaborn scatterplot by calling `sns.scatterplot`
#Specify the dataframe pca_df as the source of the data,
#specify 'PC1' for x and 'PC2' for y,
#specify 'AdultWeekend' for the pointsize (scatterplot's `size` argument),
#specify 'Quartile' for `hue`
#specify pca_df.Quartile.cat.categories for `hue_order` - what happens with/without this?
x = pca_df.PC1
y = pca_df.PC2
state = pca_df.index
plt.subplots(figsize=(12, 10))
# Note the argument below to make sure we get the colours in the ascending
# order we intuitively expect!
sns.scatterplot(x='PC1', y='PC2', size='AdultWeekend', hue='Quartile', 
                 data=pca_df)
#and we can still annotate with the state labels
for s, x, y in zip(state, x, y):
    plt.annotate(s, (x, y))   
plt.title(f'Ski states summary PCA, {pc_var:.1f}% variance explained');
50/48:
#Code task 11#
#Create a seaborn scatterplot by calling `sns.scatterplot`
#Specify the dataframe pca_df as the source of the data,
#specify 'PC1' for x and 'PC2' for y,
#specify 'AdultWeekend' for the pointsize (scatterplot's `size` argument),
#specify 'Quartile' for `hue`
#specify pca_df.Quartile.cat.categories for `hue_order` - what happens with/without this?
x = pca_df.PC1
y = pca_df.PC2
state = pca_df.index
plt.subplots(figsize=(12, 10))
# Note the argument below to make sure we get the colours in the ascending
# order we intuitively expect!
sns.scatterplot(x='PC1', y='PC2', size='AdultWeekend', hue='Quartile', 
                hue_order=pca_df.Quartile.cat.categories, data=pca_df)
#and we can still annotate with the state labels
for s, x, y in zip(state, x, y):
    plt.annotate(s, (x, y))   
plt.title(f'Ski states summary PCA, {pc_var:.1f}% variance explained');
50/49: pd.DataFrame(state_pca.components_, columns=state_summary_columns)
50/50: state_summary[state_summary.state.isin(['New Hampshire', 'Vermont'])].T
50/51: state_summary_scaled_df[state_summary.state.isin(['New Hampshire', 'Vermont'])].T
50/52: ski_data.head().T
50/53:
# DataFrame's merge method provides SQL-like joins
# here 'state' is a column (not an index)
ski_data = ski_data.merge(state_summary, how='left', on='state')
ski_data.head().T
50/54:
ski_data['resort_skiable_area_ac_state_ratio'] = ski_data.SkiableTerrain_ac / ski_data.state_total_skiable_area_ac
ski_data['resort_days_open_state_ratio'] = ski_data.daysOpenLastYear / ski_data.state_total_days_open
ski_data['resort_terrain_park_state_ratio'] = ski_data.TerrainParks / ski_data.state_total_terrain_parks
ski_data['resort_night_skiing_state_ratio'] = ski_data.NightSkiing_ac / ski_data.state_total_nightskiing_ac

ski_data.drop(columns=['state_total_skiable_area_ac', 'state_total_days_open', 
                       'state_total_terrain_parks', 'state_total_nightskiing_ac'], inplace=True)
50/55:
#Code task 12#
#Show a seaborn heatmap of correlations in ski_data
#Hint: call pandas' `corr()` method on `ski_data` and pass that into `sns.heatmap`
plt.subplots(figsize=(12,10))
sns.pd.corr(ski_data.sns.heatmap);
50/56:
#Code task 12#
#Show a seaborn heatmap of correlations in ski_data
#Hint: call pandas' `corr()` method on `ski_data` and pass that into `sns.heatmap`
plt.subplots(figsize=(12,10))
sns.corr(ski_data.sns.heatmap);
50/57:
#Code task 12#
#Show a seaborn heatmap of correlations in ski_data
#Hint: call pandas' `corr()` method on `ski_data` and pass that into `sns.heatmap`
plt.subplots(figsize=(12,10))
sns.heatmap(ski_data.corr());
50/58:
# define useful function to create scatterplots of ticket prices against desired columns
def scatterplots(columns, ncol=None, figsize=(15, 8)):
    if ncol is None:
        ncol = len(columns)
    nrow = int(np.ceil(len(columns) / ncol))
    fig, axes = plt.subplots(nrow, ncol, figsize=figsize, squeeze=False)
    fig.subplots_adjust(wspace=0.5, hspace=0.6)
    for i, col in enumerate(columns):
        ax = axes.flatten()[i]
        ax.scatter(x = col, y = 'AdultWeekend', data=ski_data, alpha=0.5)
        ax.set(xlabel=col, ylabel='Ticket price')
    nsubplots = nrow * ncol    
    for empty in range(i+1, nsubplots):
        axes.flatten()[empty].set_visible(False)
50/59:
#Code task 13#
#Use a list comprehension to build a list of features from the columns of `ski_data` that
#are _not_ any of 'Name', 'Region', 'state', or 'AdultWeekend'
features = [feature for feature in ski_data.columns if feature not in ['Name', 'Region', 'state', 'AdultWeekend']]
50/60: scatterplots(features, ncol=4, figsize=(15, 15))
50/61:
ski_data['total_chairs_runs_ratio'] = ski_data.total_chairs / ski_data.Runs
ski_data['total_chairs_skiable_ratio'] = ski_data.total_chairs / ski_data.SkiableTerrain_ac
ski_data['fastQuads_runs_ratio'] = ski_data.fastQuads / ski_data.Runs
ski_data['fastQuads_skiable_ratio'] = ski_data.fastQuads / ski_data.SkiableTerrain_ac
50/62:
scatterplots(['total_chairs_runs_ratio', 'total_chairs_skiable_ratio', 
              'fastQuads_runs_ratio', 'fastQuads_skiable_ratio'], ncol=2)
50/63: ski_data.head().T
50/64:
# The 100_000 scaling is simply based on eyeballing the magnitudes of the data
state_summary['resorts_per_100kcapita'] = 100_000 * state_summary.resorts_per_state / state_summary.state_population
state_summary['resorts_per_100ksq_mile'] = 100_000 * state_summary.resorts_per_state / state_summary.state_area_sq_miles
state_summary.drop(columns=['state_population', 'state_area_sq_miles'], inplace=True)
state_summary.head()
50/65: state_summary.head()
50/66:
# The 100_000 scaling is simply based on eyeballing the magnitudes of the data
state_summary['resorts_per_100kcapita'] = 100_000 * state_summary.resorts_per_state / state_summary.state_population
state_summary['resorts_per_100ksq_mile'] = 100_000 * state_summary.resorts_per_state / state_summary.state_area_sq_miles
state_summary.drop(columns=['state_population', 'state_area_sq_miles'], inplace=True)
state_summary.head()
50/67: ski_data.head().T
50/68:
datapath = 'data'
datapath_skidata = os.path.join(datapath, 'ski_data_step3_features.csv')
if not os.path.exists(datapath_skidata):
    ski_data.to_csv(datapath_skidata, index=False)
52/1:
# The 100_000 scaling is simply based on eyeballing the magnitudes of the data
state_summary['resorts_per_100kcapita'] = 100_000 * state_summary.resorts_per_state / state_summary.state_population
state_summary['resorts_per_100ksq_mile'] = 100_000 * state_summary.resorts_per_state / state_summary.state_area_sq_miles
state_summary.drop(columns=['state_population', 'state_area_sq_miles'], inplace=True)
state_summary.head()
53/1:
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale
53/2:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data'
os.chdir(path)
53/3: os.listdir()
53/4: ski_data = pd.read_csv('data/ski_data_cleaned.csv')
53/5: ski_data.info()
53/6: ski_data.head()
53/7: state_summary = pd.read_csv('data/state_summary.csv')
53/8: state_summary.info()
53/9: state_summary.head()
53/10: state_summary_newind = state_summary.set_index('state')
53/11: state_summary_newind.state_area_sq_miles.sort_values(ascending=False).head()
53/12: state_summary_newind.state_population.sort_values(ascending=False).head()
53/13: state_summary_newind.resorts_per_state.sort_values(ascending=False).head()
53/14: state_summary_newind.state_total_skiable_area_ac.sort_values(ascending=False).head()
53/15: state_summary_newind.state_total_nightskiing_ac.sort_values(ascending=False).head()
53/16: state_summary_newind.state_total_days_open.sort_values(ascending=False).head()
53/17:
# The 100_000 scaling is simply based on eyeballing the magnitudes of the data
state_summary['resorts_per_100kcapita'] = 100_000 * state_summary.resorts_per_state / state_summary.state_population
state_summary['resorts_per_100ksq_mile'] = 100_000 * state_summary.resorts_per_state / state_summary.state_area_sq_miles
state_summary.drop(columns=['state_population', 'state_area_sq_miles'], inplace=True)
state_summary.head()
53/18:
state_summary.resorts_per_100kcapita.hist(bins=30)
plt.xlabel('Number of resorts per 100k population')
plt.ylabel('count');
53/19:
state_summary.resorts_per_100ksq_mile.hist(bins=30)
plt.xlabel('Number of resorts per 100k square miles')
plt.ylabel('count');
53/20: state_summary.set_index('state').resorts_per_100kcapita.sort_values(ascending=False).head()
53/21: state_summary.set_index('state').resorts_per_100ksq_mile.sort_values(ascending=False).head()
53/22:
#Code task 1#
#Create a new dataframe, `state_summary_scale` from `state_summary` whilst setting the index to 'state'
state_summary_scale = state_summary.set_index('state')
#Save the state labels (using the index attribute of `state_summary_scale`) into the variable 'state_summary_index'
state_summary_index = state_summary_scale.index
#Save the column names (using the `columns` attribute) of `state_summary_scale` into the variable 'state_summary_columns'
state_summary_columns = state_summary_scale.columns
state_summary_scale.head()
53/23: state_summary_scale = scale(state_summary_scale)
53/24:
#Code task 2#
#Create a new dataframe from `state_summary_scale` using the column names we saved in `state_summary_columns`
state_summary_scaled_df = pd.DataFrame(state_summary_scale, columns=state_summary_columns)
state_summary_scaled_df.head()
53/25:
#Code task 3#
#Call `state_summary_scaled_df`'s `mean()` method
state_summary_scaled_df.mean()
53/26:
#Code task 4#
#Call `state_summary_scaled_df`'s `std()` method
state_summary_scaled_df.std()
53/27:
#Code task 5#
#Repeat the previous call to `std()` but pass in ddof=0 
state_summary_scaled_df.std(ddof=0)
53/28: state_pca = PCA().fit(state_summary_scale)
53/29:
#Code task 6#
#Call the `cumsum()` method on the 'explained_variance_ratio_' attribute of `state_pca` and
#create a line plot to visualize the cumulative explained variance ratio with number of components
#Set the xlabel to 'Component #', the ylabel to 'Cumulative ratio variance', and the
#title to 'Cumulative variance ratio explained by PCA components for state/resort summary statistics'
#Hint: remember the handy ';' at the end of the last plot call to suppress that untidy output
plt.subplots(figsize=(10, 6))
plt.plot(state_pca.explained_variance_ratio_.cumsum())
plt.xlabel('Component #')
plt.ylabel('Cumulative ratio variance')
plt.title('Cumulative variance ratio explained by PCA components for state/resort summary statistics');
53/30:
#Code task 7#
#Call `state_pca`'s `transform()` method, passing in `state_summary_scale` as its argument
state_pca_x = state_pca.transform(state_summary_scale)
53/31: state_pca_x.shape
53/32:
x = state_pca_x[:, 0]
y = state_pca_x[:, 1]
state = state_summary_index
pc_var = 100 * state_pca.explained_variance_ratio_.cumsum()[1]
plt.subplots(figsize=(10,8))
plt.scatter(x=x, y=y)
plt.xlabel('First component')
plt.ylabel('Second component')
plt.title(f'Ski states summary PCA, {pc_var:.1f}% variance explained')
for s, x, y in zip(state, x, y):
    plt.annotate(s, (x, y))
53/33:
#Code task 8#
#Calculate the average 'AdultWeekend' ticket price by state
state_avg_price = ski_data.groupby('state')['AdultWeekend'].mean()
state_avg_price.head()
53/34:
state_avg_price.hist(bins=30)
plt.title('Distribution of state averaged prices')
plt.xlabel('Mean state adult weekend ticket price')
plt.ylabel('count');
53/35:
#Code task 9#
#Create a dataframe containing the values of the first two PCA components
#Remember the first component was given by state_pca_x[:, 0],
#and the second by state_pca_x[:, 1]
#Call these 'PC1' and 'PC2', respectively and set the dataframe index to `state_summary_index`
pca_df = pd.DataFrame({'PC1': state_pca_x[:, 0], 'PC2': state_pca_x[:, 1]}, index=state_summary_index)
pca_df.head()
53/36:
# our average state prices also have state as an index
state_avg_price.head()
53/37:
# we can also cast it to a dataframe using Series' to_frame() method:
state_avg_price.to_frame().head()
53/38:
#Code task 10#
#Use pd.concat to concatenate `pca_df` and `state_avg_price` along axis 1
# remember, pd.concat will align on index
pca_df = pd.concat([pca_df, state_avg_price], axis=1)
pca_df.head()
53/39:
pca_df['Quartile'] = pd.qcut(pca_df.AdultWeekend, q=4, precision=1)
pca_df.head()
53/40:
# Note that Quartile is a new data type: category
# This will affect how we handle it later on
pca_df.dtypes
53/41: pca_df[pca_df.isnull().any(axis=1)]
53/42:
pca_df['AdultWeekend'].fillna(pca_df.AdultWeekend.mean(), inplace=True)
pca_df['Quartile'] = pca_df['Quartile'].cat.add_categories('NA')
pca_df['Quartile'].fillna('NA', inplace=True)
pca_df.loc['Rhode Island']
53/43:
x = pca_df.PC1
y = pca_df.PC2
price = pca_df.AdultWeekend
quartiles = pca_df.Quartile
state = pca_df.index
pc_var = 100 * state_pca.explained_variance_ratio_.cumsum()[1]
fig, ax = plt.subplots(figsize=(10,8))
for q in quartiles.cat.categories:
    im = quartiles == q
    ax.scatter(x=x[im], y=y[im], s=price[im], label=q)
ax.set_xlabel('First component')
ax.set_ylabel('Second component')
plt.legend()
ax.set_title(f'Ski states summary PCA, {pc_var:.1f}% variance explained')
for s, x, y in zip(state, x, y):
    plt.annotate(s, (x, y))
53/44:
#Code task 11#
#Create a seaborn scatterplot by calling `sns.scatterplot`
#Specify the dataframe pca_df as the source of the data,
#specify 'PC1' for x and 'PC2' for y,
#specify 'AdultWeekend' for the pointsize (scatterplot's `size` argument),
#specify 'Quartile' for `hue`
#specify pca_df.Quartile.cat.categories for `hue_order` - what happens with/without this?
x = pca_df.PC1
y = pca_df.PC2
state = pca_df.index
plt.subplots(figsize=(12, 10))
# Note the argument below to make sure we get the colours in the ascending
# order we intuitively expect!
sns.scatterplot(x='PC1', y='PC2', size='AdultWeekend', hue='Quartile', 
                hue_order=pca_df.Quartile.cat.categories, data=pca_df)
#and we can still annotate with the state labels
for s, x, y in zip(state, x, y):
    plt.annotate(s, (x, y))   
plt.title(f'Ski states summary PCA, {pc_var:.1f}% variance explained');
53/45: pd.DataFrame(state_pca.components_, columns=state_summary_columns)
53/46: state_summary[state_summary.state.isin(['New Hampshire', 'Vermont'])].T
53/47: state_summary_scaled_df[state_summary.state.isin(['New Hampshire', 'Vermont'])].T
53/48: ski_data.head().T
53/49: state_summary.head()
53/50:
# DataFrame's merge method provides SQL-like joins
# here 'state' is a column (not an index)
ski_data = ski_data.merge(state_summary, how='left', on='state')
ski_data.head().T
53/51:
ski_data['resort_skiable_area_ac_state_ratio'] = ski_data.SkiableTerrain_ac / ski_data.state_total_skiable_area_ac
ski_data['resort_days_open_state_ratio'] = ski_data.daysOpenLastYear / ski_data.state_total_days_open
ski_data['resort_terrain_park_state_ratio'] = ski_data.TerrainParks / ski_data.state_total_terrain_parks
ski_data['resort_night_skiing_state_ratio'] = ski_data.NightSkiing_ac / ski_data.state_total_nightskiing_ac

ski_data.drop(columns=['state_total_skiable_area_ac', 'state_total_days_open', 
                       'state_total_terrain_parks', 'state_total_nightskiing_ac'], inplace=True)
53/52:
#Code task 12#
#Show a seaborn heatmap of correlations in ski_data
#Hint: call pandas' `corr()` method on `ski_data` and pass that into `sns.heatmap`
plt.subplots(figsize=(12,10))
sns.heatmap(ski_data.corr());
53/53:
# define useful function to create scatterplots of ticket prices against desired columns
def scatterplots(columns, ncol=None, figsize=(15, 8)):
    if ncol is None:
        ncol = len(columns)
    nrow = int(np.ceil(len(columns) / ncol))
    fig, axes = plt.subplots(nrow, ncol, figsize=figsize, squeeze=False)
    fig.subplots_adjust(wspace=0.5, hspace=0.6)
    for i, col in enumerate(columns):
        ax = axes.flatten()[i]
        ax.scatter(x = col, y = 'AdultWeekend', data=ski_data, alpha=0.5)
        ax.set(xlabel=col, ylabel='Ticket price')
    nsubplots = nrow * ncol    
    for empty in range(i+1, nsubplots):
        axes.flatten()[empty].set_visible(False)
53/54:
#Code task 13#
#Use a list comprehension to build a list of features from the columns of `ski_data` that
#are _not_ any of 'Name', 'Region', 'state', or 'AdultWeekend'
features = [feature for feature in ski_data.columns if feature not in ['Name', 'Region', 'state', 'AdultWeekend']]
53/55: scatterplots(features, ncol=4, figsize=(15, 15))
53/56:
ski_data['total_chairs_runs_ratio'] = ski_data.total_chairs / ski_data.Runs
ski_data['total_chairs_skiable_ratio'] = ski_data.total_chairs / ski_data.SkiableTerrain_ac
ski_data['fastQuads_runs_ratio'] = ski_data.fastQuads / ski_data.Runs
ski_data['fastQuads_skiable_ratio'] = ski_data.fastQuads / ski_data.SkiableTerrain_ac
53/57:
scatterplots(['total_chairs_runs_ratio', 'total_chairs_skiable_ratio', 
              'fastQuads_runs_ratio', 'fastQuads_skiable_ratio'], ncol=2)
53/58: ski_data.head().T
53/59:
datapath = 'data'
datapath_skidata = os.path.join(datapath, 'ski_data_step3_features.csv')
if not os.path.exists(datapath_skidata):
    ski_data.to_csv(datapath_skidata, index=False)
54/1:
import pandas as pd
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import __version__ as sklearn_version
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale
from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.dummy import DummyRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_regression
import datetime
54/2:
ski_data = pd.read_csv('data/ski_data_step3_features.csv')
ski_data.head().T
54/3:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data'
os.chdir(path)
54/4: os.listdir()
54/5:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone'
os.chdir(path)
54/6: os.listdir()
54/7:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/'Step Three - Exploratory Data Analysis''
os.chdir(path)
54/8:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/'Step Three - Exploratory Data Analysis
os.chdir(path)
54/9:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/'Step Three - Exploratory Data Analysis'
os.chdir(path)
54/10:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/Step Three - Exploratory Data Analysis
os.chdir(path)
54/11:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/Step Three - Exploratory Data Analysis'
os.chdir(path)
54/12: os.listdir()
54/13:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data/data'
os.chdir(path)
54/14: os.listdir()
54/15:
ski_data = pd.read_csv('data/ski_data_step3_features.csv')
ski_data.head().T
54/16:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data'
os.chdir(path)
54/17: os.listdir()
54/18:
ski_data = pd.read_csv('data/ski_data_step3_features.csv')
ski_data.head().T
54/19: big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']
54/20: big_mountain.T
54/21: ski_data.shape
54/22: ski_data = ski_data[ski_data.Name != 'Big Mountain Resort']
54/23: ski_data.shape
54/24: len(ski_data) * .7, len(ski_data) * .3
54/25:
X_train, X_test, y_train, y_test = train_test_split(ski_data.drop(columns='AdultWeekend'), 
                                                    ski_data.AdultWeekend, test_size=0.3, 
                                                    random_state=47)
54/26: X_train.shape, X_test.shape
54/27: y_train.shape, y_test.shape
54/28:
#Code task 1#
#Save the 'Name', 'state', and 'Region' columns from the train/test data into names_train and names_test
#Then drop those columns from `X_train` and `X_test`. Use 'inplace=True'
names_list = ['Name', 'state', 'Region']
names_train = X_train['Name', 'state', 'Region']
names_test = X_test['Name', 'state', 'Region']
X_train.drop(columns=names_list, inplace=True)
X_test.drop(columns=names_list, inplace=True)
X_train.shape, X_test.shape
54/29:
#Code task 1#
#Save the 'Name', 'state', and 'Region' columns from the train/test data into names_train and names_test
#Then drop those columns from `X_train` and `X_test`. Use 'inplace=True'
names_list = ['Name', 'state', 'Region']
names_train = X_train[names_list]
names_test = X_test[names_list]
X_train.drop(columns=names_list, inplace=True)
X_test.drop(columns=names_list, inplace=True)
X_train.shape, X_test.shape
54/30:
#Code task 2#
#Check the `dtypes` attribute of `X_train` to verify all features are numeric
X_train.dtypes
54/31:
#Code task 3#
#Repeat this check for the test split in `X_test`
X_test.dtypes
54/32:
#Code task 4#
#Calculate the mean of `y_train`
train_mean = y_train.mean()
train_mean
54/33:
#Code task 5#
#Fit the dummy regressor on the training data
#Hint, call its `.fit()` method with `X_train` and `y_train` as arguments
#Then print the object's `constant_` attribute and verify it's the same as the mean above
dumb_reg = DummyRegressor(strategy='mean')
dumb_reg.fit(X_train, y_train)
dumb_reg.constant_
54/34:
#Code task 5#
#Fit the dummy regressor on the training data
#Hint, call its `.fit()` method with `X_train` and `y_train` as arguments
#Then print the object's `constant_` attribute and verify it's the same as the mean above
dumb_reg = DummyRegressor(strategy='mean')
dumb_reg.fit(X_train, y_train)
dumb_reg.constant_
54/35:
#Code task 6#
#Calculate the R^2 as defined above
def r_squared(y, ypred):
    """R-squared score.
    
    Calculate the R-squared, or coefficient of determination, of the input.
    
    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    ybar = np.sum(y) / len(y) #yes, we could use np.mean(y)
    sum_sq_tot = np.sum((y - ybar)**2) #total sum of squares error
    sum_sq_res = np.sum((y - ypred)**2) #residual sum of squares error
    R2 = 1.0 - sum_sq_res / sum_sq_tot
    return R2
54/36:
y_tr_pred_ = train_mean * np.ones(len(y_train))
y_tr_pred_[:5]
54/37:
y_tr_pred = dumb_reg.predict(X_train)
y_tr_pred[:5]
54/38: r_squared(y_train, y_tr_pred)
54/39:
y_te_pred = train_mean * np.ones(len(y_test))
r_squared(y_test, y_te_pred)
54/40:
#Code task 7#
#Calculate the MAE as defined above
def mae(y, ypred):
    """Mean absolute error.
    
    Calculate the mean absolute error of the arguments

    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    abs_error = np.abs(y - ypred)
    mae = np.mean(abs_error)
    return mae
54/41: mae(y_train, y_tr_pred)
54/42: mae(y_test, y_te_pred)
54/43:
#Code task 8#
#Calculate the MSE as defined above
def mse(y, ypred):
    """Mean square error.
    
    Calculate the mean square error of the arguments

    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    sq_error = (y - ypred)**2
    mse = np.mean(sq_error)
    return mse
54/44: mse(y_train, y_tr_pred)
54/45: mse(y_test, y_te_pred)
54/46: np.sqrt([mse(y_train, y_tr_pred), mse(y_test, y_te_pred)])
54/47: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
54/48: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
54/49: mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)
54/50: r2_score?
54/51: r2_score??
54/52:
# train set - sklearn
# correct order, incorrect order
r2_score(y_train, y_tr_pred), r2_score(y_tr_pred, y_train)
54/53:
# test set - sklearn
# correct order, incorrect order
r2_score(y_test, y_te_pred), r2_score(y_te_pred, y_test)
54/54:
# train set - using our homebrew function
# correct order, incorrect order
r_squared(y_train, y_tr_pred), r_squared(y_tr_pred, y_train)
54/55:
# test set - using our homebrew function
# correct order, incorrect order
r_squared(y_test, y_te_pred), r_squared(y_te_pred, y_test)
54/56:
# These are the values we'll use to fill in any missing values
X_defaults_median = X_train.median()
X_defaults_median
54/57:
#Code task 9#
#Call `X_train` and `X_test`'s `fillna()` method, passing `X_defaults_median` as the values to use
#Assign the results to `X_tr` and `X_te`, respectively
X_tr = X_train.fillna(X_defaults_median)
X_te = X_test.fillna(X_defaults_median)
54/58:
#Code task 10#
#Call the StandardScaler`s fit method on `X_tr` to fit the scaler
#then use it's `transform()` method to apply the scaling to both the train and test split
#data (`X_tr` and `X_te`), naming the results `X_tr_scaled` and `X_te_scaled`, respectively
scaler = StandardScaler()
scaler.fit(X_tr)
X_tr_scaled = scaler.transform(X_tr)
X_te_scaled = scaler.transform(X_te)
54/59: lm = LinearRegression().fit(X_tr_scaled, y_train)
54/60:
#Code task 11#
#Call the `predict()` method of the model (`lm`) on both the (scaled) train and test data
#Assign the predictions to `y_tr_pred` and `y_te_pred`, respectively
y_tr_pred = lm.predict(X_tr_scaled)
y_te_pred = lm.predict(X_te_scaled)
54/61:
# r^2 - train, test
median_r2 = r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
median_r2
54/62:
# r^2 - train, test
median_r2 = r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
median_r2
54/63:
#Code task 12#
#Now calculate the mean absolute error scores using `sklearn`'s `mean_absolute_error` function
# as we did above for R^2
# MAE - train, test
median_mae = sklearn.mean_absolute_error(y_train, y_tr_pred), sklearn.mean_absolute_error(y_test, y_te_pred)
median_mae
54/64:
#Code task 12#
#Now calculate the mean absolute error scores using `sklearn`'s `mean_absolute_error` function
# as we did above for R^2
# MAE - train, test
median_mae = mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
median_mae
54/65:
#Code task 13#
#And also do the same using `sklearn`'s `mean_squared_error`
# MSE - train, test
median_mse = mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)
median_mse
54/66:
#Code task 14#
#As we did for the median above, calculate mean values for imputing missing values
# These are the values we'll use to fill in any missing values
X_defaults_mean = X_train.mean()
X_defaults_mean
54/67:
X_tr = X_train.fillna(X_defaults_mean)
X_te = X_test.fillna(X_defaults_mean)
54/68:
scaler = StandardScaler()
scaler.fit(X_tr)
X_tr_scaled = scaler.transform(X_tr)
X_te_scaled = scaler.transform(X_te)
54/69: lm = LinearRegression().fit(X_tr_scaled, y_train)
54/70:
y_tr_pred = lm.predict(X_tr_scaled)
y_te_pred = lm.predict(X_te_scaled)
54/71: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
57/1:
import pandas as pd
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import __version__ as sklearn_version
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale
from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.dummy import DummyRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_regression
import datetime
57/2:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data'
os.chdir(path)
57/3: os.listdir()
57/4:
ski_data = pd.read_csv('data/ski_data_step3_features.csv')
ski_data.head().T
57/5: big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']
57/6: big_mountain.T
57/7: ski_data.shape
57/8: ski_data = ski_data[ski_data.Name != 'Big Mountain Resort']
57/9: ski_data.shape
57/10: len(ski_data) * .7, len(ski_data) * .3
57/11:
X_train, X_test, y_train, y_test = train_test_split(ski_data.drop(columns='AdultWeekend'), 
                                                    ski_data.AdultWeekend, test_size=0.3, 
                                                    random_state=47)
57/12: X_train.shape, X_test.shape
57/13: y_train.shape, y_test.shape
57/14:
#Code task 1#
#Save the 'Name', 'state', and 'Region' columns from the train/test data into names_train and names_test
#Then drop those columns from `X_train` and `X_test`. Use 'inplace=True'
names_list = ['Name', 'state', 'Region']
names_train = X_train[names_list]
names_test = X_test[names_list]
X_train.drop(columns=names_list, inplace=True)
X_test.drop(columns=names_list, inplace=True)
X_train.shape, X_test.shape
57/15:
#Code task 2#
#Check the `dtypes` attribute of `X_train` to verify all features are numeric
X_train.dtypes
57/16:
#Code task 3#
#Repeat this check for the test split in `X_test`
X_test.dtypes
57/17:
#Code task 4#
#Calculate the mean of `y_train`
train_mean = y_train.mean()
train_mean
57/18:
#Code task 5#
#Fit the dummy regressor on the training data
#Hint, call its `.fit()` method with `X_train` and `y_train` as arguments
#Then print the object's `constant_` attribute and verify it's the same as the mean above
dumb_reg = DummyRegressor(strategy='mean')
dumb_reg.fit(X_train, y_train)
dumb_reg.constant_
57/19:
#Code task 6#
#Calculate the R^2 as defined above
def r_squared(y, ypred):
    """R-squared score.
    
    Calculate the R-squared, or coefficient of determination, of the input.
    
    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    ybar = np.sum(y) / len(y) #yes, we could use np.mean(y)
    sum_sq_tot = np.sum((y - ybar)**2) #total sum of squares error
    sum_sq_res = np.sum((y - ypred)**2) #residual sum of squares error
    R2 = 1.0 - sum_sq_res / sum_sq_tot
    return R2
57/20:
y_tr_pred_ = train_mean * np.ones(len(y_train))
y_tr_pred_[:5]
57/21:
y_tr_pred = dumb_reg.predict(X_train)
y_tr_pred[:5]
57/22: r_squared(y_train, y_tr_pred)
57/23:
y_te_pred = train_mean * np.ones(len(y_test))
r_squared(y_test, y_te_pred)
57/24:
#Code task 7#
#Calculate the MAE as defined above
def mae(y, ypred):
    """Mean absolute error.
    
    Calculate the mean absolute error of the arguments

    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    abs_error = np.abs(y - ypred)
    mae = np.mean(abs_error)
    return mae
57/25: mae(y_train, y_tr_pred)
57/26: mae(y_test, y_te_pred)
57/27:
#Code task 8#
#Calculate the MSE as defined above
def mse(y, ypred):
    """Mean square error.
    
    Calculate the mean square error of the arguments

    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    sq_error = (y - ypred)**2
    mse = np.mean(sq_error)
    return mse
57/28: mse(y_train, y_tr_pred)
57/29: mse(y_test, y_te_pred)
57/30: np.sqrt([mse(y_train, y_tr_pred), mse(y_test, y_te_pred)])
57/31: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
57/32: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
57/33: mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)
57/34:
# train set - sklearn
# correct order, incorrect order
r2_score(y_train, y_tr_pred), r2_score(y_tr_pred, y_train)
57/35:
# test set - sklearn
# correct order, incorrect order
r2_score(y_test, y_te_pred), r2_score(y_te_pred, y_test)
57/36:
# train set - using our homebrew function
# correct order, incorrect order
r_squared(y_train, y_tr_pred), r_squared(y_tr_pred, y_train)
57/37:
# test set - using our homebrew function
# correct order, incorrect order
r_squared(y_test, y_te_pred), r_squared(y_te_pred, y_test)
57/38:
# These are the values we'll use to fill in any missing values
X_defaults_median = X_train.median()
X_defaults_median
57/39:
#Code task 9#
#Call `X_train` and `X_test`'s `fillna()` method, passing `X_defaults_median` as the values to use
#Assign the results to `X_tr` and `X_te`, respectively
X_tr = X_train.fillna(X_defaults_median)
X_te = X_test.fillna(X_defaults_median)
57/40:
#Code task 10#
#Call the StandardScaler`s fit method on `X_tr` to fit the scaler
#then use it's `transform()` method to apply the scaling to both the train and test split
#data (`X_tr` and `X_te`), naming the results `X_tr_scaled` and `X_te_scaled`, respectively
scaler = StandardScaler()
scaler.fit(X_tr)
X_tr_scaled = scaler.transform(X_tr)
X_te_scaled = scaler.transform(X_te)
57/41: lm = LinearRegression().fit(X_tr_scaled, y_train)
57/42:
#Code task 11#
#Call the `predict()` method of the model (`lm`) on both the (scaled) train and test data
#Assign the predictions to `y_tr_pred` and `y_te_pred`, respectively
y_tr_pred = lm.predict(X_tr_scaled)
y_te_pred = lm.predict(X_te_scaled)
57/43:
# r^2 - train, test
median_r2 = r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
median_r2
57/44:
#Code task 12#
#Now calculate the mean absolute error scores using `sklearn`'s `mean_absolute_error` function
# as we did above for R^2
# MAE - train, test
median_mae = mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
median_mae
57/45:
#Code task 13#
#And also do the same using `sklearn`'s `mean_squared_error`
# MSE - train, test
median_mse = mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)
median_mse
57/46:
#Code task 14#
#As we did for the median above, calculate mean values for imputing missing values
# These are the values we'll use to fill in any missing values
X_defaults_mean = X_train.mean()
X_defaults_mean
57/47:
X_tr = X_train.fillna(X_defaults_mean)
X_te = X_test.fillna(X_defaults_mean)
57/48:
scaler = StandardScaler()
scaler.fit(X_tr)
X_tr_scaled = scaler.transform(X_tr)
X_te_scaled = scaler.transform(X_te)
57/49: lm = LinearRegression().fit(X_tr_scaled, y_train)
57/50:
y_tr_pred = lm.predict(X_tr_scaled)
y_te_pred = lm.predict(X_te_scaled)
57/51: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
57/52: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
57/53: mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)
57/54:
pipe = make_pipeline(
    SimpleImputer(strategy='median'), 
    StandardScaler(), 
    LinearRegression()
)
57/55: type(pipe)
57/56: hasattr(pipe, 'fit'), hasattr(pipe, 'predict')
57/57:
#Code task 15#
#Call the pipe's `fit()` method with `X_train` and `y_train` as arguments
pipe.___(___, ___)
57/58: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
57/59: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
57/60: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
57/61: mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)
57/62:
pipe = make_pipeline(
    SimpleImputer(strategy='median'), 
    StandardScaler(), 
    LinearRegression()
)
57/63: type(pipe)
57/64: hasattr(pipe, 'fit'), hasattr(pipe, 'predict')
57/65:
#Code task 15#
#Call the pipe's `fit()` method with `X_train` and `y_train` as arguments
pipe.fit(X_train, y_train)
57/66:
#Code task 15#
#Call the pipe's `fit()` method with `X_train` and `y_train` as arguments
pipe.fit(X_train, y_train)
57/67:
y_tr_pred = pipe.predict(X_train)
y_te_pred = pipe.predict(X_test)
57/68: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
57/69: median_r2
57/70: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
57/71: Compare with your earlier result:
57/72: median_mae
57/73: mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)
57/74: median_mse
57/75:
#Code task 16#
#Add `SelectKBest` as a step in the pipeline between `StandardScaler()` and `LinearRegression()`
#Don't forget to tell it to use `f_regression` as its score function
pipe = make_pipeline(
    SimpleImputer(strategy='median'), 
    StandardScaler(),
    SelectKBest(f_regression),
    LinearRegression()
)
57/76: pipe.fit(X_train, y_train)
57/77:
y_tr_pred = pipe.predict(X_train)
y_te_pred = pipe.predict(X_test)
57/78: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
57/79: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
57/80:
#Code task 17#
#Modify the `SelectKBest` step to use a value of 15 for k
pipe15 = make_pipeline(
    SimpleImputer(strategy='median'), 
    StandardScaler(),
    SelectKBest(f_regression, k=15),
    LinearRegression()
)
57/81: pipe15.fit(X_train, y_train)
57/82:
y_tr_pred = pipe15.predict(X_train)
y_te_pred = pipe15.predict(X_test)
57/83: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
57/84: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
57/85: cv_results = cross_validate(pipe15, X_train, y_train, cv=5)
57/86:
cv_scores = cv_results['test_score']
cv_scores
57/87: np.mean(cv_scores), np.std(cv_scores)
57/88: np.round((np.mean(cv_scores) - 2 * np.std(cv_scores), np.mean(cv_scores) + 2 * np.std(cv_scores)), 2)
57/89:
#Code task 18#
#Call `pipe`'s `get_params()` method to get a dict of available parameters and print their names
#using dict's `keys()` method
pipe.get_params().keys()
57/90:
k = [k+1 for k in range(len(X_train.columns))]
grid_params = {'selectkbest__k': k}
57/91: lr_grid_cv = GridSearchCV(pipe, param_grid=grid_params, cv=5, n_jobs=-1)
57/92: lr_grid_cv.fit(X_train, y_train)
57/93:
score_mean = lr_grid_cv.cv_results_['mean_test_score']
score_std = lr_grid_cv.cv_results_['std_test_score']
cv_k = [k for k in lr_grid_cv.cv_results_['param_selectkbest__k']]
57/94:
#Code task 19#
#Print the `best_params_` attribute of `lr_grid_cv`
lr_grid_cv.best_params
57/95:
#Code task 19#
#Print the `best_params_` attribute of `lr_grid_cv`
lr_grid_cv.best_params_
57/96:
#Code task 20#
#Assign the value of k from the above dict of `best_params_` and assign it to `best_k`
best_k = lr_grid_cv.best_params_['selectkbest__k']
plt.subplots(figsize=(10, 5))
plt.errorbar(cv_k, score_mean, yerr=score_std)
plt.axvline(x=best_k, c='r', ls='--', alpha=.5)
plt.xlabel('k')
plt.ylabel('CV score (r-squared)')
plt.title('Pipeline mean CV score (error bars +/- 1sd)');
57/97: selected = lr_grid_cv.best_estimator_.named_steps.selectkbest.get_support()
57/98: selected = lr_grid_cv.best_estimator_.named_steps.selectkbest.get_support()
57/99:
#Code task 21#
#Get the linear model coefficients from the `coef_` attribute and store in `coefs`,
#get the matching feature names from the column names of the dataframe,
#and display the results as a pandas Series with `coefs` as the values and `features` as the index,
#sorting the values in descending order
coefs = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_
features = X_train.columns[selected]
pd.Series(coefs, index=features).plot(ascending=False)
57/100:
#Code task 21#
#Get the linear model coefficients from the `coef_` attribute and store in `coefs`,
#get the matching feature names from the column names of the dataframe,
#and display the results as a pandas Series with `coefs` as the values and `features` as the index,
#sorting the values in descending order
coefs = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_
features = X_train.columns[selected]
pd.Series(coefs, index=features).plot(ascending=True)
57/101:
#Code task 21#
#Get the linear model coefficients from the `coef_` attribute and store in `coefs`,
#get the matching feature names from the column names of the dataframe,
#and display the results as a pandas Series with `coefs` as the values and `features` as the index,
#sorting the values in descending order
coefs = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_
features = X_train.columns[selected]
pd.Series(coefs, index=features).plt(ascending=False)
57/102:
#Code task 21#
#Get the linear model coefficients from the `coef_` attribute and store in `coefs`,
#get the matching feature names from the column names of the dataframe,
#and display the results as a pandas Series with `coefs` as the values and `features` as the index,
#sorting the values in descending order
coefs = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_
features = X_train.columns[selected]
pd.Series(coefs, index=features).show(ascending=False)
57/103:
#Code task 21#
#Get the linear model coefficients from the `coef_` attribute and store in `coefs`,
#get the matching feature names from the column names of the dataframe,
#and display the results as a pandas Series with `coefs` as the values and `features` as the index,
#sorting the values in descending order
coefs = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_
features = X_train.columns[selected]
pd.Series(coefs, index=features).pd.plt(ascending=False)
57/104:
#Code task 21#
#Get the linear model coefficients from the `coef_` attribute and store in `coefs`,
#get the matching feature names from the column names of the dataframe,
#and display the results as a pandas Series with `coefs` as the values and `features` as the index,
#sorting the values in descending order
coefs = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_
features = X_train.columns[selected]
pd.Series(coefs, index=features).sort(ascending=False)
57/105:
#Code task 21#
#Get the linear model coefficients from the `coef_` attribute and store in `coefs`,
#get the matching feature names from the column names of the dataframe,
#and display the results as a pandas Series with `coefs` as the values and `features` as the index,
#sorting the values in descending order
coefs = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_
features = X_train.columns[selected]
pd.Series(coefs, index=features).sort_values(ascending=False)
57/106:
#Code task 22#
#Define a pipeline comprising the steps:
#SimpleImputer() with a strategy of 'median'
#StandardScaler(),
#and then RandomForestRegressor() with a random state of 47
RF_pipe = make_pipeline(
    SimpleImputer(strategy='median'),
    StandardScaler(),
    RandomForestRegressor(random_state=47)
)
57/107: selected = lr_grid_cv.best_estimator_.named_steps.selectkbest.get_support()
57/108:
#Code task 21#
#Get the linear model coefficients from the `coef_` attribute and store in `coefs`,
#get the matching feature names from the column names of the dataframe,
#and display the results as a pandas Series with `coefs` as the values and `features` as the index,
#sorting the values in descending order
coefs = lr_grid_cv.best_estimator_.named_steps.linearregression.coef_
features = X_train.columns[selected]
pd.Series(coefs, index=features).sort_values(ascending=False)
57/109:
#Code task 22#
#Define a pipeline comprising the steps:
#SimpleImputer() with a strategy of 'median'
#StandardScaler(),
#and then RandomForestRegressor() with a random state of 47
RF_pipe = make_pipeline(
    SimpleImputer(strategy='median'),
    StandardScaler(),
    RandomForestRegressor(random_state=47)
)
57/110:
#Code task 22#
#Define a pipeline comprising the steps:
#SimpleImputer() with a strategy of 'median'
#StandardScaler(),
#and then RandomForestRegressor() with a random state of 47
RF_pipe = make_pipeline(
    SimpleImputer(strategy='median'),
    StandardScaler(),
    RandomForestRegressor(random_state=47)
)
57/111:
#Code task 23#
#Call `cross_validate` to estimate the pipeline's performance.
#Pass it the random forest pipe object, `X_train` and `y_train`,
#and get it to use 5-fold cross-validation
rf_default_cv_results = cross_validate(RF_pipe, X_train, y_train, cv=5)
57/112:
rf_cv_scores = rf_default_cv_results['test_score']
rf_cv_scores
57/113: np.mean(rf_cv_scores), np.std(rf_cv_scores)
57/114:
n_est = [int(n) for n in np.logspace(start=1, stop=3, num=20)]
grid_params = {
        'randomforestregressor__n_estimators': n_est,
        'standardscaler': [StandardScaler(), None],
        'simpleimputer__strategy': ['mean', 'median']
}
grid_params
57/115:
#Code task 24#
#Call `GridSearchCV` with the random forest pipeline, passing in the above `grid_params`
#dict for parameters to evaluate, 5-fold cross-validation, and all available CPU cores (if desired)
rf_grid_cv = GridSearchCV(RF_pipe, param_grid=grid_params, cv=5, n_jobs=-1)
57/116:
#Code task 25#
#Now call the `GridSearchCV`'s `fit()` method with `X_train` and `y_train` as arguments
#to actually start the grid search. This may take a minute or two.
rf_grid_cv.fit(X_train, y_train)
57/117:
#Code task 24#
#Call `GridSearchCV` with the random forest pipeline, passing in the above `grid_params`
#dict for parameters to evaluate, 5-fold cross-validation, and all available CPU cores (if desired)
rf_grid_cv = GridSearchCV(RF_pipe, param_grid=grid_params, cv=5, n_jobs=-1)
57/118:
#Code task 25#
#Now call the `GridSearchCV`'s `fit()` method with `X_train` and `y_train` as arguments
#to actually start the grid search. This may take a minute or two.
rf_grid_cv.fit(X_train, y_train)
57/119:
#Code task 26#
#Print the best params (`best_params_` attribute) from the grid search
rf_grid_cv.best_params_
57/120:
#Code task 26#
#Print the best params (`best_params_` attribute) from the grid search
rf_grid_cv.best_params_
57/121:
rf_best_cv_results = cross_validate(rf_grid_cv.best_estimator_, X_train, y_train, cv=5)
rf_best_scores = rf_best_cv_results['test_score']
rf_best_scores
57/122: np.mean(rf_best_scores), np.std(rf_best_scores)
57/123:
#Code task 27#
#Plot a barplot of the random forest's feature importances,
#assigning the `feature_importances_` attribute of 
#`rf_grid_cv.best_estimator_.named_steps.randomforestregressor` to the name `imps` to then
#create a pandas Series object of the feature importances, with the index given by the
#training data column names, sorting the values in descending order
plt.subplots(figsize=(10, 5))
imps = rf_grid_cv.best_estimator_.named_steps.randomforestregressor.feature_importances_
rf_feat_imps = pd.Series(imps, index=X_train.columns).sort_values(ascending=False)
rf_feat_imps.plot(kind='bar')
plt.xlabel('features')
plt.ylabel('importance')
plt.title('Best random forest regressor feature importances');
57/124:
# 'neg_mean_absolute_error' uses the (negative of) the mean absolute error
lr_neg_mae = cross_validate(lr_grid_cv.best_estimator_, X_train, y_train, 
                            scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)
57/125:
lr_mae_mean = np.mean(-1 * lr_neg_mae['test_score'])
lr_mae_std = np.std(-1 * lr_neg_mae['test_score'])
lr_mae_mean, lr_mae_std
57/126: mean_absolute_error(y_test, lr_grid_cv.best_estimator_.predict(X_test))
57/127:
rf_neg_mae = cross_validate(rf_grid_cv.best_estimator_, X_train, y_train, 
                            scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)
57/128:
rf_mae_mean = np.mean(-1 * rf_neg_mae['test_score'])
rf_mae_std = np.std(-1 * rf_neg_mae['test_score'])
rf_mae_mean, rf_mae_std
57/129: mean_absolute_error(y_test, rf_grid_cv.best_estimator_.predict(X_test))
57/130:
fractions = [.2, .25, .3, .35, .4, .45, .5, .6, .75, .8, 1.0]
train_size, train_scores, test_scores = learning_curve(pipe, X_train, y_train, train_sizes=fractions)
train_scores_mean = np.mean(train_scores, axis=1)
train_scores_std = np.std(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)
test_scores_std = np.std(test_scores, axis=1)
57/131:
plt.subplots(figsize=(10, 5))
plt.errorbar(train_size, test_scores_mean, yerr=test_scores_std)
plt.xlabel('Training set size')
plt.ylabel('CV scores')
plt.title('Cross-validation score as training set size increases');
57/132:
#Code task 28#
#This may not be "production grade ML deployment" practice, but adding some basic
#information to your saved models can save your bacon in development.
#Just what version model have you just loaded to reuse? What version of `sklearn`
#created it? When did you make it?
#Assign the pandas version number (`pd.__version__`) to the `pandas_version` attribute,
#the numpy version (`np.__version__`) to the `numpy_version` attribute,
#the sklearn version (`sklearn_version`) to the `sklearn_version` attribute,
#and the current datetime (`datetime.datetime.now()`) to the `build_datetime` attribute
#Let's call this model version '1.0'
best_model = rf_grid_cv.best_estimator_
best_model.version = 1.0
best_model.pandas_version = pd.__version__
best_model.numpy_version = np.__version__
best_model.sklearn_version = sklearn_version
best_model.X_columns = [col for col in X_train.columns]
best_model.build_datetime = datetime.datetime.now()
    
modelpath = 'models'
if not os.path.exists(modelpath):
    os.mkdir(modelpath)
skimodel_path = os.path.join(modelpath, 'ski_resort_pricing_model.pkl')
if not os.path.exists(skimodel_path):
    with open(skimodel_path, 'wb') as f:
        pickle.dump(best_model, f)
58/1:
import pandas as pd
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import __version__ as sklearn_version
from sklearn.model_selection import cross_validate
58/2:
# This isn't exactly production-grade, but a quick check for development
# These checks can save some head-scratching in development when moving from
# one python environment to another, for example
expected_model_version = '1.0'
model_path = 'models/ski_resort_pricing_model.pkl'
if os.path.exists(model_path):
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    if model.version != expected_model_version:
        print("Expected model version doesn't match version loaded")
    if model.sklearn_version != sklearn_version:
        print("Warning: model created under different sklearn version")
else:
    print("Expected model not found")
58/3: os.pwd
58/4: os.getcd
58/5: os.cd
58/6:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data'
os.chdir(path)
58/7:
# This isn't exactly production-grade, but a quick check for development
# These checks can save some head-scratching in development when moving from
# one python environment to another, for example
expected_model_version = '1.0'
model_path = 'models/ski_resort_pricing_model.pkl'
if os.path.exists(model_path):
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    if model.version != expected_model_version:
        print("Expected model version doesn't match version loaded")
    if model.sklearn_version != sklearn_version:
        print("Warning: model created under different sklearn version")
else:
    print("Expected model not found")
58/8: ski_data = pd.read_csv('data/ski_data_step3_features.csv')
58/9: big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']
58/10: big_mountain.T
61/1:
import pandas as pd
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import __version__ as sklearn_version
from sklearn.decomposition import PCA
from sklearn.preprocessing import scale
from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, learning_curve
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.dummy import DummyRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_regression
import datetime
61/2:
path= '/Users/josephfrasca/Coding Stuff/Springboard/DataScienceGuidedCapstone/raw_data'
os.chdir(path)
61/3: os.listdir()
61/4:
ski_data = pd.read_csv('data/ski_data_step3_features.csv')
ski_data.head().T
61/5: big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']
61/6: big_mountain.T
61/7: ski_data.shape
61/8: ski_data = ski_data[ski_data.Name != 'Big Mountain Resort']
61/9: ski_data.shape
61/10: len(ski_data) * .7, len(ski_data) * .3
61/11:
X_train, X_test, y_train, y_test = train_test_split(ski_data.drop(columns='AdultWeekend'), 
                                                    ski_data.AdultWeekend, test_size=0.3, 
                                                    random_state=47)
61/12: X_train.shape, X_test.shape
61/13: y_train.shape, y_test.shape
61/14:
#Code task 1#
#Save the 'Name', 'state', and 'Region' columns from the train/test data into names_train and names_test
#Then drop those columns from `X_train` and `X_test`. Use 'inplace=True'
names_list = ['Name', 'state', 'Region']
names_train = X_train[names_list]
names_test = X_test[names_list]
X_train.drop(columns=names_list, inplace=True)
X_test.drop(columns=names_list, inplace=True)
X_train.shape, X_test.shape
61/15:
#Code task 2#
#Check the `dtypes` attribute of `X_train` to verify all features are numeric
X_train.dtypes
61/16:
#Code task 3#
#Repeat this check for the test split in `X_test`
X_test.dtypes
61/17:
#Code task 4#
#Calculate the mean of `y_train`
train_mean = y_train.mean()
train_mean
61/18:
#Code task 5#
#Fit the dummy regressor on the training data
#Hint, call its `.fit()` method with `X_train` and `y_train` as arguments
#Then print the object's `constant_` attribute and verify it's the same as the mean above
dumb_reg = DummyRegressor(strategy='mean')
dumb_reg.fit(X_train, y_train)
dumb_reg.constant_
61/19:
#Code task 6#
#Calculate the R^2 as defined above
def r_squared(y, ypred):
    """R-squared score.
    
    Calculate the R-squared, or coefficient of determination, of the input.
    
    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    ybar = np.sum(y) / len(y) #yes, we could use np.mean(y)
    sum_sq_tot = np.sum((y - ybar)**2) #total sum of squares error
    sum_sq_res = np.sum((y - ypred)**2) #residual sum of squares error
    R2 = 1.0 - sum_sq_res / sum_sq_tot
    return R2
61/20:
y_tr_pred_ = train_mean * np.ones(len(y_train))
y_tr_pred_[:5]
61/21:
y_tr_pred = dumb_reg.predict(X_train)
y_tr_pred[:5]
61/22: r_squared(y_train, y_tr_pred)
61/23:
y_te_pred = train_mean * np.ones(len(y_test))
r_squared(y_test, y_te_pred)
61/24:
#Code task 7#
#Calculate the MAE as defined above
def mae(y, ypred):
    """Mean absolute error.
    
    Calculate the mean absolute error of the arguments

    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    abs_error = np.abs(y - ypred)
    mae = np.mean(abs_error)
    return mae
61/25: mae(y_train, y_tr_pred)
61/26: mae(y_test, y_te_pred)
61/27:
#Code task 8#
#Calculate the MSE as defined above
def mse(y, ypred):
    """Mean square error.
    
    Calculate the mean square error of the arguments

    Arguments:
    y -- the observed values
    ypred -- the predicted values
    """
    sq_error = (y - ypred)**2
    mse = np.mean(sq_error)
    return mse
61/28: mse(y_train, y_tr_pred)
61/29: mse(y_test, y_te_pred)
61/30: np.sqrt([mse(y_train, y_tr_pred), mse(y_test, y_te_pred)])
61/31: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
61/32: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
61/33: mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)
61/34:
# train set - sklearn
# correct order, incorrect order
r2_score(y_train, y_tr_pred), r2_score(y_tr_pred, y_train)
61/35:
# test set - sklearn
# correct order, incorrect order
r2_score(y_test, y_te_pred), r2_score(y_te_pred, y_test)
61/36:
# train set - using our homebrew function
# correct order, incorrect order
r_squared(y_train, y_tr_pred), r_squared(y_tr_pred, y_train)
61/37:
# test set - using our homebrew function
# correct order, incorrect order
r_squared(y_test, y_te_pred), r_squared(y_te_pred, y_test)
61/38:
# These are the values we'll use to fill in any missing values
X_defaults_median = X_train.median()
X_defaults_median
61/39:
#Code task 9#
#Call `X_train` and `X_test`'s `fillna()` method, passing `X_defaults_median` as the values to use
#Assign the results to `X_tr` and `X_te`, respectively
X_tr = X_train.fillna(X_defaults_median)
X_te = X_test.fillna(X_defaults_median)
61/40:
#Code task 10#
#Call the StandardScaler`s fit method on `X_tr` to fit the scaler
#then use it's `transform()` method to apply the scaling to both the train and test split
#data (`X_tr` and `X_te`), naming the results `X_tr_scaled` and `X_te_scaled`, respectively
scaler = StandardScaler()
scaler.fit(X_tr)
X_tr_scaled = scaler.transform(X_tr)
X_te_scaled = scaler.transform(X_te)
61/41: lm = LinearRegression().fit(X_tr_scaled, y_train)
61/42:
#Code task 11#
#Call the `predict()` method of the model (`lm`) on both the (scaled) train and test data
#Assign the predictions to `y_tr_pred` and `y_te_pred`, respectively
y_tr_pred = lm.predict(X_tr_scaled)
y_te_pred = lm.predict(X_te_scaled)
61/43:
# r^2 - train, test
median_r2 = r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
median_r2
61/44:
#Code task 12#
#Now calculate the mean absolute error scores using `sklearn`'s `mean_absolute_error` function
# as we did above for R^2
# MAE - train, test
median_mae = mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
median_mae
61/45:
#Code task 13#
#And also do the same using `sklearn`'s `mean_squared_error`
# MSE - train, test
median_mse = mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)
median_mse
61/46:
#Code task 14#
#As we did for the median above, calculate mean values for imputing missing values
# These are the values we'll use to fill in any missing values
X_defaults_mean = X_train.mean()
X_defaults_mean
61/47:
X_tr = X_train.fillna(X_defaults_mean)
X_te = X_test.fillna(X_defaults_mean)
61/48:
scaler = StandardScaler()
scaler.fit(X_tr)
X_tr_scaled = scaler.transform(X_tr)
X_te_scaled = scaler.transform(X_te)
61/49: lm = LinearRegression().fit(X_tr_scaled, y_train)
61/50:
y_tr_pred = lm.predict(X_tr_scaled)
y_te_pred = lm.predict(X_te_scaled)
61/51: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
61/52: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
61/53: mean_squared_error(y_train, y_tr_pred), mean_squared_error(y_test, y_te_pred)
61/54:
pipe = make_pipeline(
    SimpleImputer(strategy='median'), 
    StandardScaler(), 
    LinearRegression()
)
61/55: type(pipe)
61/56: hasattr(pipe, 'fit'), hasattr(pipe, 'predict')
61/57:
#Code task 15#
#Call the pipe's `fit()` method with `X_train` and `y_train` as arguments
pipe.fit(X_train, y_train)
61/58:
y_tr_pred = pipe.predict(X_train)
y_te_pred = pipe.predict(X_test)
61/59: r2_score(y_train, y_tr_pred), r2_score(y_test, y_te_pred)
61/60: median_r2
61/61: mean_absolute_error(y_train, y_tr_pred), mean_absolute_error(y_test, y_te_pred)
61/62: Compare with your earlier result:
58/11:
# This isn't exactly production-grade, but a quick check for development
# These checks can save some head-scratching in development when moving from
# one python environment to another, for example
expected_model_version = '1.0'
model_path = 'models/ski_resort_pricing_model.pkl'
if os.path.exists(model_path):
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    if model.version != expected_model_version:
        print("Expected model version doesn't match version loaded")
    if model.sklearn_version != sklearn_version:
        print("Warning: model created under different sklearn version")
else:
    print("Expected model not found")
58/12: ski_data = pd.read_csv('data/ski_data_step3_features.csv')
58/13: big_mountain = ski_data[ski_data.Name == 'Big Mountain Resort']
58/14: big_mountain.T
58/15:
X = ski_data.loc[ski_data.Name != "Big Mountain Resort", model.X_columns]
y = ski_data.loc[ski_data.Name != "Big Mountain Resort", 'AdultWeekend']
58/16:
X = ski_data.loc[ski_data.Name != "Big Mountain Resort", model.X_columns]
y = ski_data.loc[ski_data.Name != "Big Mountain Resort", 'AdultWeekend']
58/17: len(X), len(y)
58/18: model.fit(X, y)
58/19: cv_results = cross_validate(model, X, y, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)
58/20: cv_results = cross_validate(model, X, y, scoring='neg_mean_absolute_error', cv=5, n_jobs=-1)
58/21: cv_results['test_score']
58/22:
mae_mean, mae_std = np.mean(-1 * cv_results['test_score']), np.std(-1 * cv_results['test_score'])
mae_mean, mae_std
58/23:
mae_mean, mae_std = np.mean(-1 * cv_results['test_score']), np.std(-1 * cv_results['test_score'])
mae_mean, mae_std
58/24:
mae_mean, mae_std = np.mean(-1 * cv_results['test_score']), np.std(-1 * cv_results['test_score'])
mae_mean, mae_std
58/25: cv_results['test_score']
58/26:
mae_mean, mae_std = np.mean(-1 * cv_results['test_score']), np.std(-1 * cv_results['test_score'])
mae_mean, mae_std
58/27:
mae_mean, mae_std = np.mean(-1 * cv_results['test_score']), np.std(-1 * cv_results['test_score'])
mae_mean, mae_std
58/28:
X_bm = ski_data.loc[ski_data.Name == "Big Mountain Resort", model.X_columns]
y_bm = ski_data.loc[ski_data.Name == "Big Mountain Resort", 'AdultWeekend']
58/29: bm_pred = model.predict(X_bm).item()
58/30: y_bm = y_bm.values.item()
58/31:
print(f'Big Mountain Resort modelled price is ${bm_pred:.2f}, actual price is ${y_bm:.2f}.')
print(f'Even with the expected mean absolute error of ${mae_mean:.2f}, this suggests there is room for an increase.')
58/32:
#Code task 1#
#Add code to the `plot_compare` function that displays a vertical, dashed line
#on the histogram to indicate Big Mountain's position in the distribution
#Hint: plt.axvline() plots a vertical line, its position for 'feature1'
#would be `big_mountain['feature1'].values, we'd like a red line, which can be
#specified with c='r', a dashed linestyle is produced by ls='--',
#and it's nice to give it a slightly reduced alpha value, such as 0.8.
#Don't forget to give it a useful label (e.g. 'Big Mountain') so it's listed
#in the legend.
def plot_compare(feat_name, description, state=None, figsize=(10, 5)):
    """Graphically compare distributions of features.
    
    Plot histogram of values for all resorts and reference line to mark
    Big Mountain's position.
    
    Arguments:
    feat_name - the feature column name in the data
    description - text description of the feature
    state - select a specific state (None for all states)
    figsize - (optional) figure size
    """
    
    plt.subplots(figsize=figsize)
    # quirk that hist sometimes objects to NaNs, sometimes doesn't
    # filtering only for finite values tidies this up
    if state is None:
        ski_x = ski_data[feat_name]
    else:
        ski_x = ski_data.loc[ski_data.state == state, feat_name]
    ski_x = ski_x[np.isfinite(ski_x)]
    plt.hist(ski_x, bins=30)
    plt.axvline(x=big_mountain[feat_name].values, c='r', ls='--', alpha=0.8, label='Big Mountain')
    plt.xlabel(description)
    plt.ylabel('frequency')
    plt.title(description + ' distribution for resorts in market share')
    plt.legend()
58/33:
#Code task 1#
#Add code to the `plot_compare` function that displays a vertical, dashed line
#on the histogram to indicate Big Mountain's position in the distribution
#Hint: plt.axvline() plots a vertical line, its position for 'feature1'
#would be `big_mountain['feature1'].values, we'd like a red line, which can be
#specified with c='r', a dashed linestyle is produced by ls='--',
#and it's nice to give it a slightly reduced alpha value, such as 0.8.
#Don't forget to give it a useful label (e.g. 'Big Mountain') so it's listed
#in the legend.
def plot_compare(feat_name, description, state=None, figsize=(10, 5)):
    """Graphically compare distributions of features.
    
    Plot histogram of values for all resorts and reference line to mark
    Big Mountain's position.
    
    Arguments:
    feat_name - the feature column name in the data
    description - text description of the feature
    state - select a specific state (None for all states)
    figsize - (optional) figure size
    """
    
    plt.subplots(figsize=figsize)
    # quirk that hist sometimes objects to NaNs, sometimes doesn't
    # filtering only for finite values tidies this up
    if state is None:
        ski_x = ski_data[feat_name]
    else:
        ski_x = ski_data.loc[ski_data.state == state, feat_name]
    ski_x = ski_x[np.isfinite(ski_x)]
    plt.hist(ski_x, bins=30)
    plt.axvline(x=big_mountain[feat_name].values, c='r', ls='--', alpha=0.8, label='Big Mountain')
    plt.xlabel(description)
    plt.ylabel('frequency')
    plt.title(description + ' distribution for resorts in market share')
    plt.legend()
58/34: plot_compare('AdultWeekend', 'Adult weekend ticket price ($)')
58/35: plot_compare('AdultWeekend', 'Adult weekend ticket price ($)')
58/36: plot_compare('AdultWeekend', 'Adult weekend ticket price ($) - Montana only', state='Montana')
58/37: plot_compare('vertical_drop', 'Vertical drop (feet)')
58/38: plot_compare('Snow Making_ac', 'Area covered by snow makers (acres)')
58/39: plot_compare('total_chairs', 'Total number of chairs')
58/40: plot_compare('fastQuads', 'Number of fast quads')
58/41: plot_compare('Runs', 'Total number of runs')
58/42: plot_compare('LongestRun_mi', 'Longest run length (miles)')
58/43: plot_compare('trams', 'Number of trams')
58/44: plot_compare('SkiableTerrain_ac', 'Skiable terrain area (acres)')
58/45: expected_visitors = 350_000
58/46:
all_feats = ['vertical_drop', 'Snow Making_ac', 'total_chairs', 'fastQuads', 
             'Runs', 'LongestRun_mi', 'trams', 'SkiableTerrain_ac']
big_mountain[all_feats]
58/47: expected_visitors = 350_000
58/48:
all_feats = ['vertical_drop', 'Snow Making_ac', 'total_chairs', 'fastQuads', 
             'Runs', 'LongestRun_mi', 'trams', 'SkiableTerrain_ac']
big_mountain[all_feats]
58/49:
#Code task 2#
#In this function, copy the Big Mountain data into a new data frame
#(Note we use .copy()!)
#And then for each feature, and each of its deltas (changes from the original),
#create the modified scenario dataframe (bm2) and make a ticket price prediction
#for it. The difference between the scenario's prediction and the current
#prediction is then calculated and returned.
#Complete the code to increment each feature by the associated delta
def predict_increase(features, deltas):
    """Increase in modelled ticket price by applying delta to feature.
    
    Arguments:
    features - list, names of the features in the ski_data dataframe to change
    deltas - list, the amounts by which to increase the values of the features
    
    Outputs:
    Amount of increase in the predicted ticket price
    """
    
    bm2 = X_bm.copy()
    for f, d in zip(features, deltas):
        bm2[f] += d
    return model.predict(bm2).item() - model.predict(X_bm).item()
58/50: [i for i in range(-1, -11, -1)]
58/51:
runs_delta = [i for i in range(-1, -11, -1)]
price_deltas = [predict_increase(['Runs'], [delta]) for delta in runs_delta]
58/52: price_deltas
58/53:
#Code task 3#
#Create two plots, side by side, for the predicted ticket price change (delta) for each
#condition (number of runs closed) in the scenario and the associated predicted revenue
#change on the assumption that each of the expected visitors buys 5 tickets
#There are two things to do here:
#1 - use a list comprehension to create a list of the number of runs closed from `runs_delta`
#2 - use a list comprehension to create a list of predicted revenue changes from `price_deltas`
runs_closed = [-1 * x for x in runs_delta] #1
fig, ax = plt.subplots(1, 2, figsize=(10, 5))
fig.subplots_adjust(wspace=0.5)
ax[0].plot(runs_closed, price_deltas, 'o-')
ax[0].set(xlabel='Runs closed', ylabel='Change ($)', title='Ticket price')
revenue_deltas = [5 * expected_visitors * x for x in price_deltas] #2
ax[1].plot(runs_closed, revenue_deltas, 'o-')
ax[1].set(xlabel='Runs closed', ylabel='Change ($)', title='Revenue');
58/54:
#Code task 4#
#Call `predict_increase` with a list of the features 'Runs', 'vertical_drop', and 'total_chairs'
#and associated deltas of 1, 150, and 1
ticket2_increase = predict_increase(['Runs', 'vertical_drop', 'total_chairs'], [1, 150, 1])
revenue2_increase = 5 * expected_visitors * ticket2_increase
58/55:
print(f'This scenario increases support for ticket price by ${ticket2_increase:.2f}')
print(f'Over the season, this could be expected to amount to ${revenue2_increase:.0f}')
58/56:
#Code task 5#
#Repeat scenario 2 conditions, but add an increase of 2 to `Snow Making_ac`
ticket3_increase = predict_increase(['Runs', 'vertical_drop', 'total_chairs', 'Snow Making_ac'], [1, 150, 1, 2])
revenue3_increase = 5 * expected_visitors * ticket3_increase
58/57:
print(f'This scenario increases support for ticket price by ${ticket3_increase:.2f}')
print(f'Over the season, this could be expected to amount to ${revenue3_increase:.0f}')
58/58:
#Code task 6#
#Predict the increase from adding 0.2 miles to `LongestRun_mi` and 4 to `Snow Making_ac`
predict_increase(['LongestRun_mi', 'Snow Making_ac'], [.2, 4])
66/1:
# First, import the relevant modules
import requests
import collections
66/2:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse
# into the JSON structure that will be returned
url = '"https://www.quandl.com/api/v3/datasets/WIKI/AFX_X/data.json?api_key=API_KEY"'
r = requests.get(url)
print(r.text.head())
66/3:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse
# into the JSON structure that will be returned
url = 'https://www.quandl.com/api/v3/datasets/WIKI/AFX_X/data.json?api_key=API_KEY'
r = requests.get(url)
print(r.text.head())
66/4:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse
# into the JSON structure that will be returned
url = 'https://www.quandl.com/api/v3/datasets/WIKI/AFX_X/data.json?api_key=API_KEY'
r = requests.get(url)
print(r.text)
66/5:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse
# into the JSON structure that will be returned
url = 'https://www.quandl.com/api/v3/datasets/WIKI/AFX_X/data.json?api_key=ys6wt6HQEhLdhmxQYSzn'
r = requests.get(url)
print(r.text)
66/6:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse
# into the JSON structure that will be returned
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=API_KEY'
r = requests.get(url)
print(r.text)
66/7:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse
# into the JSON structure that will be returned
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=ys6wt6HQEhLdhmxQYSzn'
r = requests.get(url)
print(r.text)
66/8:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse
# into the JSON structure that will be returned
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=ys6wt6HQEhLdhmxQYSzn'
r = requests.get(url, limit=n)
print(r.text)
66/9:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse
# into the JSON structure that will be returned
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=ys6wt6HQEhLdhmxQYSzn'
r = requests.get(url, limit=1)
print(r.text)
66/10:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse
# into the JSON structure that will be returned
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?api_key=ys6wt6HQEhLdhmxQYSzn'
r = requests.get(url)
print(r.text)
66/11:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse
# into the JSON structure that will be returned
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1api_key=ys6wt6HQEhLdhmxQYSzn'
r = requests.get(url)
print(r.text)
66/12:
# Inspect the JSON structure of the object you created, and take note of how nested it is,
# as well as the overall structure
json_data = r.json()
66/13:
# Inspect the JSON structure of the object you created, and take note of how nested it is,
# as well as the overall structure
json_data = r.json()
for k in json_data.keys():
    print(k + ':', json_data[k])
66/14:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse
# into the JSON structure that will be returned
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1&api_key=ys6wt6HQEhLdhmxQYSzn'
r = requests.get(url)
print(r.text)
66/15:
#Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
url2 = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=ys6wt6HQEhLdhmxQYSzn'
r2 = requests.get(url2)
print(r2.text)
66/16:
#Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
url2 = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=ys6wt6HQEhLdhmxQYSzn'
r2 = requests.get(url2)
print(r2.text)
66/17:
#2. Convert the returned JSON object into a Python dictionary.
json_data = r2.json()
66/18:
#2. Convert the returned JSON object into a Python dictionary.
json_data2 = r2.json()

# Print each key-value pair in json_data
for k in json2_data.keys():
    print(k + ': ', json2_data[k])
66/19:
#2. Convert the returned JSON object into a Python dictionary.
json_data2 = r2.json()

# Print each key-value pair in json_data
for k in json_data2.keys():
    print(k + ': ', json2_data[k])
66/20:
#2. Convert the returned JSON object into a Python dictionary.
json_data2 = r2.json()

# Print each key-value pair in json_data
for k in json_data2.keys():
    print(k + ': ', json_data2[k])
66/21:
#2. Convert the returned JSON object into a Python dictionary.
json_data2 = r2.json()
66/22:
#2. Convert the returned JSON object into a Python dictionary.
json_data2 = r2.json()
66/23:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2.max()
66/24:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2['column_names']['Open']
66/25:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2["column_names"]['Open']
66/26:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max = json_data2["column_names"]['Open']
print(max)
66/27:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max = json_data2['column_names']['Open']
print(max)
66/28:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max = json_data2['column_names']
print(max)
66/29:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2[dataset_data]['column_names']
66/30:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2['dataset_data']['column_names']
66/31:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2['dataset_data']['column_names']['Open']
66/32:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2['dataset_data']['column_names']['Open'].max()
66/33:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2['dataset_data']['column_names']['Open'].max()
66/34:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2['dataset_data']['column_names']['Open']
66/35:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2['dataset_data']['column_names']
66/36:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max = json_data2['dataset_data']['column_names']['Open']
print(max)
66/37:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max = json_data2['dataset_data']['column_names']
print(max)
66/38:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max = json_data2['dataset_data']['column_names']['Open']
print(max)
66/39:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max = int(json_data2['dataset_data']['column_names']['Open'])
print(max)
66/40:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2['dataset_data']['column_names']['Open'].dtype
66/41:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2['dataset_data']['column_names']int(['Open'])
66/42:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2['dataset_data']['column_names']['Open']
66/43:
#2. Convert the returned JSON object into a Python dictionary.
json_data2 = r2.json()

# Print each key-value pair in json_data2
for k in json_data2.keys():
    print(k + ': ', json_data2[k])
66/44:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data2['dataset_data']['data']
66/45:
#1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
url17 = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=ys6wt6HQEhLdhmxQYSzn'
r17 = requests.get(url17)
print(r17.text)
66/46:
#2. Convert the returned JSON object into a Python dictionary.
json_data17 = r17.json()

# Print each key-value pair in json_data2
for k in json_data17.keys():
    print(k + ': ', json_data17[k])
66/47:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data17['dataset_data']['data']
66/48:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
json_data17['dataset_data']['data']
66/49:
data_2017 = json_data17['dataset_data']['data']
col_names = json_data17['dataset']['column_names']
66/50:
data_2017 = json_data17['dataset_data']['data']
col_names = json_data17['dataset_data']['column_names']
66/51:
data_2017 = json_data17['dataset_data']['data']
col_names = json_data17['dataset_data']['column_names']
66/52:
data_2017 = json_data17['dataset_data']['data']
col_names = json_data17['dataset_data']['column_names']
66/53: res = dict(zip(col_names, data_2017))
66/54:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
print(res)
66/55:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
print(json_data17['dataset_data']['data'][2])
66/56:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
print(json_data17['dataset_data']['data'][1])
66/57:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
print(json_data17['dataset_data']['data'][][2])
66/58:
#create keys/values for new dictionary 
data_2017 = json_data17['dataset_data']['data']
col_names = json_data17['dataset_data']['column_names']
print(data_2017)
66/59:
#create keys/values for new dictionary 
data_2017 = json_data17['dataset_data']['data']
col_names = json_data17['dataset_data']['column_names']
print(col_names)
66/60:
 # extract 'Open' from each sublist in a list of lists 
def Extract(lst): 
    return [item[1] for item in lst] 

print(Extract(data_2017))
66/61:
#create keys/values for new dictionary 
data_2017 = json_data17['dataset_data']['data']
col_names = json_data17['dataset_data']['column_names']
print(data_2017)
print(col_names)
66/62:
 # extract Opening Price from each sublist in a list of lists 
def Extract(lst): 
    return [item[1] for item in lst] 
open = Extract(data_2017)
print(open)
66/63:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max(open)
66/64:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max(open)
66/65:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
print(max(open))
66/66:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max = max(open)
print(max)
66/67:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max = list(max(open))
print(max)
66/68:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
open.max()
66/69:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max(open)
66/70: open_17 = list(filter(None, open))
66/71:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max(open_17)
66/72:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max([open_17])
66/73:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max[open_17]
66/74:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max(open_17)
66/75:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
res_max = max(float(sub) for sub in open_17)
66/76:
open_17 = list(filter(None, open)) 
print(type(open_17))
66/77:
open_17 = list(filter(None, open)) 
print(open_17)
66/78:
open_17 = list(filter(None, open)) 
print(open_17)
66/79: max(open_17)
66/80:
 # extract Opening Price from each sublist in a list of lists 
def Extract(lst): 
    return [item[1] for item in lst] 
open_price = Extract(data_2017)
print(max(open_price)
66/81:
 # extract Opening Price from each sublist in a list of lists 
def Extract(lst): 
    return [item[1] for item in lst] 
open_price = Extract(data_2017)
print(max(open_price))
66/82:
 # extract Opening Price from each sublist in a list of lists 
def Extract(lst): 
    return [item[1] for item in lst] 
open_price = Extract(data_2017)
print(open_price)
66/83:
open_17 = list(filter(None, open_price)) 
print(open_17)
66/84: max(open_17)
66/85: max(open_price)
67/1:
# Store the API key as a string - according to PEP8, constants are always named in all upper case
API_KEY = 'ys6wt6HQEhLdhmxQYSzn'
67/2:
# First, import the relevant modules
import requests
import collections
67/3:
# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse
# into the JSON structure that will be returned
url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?limit=1&api_key=ys6wt6HQEhLdhmxQYSzn'
r = requests.get(url)
print(r.text)
67/4:
# Inspect the JSON structure of the object you created, and take note of how nested it is,
# as well as the overall structure
json_data = r.json()
for k in json_data.keys():
    print(k + ':', json_data[k])
67/5:
#1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017
url17 = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X/data.json?start_date=2017-01-01&end_date=2017-12-31&api_key=ys6wt6HQEhLdhmxQYSzn'
r17 = requests.get(url17)
print(r17.text)
67/6:
#2. Convert the returned JSON object into a Python dictionary.
json_data17 = r17.json()

# Print each key-value pair in json_data2
for k in json_data17.keys():
    print(k + ': ', json_data17[k])
67/7:
#create keys/values for new dictionary 
data_2017 = json_data17['dataset_data']['data']
col_names = json_data17['dataset_data']['column_names']
print(data_2017)
print(col_names)
67/8:
 # extract Opening Price from each sublist in a list of lists 
def Extract(lst): 
    return [item[1] for item in lst] 
open_price = Extract(data_2017)
print(open_price)
67/9:
open_17 = list(filter(None, open_price)) 
print(open_17)
67/10: max(open_price)
67/11: max(open_17)
67/12:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max17 = max(open_17)
min17 = min(open_17)
print('The highest opening price is:' + max17)
67/13:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max17 = max(open_17)
min17 = min(open_17)
print('The highest opening price is:', max17)
67/14:
#3. Calculate what the highest and lowest opening prices were for the stock in this period.
max17 = max(open_17)
min17 = min(open_17)
print('The highest opening price is:', max17)
print('The lowest opening price is:', min17)
67/15:
 # extract Opening Price from each sublist in a list of lists 
def Extract(lst, n): 
    return [item[n] for item in lst] 
open_price = Extract(data_2017, 1)
print(open_price)
67/16:
'''
#create keys/values for new dictionary 
data_2017 = json_data17['dataset_data']['data']
col_names = json_data17['dataset_data']['column_names']
print(data_2017)
print(col_names)
''''''
67/17:
'''
#create keys/values for new dictionary 
data_2017 = json_data17['dataset_data']['data']
col_names = json_data17['dataset_data']['column_names']
print(data_2017)
print(col_names)
'''
67/18:
#create keys/values for new dictionary 
data_2017 = json_data17['dataset_data']['data']
col_names = json_data17['dataset_data']['column_names']
print(col_names)
67/19:
#separate data list and column names list from dictionary
data_2017 = json_data17['dataset_data']['data']
col_names = json_data17['dataset_data']['column_names']
print(col_names)
67/20:
#6. What was the average daily trading volume during this year?

# Extract Trading Volume from each sublist in a list of lists 
trade_vol = Extract(data_2017, 6)
print(trade_vol)
67/21:
#6. What was the average daily trading volume during this year?

# Extract Trading Volume from each sublist in a list of lists 
trade_vol = Extract(data_2017, 6)
print(trade_vol)
67/22: mean(trade_vol)
67/23:
avg_trade_vol = sum(trade_vol)/len(trade_vol)
print(avg_trade_vol)
67/24:
avg_trade_vol = sum(trade_vol)/len(trade_vol)
print('The average trade volume during 2017 is:', avg_trade_vol)
67/25:
#4. What was the largest change in any one day (based on High and Low price)?
high = Extract(data_2017, 2)
low = Extract(data_2017, 3)
print(high)
print(low)
67/26: print high - low
67/27: print(high - low)
67/28: print([a_i - b_i for a_i, b_i in zip(high, low)])
67/29:
change = [a_i - b_i for a_i, b_i in zip(high, low)]
print('The largest change in done day is:', max(change))
67/30:
#subtract stock highs from stocks lows to get change in stock price list
change = [a_i - b_i for a_i, b_i in zip(high, low)]
print('The largest change in one day is:', max(change))
67/31:
#5. What was the largest change between any two days (based on Closing Price)?

close = Extract(data_2017, 4)
67/32:
#5. What was the largest change between any two days (based on Closing Price)?

close = Extract(data_2017, 4)
print(close)
67/33:
#5. What was the largest change between any two days (based on Closing Price)?

#extract Closing Price data
close = Extract(data_2017, 4)
print(close)
67/34:
# Separating odd and even index elements
odd = close[::2]
even = close[1::2]
print(odd)
67/35:
# Separating odd and even index elements
odd = close[::2]
even = close[1::2]
print(even)
67/36:
#subtract to get 2 day differences (subtracting successive numbers from even list and odd list)
odd_diff = [j - i for i, j in zip(odd[: -1], odd[1 :])] 
even_diff = [j - i for i, j in zip(even[: -1], even[1 :])]
67/37:
#subtract to get 2 day differences (subtracting successive numbers from even list and odd list)
odd_diff = [j - i for i, j in zip(odd[: -1], odd[1 :])] 
even_diff = [j - i for i, j in zip(even[: -1], even[1 :])] 
print(odd_dff)
67/38:
#subtract to get 2 day differences (subtracting successive numbers from even list and odd list)
odd_diff = [j - i for i, j in zip(odd[: -1], odd[1 :])] 
even_diff = [j - i for i, j in zip(even[: -1], even[1 :])] 
print(odd_diff)
67/39:
#subtract to get 2 day differences (subtracting successive numbers from even list and odd list)
odd_diff = [j - i for i, j in zip(odd[: -1], odd[1 :])] 
even_diff = [j - i for i, j in zip(even[: -1], even[1 :])] 
print(even_diff)
67/40:
#subtract to get 2 day differences (subtracting successive numbers from even list and odd list)
odd_diff = [j - i for i, j in zip(odd[: -1], odd[1 :])] 
even_diff = [j - i for i, j in zip(even[: -1], even[1 :])] 
#recombine 2 lists to get 2 day differences 
two_day_diff = odd_diff + even_diff 
print(two_day_diff)
67/41:
#subtract to get 2 day differences (subtracting successive numbers from even list and odd list)
odd_diff = [j - i for i, j in zip(odd[: -1], odd[1 :])] 
even_diff = [j - i for i, j in zip(even[: -1], even[1 :])] 
#recombine 2 lists to get 2 day differences 
two_day_diff = odd_diff + even_diff 
print(two_day_diff)
len(two_day_diff)
67/42:
#subtract to get 2 day differences (subtracting successive numbers from even list and odd list)
odd_diff = [j - i for i, j in zip(odd[: -1], odd[1 :])] 
even_diff = [j - i for i, j in zip(even[: -1], even[1 :])] 
#recombine 2 lists to get 2 day differences 
two_day_diff = odd_diff + even_diff 
len(two_day_diff)
67/43:
#subtract to get 2 day differences (subtracting successive numbers from even list and odd list)
odd_diff = [j - i for i, j in zip(odd[: -1], odd[1 :])] 
even_diff = [j - i for i, j in zip(even[: -1], even[1 :])] 
#recombine 2 lists to get 2 day differences 
two_day_diff = odd_diff + even_diff 
len(two_day_diff)
len(close)
67/44:
#subtract to get 2 day differences (subtracting successive numbers from even list and odd list)
odd_diff = [j - i for i, j in zip(odd[: -1], odd[1 :])] 
even_diff = [j - i for i, j in zip(even[: -1], even[1 :])] 
#recombine 2 lists to get 2 day differences 
two_day_diff = odd_diff + even_diff 
print(len(two_day_diff)
print(len(close))
67/45:
#subtract to get 2 day differences (subtracting successive numbers from even list and odd list)
odd_diff = [j - i for i, j in zip(odd[: -1], odd[1 :])] 
even_diff = [j - i for i, j in zip(even[: -1], even[1 :])] 
#recombine 2 lists to get 2 day differences 
two_day_diff = odd_diff + even_diff 
print(len(two_day_diff))
print(len(close))
67/46:
#
avg_2daydiff= sum(two_day_diff)/len(two_day_diff)
67/47:
#
avg_2daydiff= sum(two_day_diff)/len(two_day_diff)
print(avg_2daydiff)
67/48:
#
max_2daydiff= max(two_day_diff)
print(2daydiff)
67/49:
#
max_2daydiff= max(two_day_diff)
print(max_2daydiff)
67/50:
#calculate largest change between any two days
max_2daydiff= max(two_day_diff)
print('T largest change between any two days (based on Closing Price) is:', max_2daydiff)
67/51:
#calculate largest change between any two days
max_2daydiff= max(two_day_diff)
print('The largest change between any two days (based on Closing Price) is:', max_2daydiff)
67/52:
#7. (Optional) What was the median trading volume during this year. 
# (Note: you may need to implement your own function for calculating the median.)

def median(lst):
    n = len(lst)
    s = sorted(lst)
    return (sum(s[n//2-1:n//2+1])/2.0, s[n//2])[n % 2] if n else None

median(trade_vol)
67/53:
#7. (Optional) What was the median trading volume during this year. 
# (Note: you may need to implement your own function for calculating the median.)

def median(lst):
    n = len(lst)
    s = sorted(lst)
    return (sum(s[n//2-1:n//2+1])/2.0, s[n//2])[n % 2] if n else None

print('The median trading volume for 2017 is:', (trade_vol))
67/54:
#7. (Optional) What was the median trading volume during this year. 
# (Note: you may need to implement your own function for calculating the median.)

def median(lst):
    n = len(lst)
    s = sorted(lst)
    return (sum(s[n//2-1:n//2+1])/2.0, s[n//2])[n % 2] if n else None

print('The median trading volume for 2017 is:', median(trade_vol))
70/1:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
70/2:
file_name = cache_file(
    "meteorites.csv",
    "https://data.nasa.gov/api/views/gh4g-9sfh/rows.csv?accessType=DOWNLOAD",
)
    
df = pd.read_csv(file_name)
    
# Note: Pandas does not support dates before 1880, so we ignore these for this analysis
df['year'] = pd.to_datetime(df['year'], errors='coerce')

# Example: Constant variable
df['source'] = "NASA"

# Example: Boolean variable
df['boolean'] = np.random.choice([True, False], df.shape[0])

# Example: Mixed with base types
df['mixed'] = np.random.choice([1, "A"], df.shape[0])

# Example: Highly correlated variables
df['reclat_city'] = df['reclat'] + np.random.normal(scale=5,size=(len(df)))

# Example: Duplicate observations
duplicates_to_add = pd.DataFrame(df.iloc[0:10])
duplicates_to_add[u'name'] = duplicates_to_add[u'name'] + " copy"

df = df.append(duplicates_to_add, ignore_index=True)
70/3:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
71/1:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
71/2:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
71/3:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
72/1:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
72/2:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
72/3:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
#from pandas_profiling.utils.cache import cache_file
72/4:
file_name = cache_file(
    "meteorites.csv",
    "https://data.nasa.gov/api/views/gh4g-9sfh/rows.csv?accessType=DOWNLOAD",
)
    
df = pd.read_csv(file_name)
    
# Note: Pandas does not support dates before 1880, so we ignore these for this analysis
df['year'] = pd.to_datetime(df['year'], errors='coerce')

# Example: Constant variable
df['source'] = "NASA"

# Example: Boolean variable
df['boolean'] = np.random.choice([True, False], df.shape[0])

# Example: Mixed with base types
df['mixed'] = np.random.choice([1, "A"], df.shape[0])

# Example: Highly correlated variables
df['reclat_city'] = df['reclat'] + np.random.normal(scale=5,size=(len(df)))

# Example: Duplicate observations
duplicates_to_add = pd.DataFrame(df.iloc[0:10])
duplicates_to_add[u'name'] = duplicates_to_add[u'name'] + " copy"

df = df.append(duplicates_to_add, ignore_index=True)
72/5:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
72/6:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

#import pandas_profiling!pip 
!pip install pandas-profiling[notebook,html]
from pandas_profiling.utils.cache import cache_file
72/7:
file_name = cache_file(
    "meteorites.csv",
    "https://data.nasa.gov/api/views/gh4g-9sfh/rows.csv?accessType=DOWNLOAD",
)
    
df = pd.read_csv(file_name)
    
# Note: Pandas does not support dates before 1880, so we ignore these for this analysis
df['year'] = pd.to_datetime(df['year'], errors='coerce')

# Example: Constant variable
df['source'] = "NASA"

# Example: Boolean variable
df['boolean'] = np.random.choice([True, False], df.shape[0])

# Example: Mixed with base types
df['mixed'] = np.random.choice([1, "A"], df.shape[0])

# Example: Highly correlated variables
df['reclat_city'] = df['reclat'] + np.random.normal(scale=5,size=(len(df)))

# Example: Duplicate observations
duplicates_to_add = pd.DataFrame(df.iloc[0:10])
duplicates_to_add[u'name'] = duplicates_to_add[u'name'] + " copy"

df = df.append(duplicates_to_add, ignore_index=True)
72/8:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
73/1:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
73/2: pandas_profiling.__version__
73/3: print(pandas_profiling.__version__)
73/4:
import pandas_profiling
print(pandas_profiling.__version__)
73/5:
import pandas_profiling
print(pandas_profiling.__version__)
73/6:
import pandas_profiling
print(pandas_profiling.__version__)
73/7: import pandas_profiling
73/8: import pandas_profiling
73/9: import pandas_profiling
73/10: print(pandas_profiling.__version__)
73/11: print(pandas_profiling.__version__)
74/1:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
74/2: print(pandas_profiling.__version__)
74/3:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
74/4:
import pandas_profiling
dir(pandas_profiling)
74/5:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
74/6:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
77/1:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
77/2:
import sys
!{sys.executable} -m pip install -U pandas-profiling
77/3:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
77/4:
import pandas_profiling
dir(pandas_profiling)
77/5: print(pandas_profiling.__version__)
77/6:
file_name = cache_file(
    "meteorites.csv",
    "https://data.nasa.gov/api/views/gh4g-9sfh/rows.csv?accessType=DOWNLOAD",
)
    
df = pd.read_csv(file_name)
    
# Note: Pandas does not support dates before 1880, so we ignore these for this analysis
df['year'] = pd.to_datetime(df['year'], errors='coerce')

# Example: Constant variable
df['source'] = "NASA"

# Example: Boolean variable
df['boolean'] = np.random.choice([True, False], df.shape[0])

# Example: Mixed with base types
df['mixed'] = np.random.choice([1, "A"], df.shape[0])

# Example: Highly correlated variables
df['reclat_city'] = df['reclat'] + np.random.normal(scale=5,size=(len(df)))

# Example: Duplicate observations
duplicates_to_add = pd.DataFrame(df.iloc[0:10])
duplicates_to_add[u'name'] = duplicates_to_add[u'name'] + " copy"

df = df.append(duplicates_to_add, ignore_index=True)
77/7:
file_name = cache_file(
    "meteorites.csv",
    "https://data.nasa.gov/api/views/gh4g-9sfh/rows.csv?accessType=DOWNLOAD",
)
    
df = pd.read_csv(file_name)
    
# Note: Pandas does not support dates before 1880, so we ignore these for this analysis
df['year'] = pd.to_datetime(df['year'], errors='coerce')

# Example: Constant variable
df['source'] = "NASA"

# Example: Boolean variable
df['boolean'] = np.random.choice([True, False], df.shape[0])

# Example: Mixed with base types
df['mixed'] = np.random.choice([1, "A"], df.shape[0])

# Example: Highly correlated variables
df['reclat_city'] = df['reclat'] + np.random.normal(scale=5,size=(len(df)))

# Example: Duplicate observations
duplicates_to_add = pd.DataFrame(df.iloc[0:10])
duplicates_to_add[u'name'] = duplicates_to_add[u'name'] + " copy"

df = df.append(duplicates_to_add, ignore_index=True)
77/8:
report = df.profile_report(sort='None', html={'style':{'full_width': True}}, progress_bar=False)
report
77/9:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
77/10:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
78/1:
from pathlib import Path

import requests
import numpy as np
import pandas as pd

import pandas_profiling
from pandas_profiling.utils.cache import cache_file
78/2:
file_name = cache_file(
    "meteorites.csv",
    "https://data.nasa.gov/api/views/gh4g-9sfh/rows.csv?accessType=DOWNLOAD",
)
    
df = pd.read_csv(file_name)
    
# Note: Pandas does not support dates before 1880, so we ignore these for this analysis
df['year'] = pd.to_datetime(df['year'], errors='coerce')

# Example: Constant variable
df['source'] = "NASA"

# Example: Boolean variable
df['boolean'] = np.random.choice([True, False], df.shape[0])

# Example: Mixed with base types
df['mixed'] = np.random.choice([1, "A"], df.shape[0])

# Example: Highly correlated variables
df['reclat_city'] = df['reclat'] + np.random.normal(scale=5,size=(len(df)))

# Example: Duplicate observations
duplicates_to_add = pd.DataFrame(df.iloc[0:10])
duplicates_to_add[u'name'] = duplicates_to_add[u'name'] + " copy"

df = df.append(duplicates_to_add, ignore_index=True)
78/3:
report = df.profile_report(sort='None', html={'style':{'full_width': True}}, progress_bar=False)
report
78/4:
profile_report = df.profile_report(html={'style': {'full_width': True}})
profile_report.to_file("/tmp/example.html")
78/5:
profile_report = df.profile_report(html={'style': {'full_width': True}})
profile_report.to_file("/tmp/example.html")
78/6:
profile_report = df.profile_report(explorative=True, html={'style': {'full_width': True}})
profile_report
78/7: profile_report.to_widgets()
79/1:
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/PDSH-cover-small.png\">\n",
    "\n",
    "*This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*\n",
    "\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [Pivot Tables](03.09-Pivot-Tables.ipynb) | [Contents](Index.ipynb) | [Working with Time Series](03.11-Working-with-Time-Series.ipynb) >\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.10-Working-With-Strings.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized String Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strength of Python is its relative ease in handling and manipulating string data.\n",
    "Pandas builds on this and provides a comprehensive set of *vectorized string operations* that become an essential piece of the type of munging required when working with (read: cleaning up) real-world data.\n",
    "In this section, we'll walk through some of the Pandas string operations, and then take a look at using them to partially clean up a very messy dataset of recipes collected from the Internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Pandas String Operations\n",
    "\n",
    "We saw in previous sections how tools like NumPy and Pandas generalize arithmetic operations so that we can easily and quickly perform the same operation on many array elements. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  6, 10, 14, 22, 26])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([2, 3, 5, 7, 11, 13])\n",
    "x * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *vectorization* of operations simplifies the syntax of operating on arrays of data: we no longer have to worry about the size or shape of the array, but just about what operation we want done.\n",
    "For arrays of strings, NumPy does not provide such simple access, and thus you're stuck using a more verbose loop syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Peter', 'Paul', 'Mary', 'Guido']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['peter', 'Paul', 'MARY', 'gUIDO']\n",
    "[s.capitalize() for s in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is perhaps sufficient to work with some data, but it will break if there are any missing values.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'capitalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fc1d891ab539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'peter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Paul'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MARY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gUIDO'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-fc1d891ab539>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'peter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Paul'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MARY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gUIDO'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'capitalize'"
     ]
    }
   ],
   "source": [
    "data = ['peter', 'Paul', None, 'MARY', 'gUIDO']\n",
    "[s.capitalize() for s in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas includes features to address both this need for vectorized string operations and for correctly handling missing data via the ``str`` attribute of Pandas Series and Index objects containing strings.\n",
    "So, for example, suppose we create a Pandas Series with this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    peter\n",
       "1     Paul\n",
       "2     None\n",
       "3     MARY\n",
       "4    gUIDO\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "names = pd.Series(data)\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call a single method that will capitalize all the entries, while skipping over any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Peter\n",
       "1     Paul\n",
       "2     None\n",
       "3     Mary\n",
       "4    Guido\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.str.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tab completion on this ``str`` attribute will list all the vectorized string methods available to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables of Pandas String Methods\n",
    "\n",
    "If you have a good understanding of string manipulation in Python, most of Pandas string syntax is intuitive enough that it's probably sufficient to just list a table of available methods; we will start with that here, before diving deeper into a few of the subtleties.\n",
    "The examples in this section use the following series of names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',\n",
    "                   'Eric Idle', 'Terry Jones', 'Michael Palin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods similar to Python string methods\n",
    "Nearly all Python's built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas ``str`` methods that mirror Python string methods:\n",
    "\n",
    "|             |                  |                  |                  |\n",
    "|-------------|------------------|------------------|------------------|\n",
    "|``len()``    | ``lower()``      | ``translate()``  | ``islower()``    | \n",
    "|``ljust()``  | ``upper()``      | ``startswith()`` | ``isupper()``    | \n",
    "|``rjust()``  | ``find()``       | ``endswith()``   | ``isnumeric()``  | \n",
    "|``center()`` | ``rfind()``      | ``isalnum()``    | ``isdecimal()``  | \n",
    "|``zfill()``  | ``index()``      | ``isalpha()``    | ``split()``      | \n",
    "|``strip()``  | ``rindex()``     | ``isdigit()``    | ``rsplit()``     | \n",
    "|``rstrip()`` | ``capitalize()`` | ``isspace()``    | ``partition()``  | \n",
    "|``lstrip()`` |  ``swapcase()``  |  ``istitle()``   | ``rpartition()`` |\n",
    "\n",
    "Notice that these have various return values. Some, like ``lower()``, return a series of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    graham chapman\n",
       "1       john cleese\n",
       "2     terry gilliam\n",
       "3         eric idle\n",
       "4       terry jones\n",
       "5     michael palin\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But some others return numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14\n",
       "1    11\n",
       "2    13\n",
       "3     9\n",
       "4    11\n",
       "5    13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or Boolean values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2     True\n",
       "3    False\n",
       "4     True\n",
       "5    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.startswith('T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still others return lists or other compound values for each element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Graham, Chapman]\n",
       "1       [John, Cleese]\n",
       "2     [Terry, Gilliam]\n",
       "3         [Eric, Idle]\n",
       "4       [Terry, Jones]\n",
       "5     [Michael, Palin]\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see further manipulations of this kind of series-of-lists object as we continue our discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods using regular expressions\n",
    "\n",
    "In addition, there are several methods that accept regular expressions to examine the content of each string element, and follow some of the API conventions of Python's built-in ``re`` module:\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| ``match()`` | Call ``re.match()`` on each element, returning a boolean. |\n",
    "| ``extract()`` | Call ``re.match()`` on each element, returning matched groups as strings.|\n",
    "| ``findall()`` | Call ``re.findall()`` on each element |\n",
    "| ``replace()`` | Replace occurrences of pattern with some other string|\n",
    "| ``contains()`` | Call ``re.search()`` on each element, returning a boolean |\n",
    "| ``count()`` | Count occurrences of pattern|\n",
    "| ``split()``   | Equivalent to ``str.split()``, but accepts regexps |\n",
    "| ``rsplit()`` | Equivalent to ``str.rsplit()``, but accepts regexps |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these, you can do a wide range of interesting operations.\n",
    "For example, we can extract the first name from each by asking for a contiguous group of characters at the beginning of each element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Graham\n",
       "1       John\n",
       "2      Terry\n",
       "3       Eric\n",
       "4      Terry\n",
       "5    Michael\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.extract('([A-Za-z]+)', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can do something more complicated, like finding all names that start and end with a consonant, making use of the start-of-string (``^``) and end-of-string (``$``) regular expression characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Graham Chapman]\n",
       "1                  []\n",
       "2     [Terry Gilliam]\n",
       "3                  []\n",
       "4       [Terry Jones]\n",
       "5     [Michael Palin]\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.findall(r'^[^AEIOU].*[^aeiou]$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to concisely apply regular expressions across ``Series`` or ``Dataframe`` entries opens up many possibilities for analysis and cleaning of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous methods\n",
    "Finally, there are some miscellaneous methods that enable other convenient operations:\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| ``get()`` | Index each element |\n",
    "| ``slice()`` | Slice each element|\n",
    "| ``slice_replace()`` | Replace slice in each element with passed value|\n",
    "| ``cat()``      | Concatenate strings|\n",
    "| ``repeat()`` | Repeat values |\n",
    "| ``normalize()`` | Return Unicode form of string |\n",
    "| ``pad()`` | Add whitespace to left, right, or both sides of strings|\n",
    "| ``wrap()`` | Split long strings into lines with length less than a given width|\n",
    "| ``join()`` | Join strings in each element of the Series with passed separator|\n",
    "| ``get_dummies()`` | extract dummy variables as a dataframe |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized item access and slicing\n",
    "\n",
    "The ``get()`` and ``slice()`` operations, in particular, enable vectorized element access from each array.\n",
    "For example, we can get a slice of the first three characters of each array using ``str.slice(0, 3)``.\n",
    "Note that this behavior is also available through Python's normal indexing syntax–for example, ``df.str.slice(0, 3)`` is equivalent to ``df.str[0:3]``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Gra\n",
       "1    Joh\n",
       "2    Ter\n",
       "3    Eri\n",
       "4    Ter\n",
       "5    Mic\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing via ``df.str.get(i)`` and ``df.str[i]`` is likewise similar.\n",
    "\n",
    "These ``get()`` and ``slice()`` methods also let you access elements of arrays returned by ``split()``.\n",
    "For example, to extract the last name of each entry, we can combine ``split()`` and ``get()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Chapman\n",
       "1     Cleese\n",
       "2    Gilliam\n",
       "3       Idle\n",
       "4      Jones\n",
       "5      Palin\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.split().str.get(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicator variables\n",
    "\n",
    "Another method that requires a bit of extra explanation is the ``get_dummies()`` method.\n",
    "This is useful when your data has a column containing some sort of coded indicator.\n",
    "For example, we might have a dataset that contains information in the form of codes, such as A=\"born in America,\" B=\"born in the United Kingdom,\" C=\"likes cheese,\" D=\"likes spam\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>info</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B|C|D</td>\n",
       "      <td>Graham Chapman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B|D</td>\n",
       "      <td>John Cleese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A|C</td>\n",
       "      <td>Terry Gilliam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B|D</td>\n",
       "      <td>Eric Idle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B|C</td>\n",
       "      <td>Terry Jones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B|C|D</td>\n",
       "      <td>Michael Palin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    info            name\n",
       "0  B|C|D  Graham Chapman\n",
       "1    B|D     John Cleese\n",
       "2    A|C   Terry Gilliam\n",
       "3    B|D       Eric Idle\n",
       "4    B|C     Terry Jones\n",
       "5  B|C|D   Michael Palin"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_monte = pd.DataFrame({'name': monte,\n",
    "                           'info': ['B|C|D', 'B|D', 'A|C',\n",
    "                                    'B|D', 'B|C', 'B|C|D']})\n",
    "full_monte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``get_dummies()`` routine lets you quickly split-out these indicator variables into a ``DataFrame``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C  D\n",
       "0  0  1  1  1\n",
       "1  0  1  0  1\n",
       "2  1  0  1  0\n",
       "3  0  1  0  1\n",
       "4  0  1  1  0\n",
       "5  0  1  1  1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_monte['info'].str.get_dummies('|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these operations as building blocks, you can construct an endless range of string processing procedures when cleaning your data.\n",
    "\n",
    "We won't dive further into these methods here, but I encourage you to read through [\"Working with Text Data\"](http://pandas.pydata.org/pandas-docs/stable/text.html) in the Pandas online documentation, or to refer to the resources listed in [Further Resources](03.13-Further-Resources.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Recipe Database\n",
    "\n",
    "These vectorized string operations become most useful in the process of cleaning up messy, real-world data.\n",
    "Here I'll walk through an example of that, using an open recipe database compiled from various sources on the Web.\n",
    "Our goal will be to parse the recipe data into ingredient lists, so we can quickly find a recipe based on some ingredients we have on hand.\n",
    "\n",
    "The scripts used to compile this can be found at https://github.com/fictivekin/openrecipes, and the link to the current version of the database is found there as well.\n",
    "\n",
    "As of Spring 2016, this database is about 30 MB, and can be downloaded and unzipped with these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !curl -O http://openrecipes.s3.amazonaws.com/recipeitems-latest.json.gz\n",
    "# !gunzip recipeitems-latest.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database is in JSON format, so we will try ``pd.read_json`` to read it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: Trailing data\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    recipes = pd.read_json('recipeitems-latest.json')\n",
    "except ValueError as e:\n",
    "    print(\"ValueError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! We get a ``ValueError`` mentioning that there is \"trailing data.\"\n",
    "Searching for the text of this error on the Internet, it seems that it's due to using a file in which *each line* is itself a valid JSON, but the full file is not.\n",
    "Let's check if this interpretation is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('recipeitems-latest.json') as f:\n",
    "    line = f.readline()\n",
    "pd.read_json(line).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, apparently each line is a valid JSON, so we'll need to string them together.\n",
    "One way we can do this is to actually construct a string representation containing all these JSON entries, and then load the whole thing with ``pd.read_json``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the entire file into a Python array\n",
    "with open('recipeitems-latest.json', 'r') as f:\n",
    "    # Extract each line\n",
    "    data = (line.strip() for line in f)\n",
    "    # Reformat so each line is the element of a list\n",
    "    data_json = \"[{0}]\".format(','.join(data))\n",
    "# read the result as a JSON\n",
    "recipes = pd.read_json(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173278, 17)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are nearly 200,000 recipes, and 17 columns.\n",
    "Let's take a look at one row to see what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_id                                {'$oid': '5160756b96cc62079cc2db15'}\n",
       "cookTime                                                          PT30M\n",
       "creator                                                             NaN\n",
       "dateModified                                                        NaN\n",
       "datePublished                                                2013-03-11\n",
       "description           Late Saturday afternoon, after Marlboro Man ha...\n",
       "image                 http://static.thepioneerwoman.com/cooking/file...\n",
       "ingredients           Biscuits\\n3 cups All-purpose Flour\\n2 Tablespo...\n",
       "name                                    Drop Biscuits and Sausage Gravy\n",
       "prepTime                                                          PT10M\n",
       "recipeCategory                                                      NaN\n",
       "recipeInstructions                                                  NaN\n",
       "recipeYield                                                          12\n",
       "source                                                  thepioneerwoman\n",
       "totalTime                                                           NaN\n",
       "ts                                             {'$date': 1365276011104}\n",
       "url                   http://thepioneerwoman.com/cooking/2013/03/dro...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information there, but much of it is in a very messy form, as is typical of data scraped from the Web.\n",
    "In particular, the ingredient list is in string format; we're going to have to carefully extract the information we're interested in.\n",
    "Let's start by taking a closer look at the ingredients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    173278.000000\n",
       "mean        244.617926\n",
       "std         146.705285\n",
       "min           0.000000\n",
       "25%         147.000000\n",
       "50%         221.000000\n",
       "75%         314.000000\n",
       "max        9067.000000\n",
       "Name: ingredients, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.ingredients.str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingredient lists average 250 characters long, with a minimum of 0 and a maximum of nearly 10,000 characters!\n",
    "\n",
    "Just out of curiousity, let's see which recipe has the longest ingredient list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carrot Pineapple Spice &amp; Brownie Layer Cake with Whipped Cream &amp; Cream Cheese Frosting and Marzipan Carrots'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.name[np.argmax(recipes.ingredients.str.len())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That certainly looks like an involved recipe.\n",
    "\n",
    "We can do other aggregate explorations; for example, let's see how many of the recipes are for breakfast food:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3524"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.description.str.contains('[Bb]reakfast').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or how many of the recipes list cinnamon as an ingredient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10526"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.ingredients.str.contains('[Cc]innamon').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could even look to see whether any recipes misspell the ingredient as \"cinamon\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.ingredients.str.contains('[Cc]inamon').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the type of essential data exploration that is possible with Pandas string tools.\n",
    "It is data munging like this that Python really excels at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple recipe recommender\n",
    "\n",
    "Let's go a bit further, and start working on a simple recipe recommendation system: given a list of ingredients, find a recipe that uses all those ingredients.\n",
    "While conceptually straightforward, the task is complicated by the heterogeneity of the data: there is no easy operation, for example, to extract a clean list of ingredients from each row.\n",
    "So we will cheat a bit: we'll start with a list of common ingredients, and simply search to see whether they are in each recipe's ingredient list.\n",
    "For simplicity, let's just stick with herbs and spices for the time being:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spice_list = ['salt', 'pepper', 'oregano', 'sage', 'parsley',\n",
    "              'rosemary', 'tarragon', 'thyme', 'paprika', 'cumin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then build a Boolean ``DataFrame`` consisting of True and False values, indicating whether this ingredient appears in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cumin</th>\n",
       "      <th>oregano</th>\n",
       "      <th>paprika</th>\n",
       "      <th>parsley</th>\n",
       "      <th>pepper</th>\n",
       "      <th>rosemary</th>\n",
       "      <th>sage</th>\n",
       "      <th>salt</th>\n",
       "      <th>tarragon</th>\n",
       "      <th>thyme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cumin oregano paprika parsley pepper rosemary   sage   salt tarragon  thyme\n",
       "0  False   False   False   False  False    False   True  False    False  False\n",
       "1  False   False   False   False  False    False  False  False    False  False\n",
       "2   True   False   False   False   True    False  False   True    False  False\n",
       "3  False   False   False   False  False    False  False  False    False  False\n",
       "4  False   False   False   False  False    False  False  False    False  False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "spice_df = pd.DataFrame(dict((spice, recipes.ingredients.str.contains(spice, re.IGNORECASE))\n",
    "                             for spice in spice_list))\n",
    "spice_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as an example, let's say we'd like to find a recipe that uses parsley, paprika, and tarragon.\n",
    "We can compute this very quickly using the ``query()`` method of ``DataFrame``s, discussed in [High-Performance Pandas: ``eval()`` and ``query()``](03.12-Performance-Eval-and-Query.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection = spice_df.query('parsley & paprika & tarragon')\n",
    "len(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find only 10 recipes with this combination; let's use the index returned by this selection to discover the names of the recipes that have this combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2069      All cremat with a Little Gem, dandelion and wa...\n",
       "74964                         Lobster with Thermidor butter\n",
       "93768      Burton's Southern Fried Chicken with White Gravy\n",
       "113926                     Mijo's Slow Cooker Shredded Beef\n",
       "137686                     Asparagus Soup with Poached Eggs\n",
       "140530                                 Fried Oyster Po’boys\n",
       "158475                Lamb shank tagine with herb tabbouleh\n",
       "158486                 Southern fried chicken in buttermilk\n",
       "163175            Fried Chicken Sliders with Pickles + Slaw\n",
       "165243                        Bar Tartine Cauliflower Salad\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.name[selection.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have narrowed down our recipe selection by a factor of almost 20,000, we are in a position to make a more informed decision about what we'd like to cook for dinner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further with recipes\n",
    "\n",
    "Hopefully this example has given you a bit of a flavor (ba-dum!) for the types of data cleaning operations that are efficiently enabled by Pandas string methods.\n",
    "Of course, building a very robust recipe recommendation system would require a *lot* more work!\n",
    "Extracting full ingredient lists from each recipe would be an important piece of the task; unfortunately, the wide variety of formats used makes this a relatively time-consuming process.\n",
    "This points to the truism that in data science, cleaning and munging of real-world data often comprises the majority of the work, and Pandas provides the tools that can help you do this efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [Pivot Tables](03.09-Pivot-Tables.ipynb) | [Contents](Index.ipynb) | [Working with Time Series](03.11-Working-with-Time-Series.ipynb) >\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.10-Working-With-Strings.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
79/2:
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"padding-right:10px;\" src=\"figures/PDSH-cover-small.png\">\n",
    "\n",
    "*This notebook contains an excerpt from the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*\n",
    "\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [Pivot Tables](03.09-Pivot-Tables.ipynb) | [Contents](Index.ipynb) | [Working with Time Series](03.11-Working-with-Time-Series.ipynb) >\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.10-Working-With-Strings.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized String Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strength of Python is its relative ease in handling and manipulating string data.\n",
    "Pandas builds on this and provides a comprehensive set of *vectorized string operations* that become an essential piece of the type of munging required when working with (read: cleaning up) real-world data.\n",
    "In this section, we'll walk through some of the Pandas string operations, and then take a look at using them to partially clean up a very messy dataset of recipes collected from the Internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Pandas String Operations\n",
    "\n",
    "We saw in previous sections how tools like NumPy and Pandas generalize arithmetic operations so that we can easily and quickly perform the same operation on many array elements. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  6, 10, 14, 22, 26])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([2, 3, 5, 7, 11, 13])\n",
    "x * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *vectorization* of operations simplifies the syntax of operating on arrays of data: we no longer have to worry about the size or shape of the array, but just about what operation we want done.\n",
    "For arrays of strings, NumPy does not provide such simple access, and thus you're stuck using a more verbose loop syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Peter', 'Paul', 'Mary', 'Guido']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['peter', 'Paul', 'MARY', 'gUIDO']\n",
    "[s.capitalize() for s in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is perhaps sufficient to work with some data, but it will break if there are any missing values.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'capitalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fc1d891ab539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'peter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Paul'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MARY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gUIDO'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-fc1d891ab539>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'peter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Paul'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MARY'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gUIDO'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'capitalize'"
     ]
    }
   ],
   "source": [
    "data = ['peter', 'Paul', None, 'MARY', 'gUIDO']\n",
    "[s.capitalize() for s in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas includes features to address both this need for vectorized string operations and for correctly handling missing data via the ``str`` attribute of Pandas Series and Index objects containing strings.\n",
    "So, for example, suppose we create a Pandas Series with this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    peter\n",
       "1     Paul\n",
       "2     None\n",
       "3     MARY\n",
       "4    gUIDO\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "names = pd.Series(data)\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call a single method that will capitalize all the entries, while skipping over any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Peter\n",
       "1     Paul\n",
       "2     None\n",
       "3     Mary\n",
       "4    Guido\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.str.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tab completion on this ``str`` attribute will list all the vectorized string methods available to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables of Pandas String Methods\n",
    "\n",
    "If you have a good understanding of string manipulation in Python, most of Pandas string syntax is intuitive enough that it's probably sufficient to just list a table of available methods; we will start with that here, before diving deeper into a few of the subtleties.\n",
    "The examples in this section use the following series of names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',\n",
    "                   'Eric Idle', 'Terry Jones', 'Michael Palin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods similar to Python string methods\n",
    "Nearly all Python's built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas ``str`` methods that mirror Python string methods:\n",
    "\n",
    "|             |                  |                  |                  |\n",
    "|-------------|------------------|------------------|------------------|\n",
    "|``len()``    | ``lower()``      | ``translate()``  | ``islower()``    | \n",
    "|``ljust()``  | ``upper()``      | ``startswith()`` | ``isupper()``    | \n",
    "|``rjust()``  | ``find()``       | ``endswith()``   | ``isnumeric()``  | \n",
    "|``center()`` | ``rfind()``      | ``isalnum()``    | ``isdecimal()``  | \n",
    "|``zfill()``  | ``index()``      | ``isalpha()``    | ``split()``      | \n",
    "|``strip()``  | ``rindex()``     | ``isdigit()``    | ``rsplit()``     | \n",
    "|``rstrip()`` | ``capitalize()`` | ``isspace()``    | ``partition()``  | \n",
    "|``lstrip()`` |  ``swapcase()``  |  ``istitle()``   | ``rpartition()`` |\n",
    "\n",
    "Notice that these have various return values. Some, like ``lower()``, return a series of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    graham chapman\n",
       "1       john cleese\n",
       "2     terry gilliam\n",
       "3         eric idle\n",
       "4       terry jones\n",
       "5     michael palin\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But some others return numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14\n",
       "1    11\n",
       "2    13\n",
       "3     9\n",
       "4    11\n",
       "5    13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or Boolean values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2     True\n",
       "3    False\n",
       "4     True\n",
       "5    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.startswith('T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still others return lists or other compound values for each element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Graham, Chapman]\n",
       "1       [John, Cleese]\n",
       "2     [Terry, Gilliam]\n",
       "3         [Eric, Idle]\n",
       "4       [Terry, Jones]\n",
       "5     [Michael, Palin]\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see further manipulations of this kind of series-of-lists object as we continue our discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods using regular expressions\n",
    "\n",
    "In addition, there are several methods that accept regular expressions to examine the content of each string element, and follow some of the API conventions of Python's built-in ``re`` module:\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| ``match()`` | Call ``re.match()`` on each element, returning a boolean. |\n",
    "| ``extract()`` | Call ``re.match()`` on each element, returning matched groups as strings.|\n",
    "| ``findall()`` | Call ``re.findall()`` on each element |\n",
    "| ``replace()`` | Replace occurrences of pattern with some other string|\n",
    "| ``contains()`` | Call ``re.search()`` on each element, returning a boolean |\n",
    "| ``count()`` | Count occurrences of pattern|\n",
    "| ``split()``   | Equivalent to ``str.split()``, but accepts regexps |\n",
    "| ``rsplit()`` | Equivalent to ``str.rsplit()``, but accepts regexps |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these, you can do a wide range of interesting operations.\n",
    "For example, we can extract the first name from each by asking for a contiguous group of characters at the beginning of each element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Graham\n",
       "1       John\n",
       "2      Terry\n",
       "3       Eric\n",
       "4      Terry\n",
       "5    Michael\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.extract('([A-Za-z]+)', expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can do something more complicated, like finding all names that start and end with a consonant, making use of the start-of-string (``^``) and end-of-string (``$``) regular expression characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Graham Chapman]\n",
       "1                  []\n",
       "2     [Terry Gilliam]\n",
       "3                  []\n",
       "4       [Terry Jones]\n",
       "5     [Michael Palin]\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.findall(r'^[^AEIOU].*[^aeiou]$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to concisely apply regular expressions across ``Series`` or ``Dataframe`` entries opens up many possibilities for analysis and cleaning of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous methods\n",
    "Finally, there are some miscellaneous methods that enable other convenient operations:\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| ``get()`` | Index each element |\n",
    "| ``slice()`` | Slice each element|\n",
    "| ``slice_replace()`` | Replace slice in each element with passed value|\n",
    "| ``cat()``      | Concatenate strings|\n",
    "| ``repeat()`` | Repeat values |\n",
    "| ``normalize()`` | Return Unicode form of string |\n",
    "| ``pad()`` | Add whitespace to left, right, or both sides of strings|\n",
    "| ``wrap()`` | Split long strings into lines with length less than a given width|\n",
    "| ``join()`` | Join strings in each element of the Series with passed separator|\n",
    "| ``get_dummies()`` | extract dummy variables as a dataframe |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorized item access and slicing\n",
    "\n",
    "The ``get()`` and ``slice()`` operations, in particular, enable vectorized element access from each array.\n",
    "For example, we can get a slice of the first three characters of each array using ``str.slice(0, 3)``.\n",
    "Note that this behavior is also available through Python's normal indexing syntax–for example, ``df.str.slice(0, 3)`` is equivalent to ``df.str[0:3]``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Gra\n",
       "1    Joh\n",
       "2    Ter\n",
       "3    Eri\n",
       "4    Ter\n",
       "5    Mic\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing via ``df.str.get(i)`` and ``df.str[i]`` is likewise similar.\n",
    "\n",
    "These ``get()`` and ``slice()`` methods also let you access elements of arrays returned by ``split()``.\n",
    "For example, to extract the last name of each entry, we can combine ``split()`` and ``get()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Chapman\n",
       "1     Cleese\n",
       "2    Gilliam\n",
       "3       Idle\n",
       "4      Jones\n",
       "5      Palin\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monte.str.split().str.get(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicator variables\n",
    "\n",
    "Another method that requires a bit of extra explanation is the ``get_dummies()`` method.\n",
    "This is useful when your data has a column containing some sort of coded indicator.\n",
    "For example, we might have a dataset that contains information in the form of codes, such as A=\"born in America,\" B=\"born in the United Kingdom,\" C=\"likes cheese,\" D=\"likes spam\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>info</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B|C|D</td>\n",
       "      <td>Graham Chapman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B|D</td>\n",
       "      <td>John Cleese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A|C</td>\n",
       "      <td>Terry Gilliam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B|D</td>\n",
       "      <td>Eric Idle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B|C</td>\n",
       "      <td>Terry Jones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B|C|D</td>\n",
       "      <td>Michael Palin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    info            name\n",
       "0  B|C|D  Graham Chapman\n",
       "1    B|D     John Cleese\n",
       "2    A|C   Terry Gilliam\n",
       "3    B|D       Eric Idle\n",
       "4    B|C     Terry Jones\n",
       "5  B|C|D   Michael Palin"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_monte = pd.DataFrame({'name': monte,\n",
    "                           'info': ['B|C|D', 'B|D', 'A|C',\n",
    "                                    'B|D', 'B|C', 'B|C|D']})\n",
    "full_monte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``get_dummies()`` routine lets you quickly split-out these indicator variables into a ``DataFrame``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C  D\n",
       "0  0  1  1  1\n",
       "1  0  1  0  1\n",
       "2  1  0  1  0\n",
       "3  0  1  0  1\n",
       "4  0  1  1  0\n",
       "5  0  1  1  1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_monte['info'].str.get_dummies('|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these operations as building blocks, you can construct an endless range of string processing procedures when cleaning your data.\n",
    "\n",
    "We won't dive further into these methods here, but I encourage you to read through [\"Working with Text Data\"](http://pandas.pydata.org/pandas-docs/stable/text.html) in the Pandas online documentation, or to refer to the resources listed in [Further Resources](03.13-Further-Resources.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Recipe Database\n",
    "\n",
    "These vectorized string operations become most useful in the process of cleaning up messy, real-world data.\n",
    "Here I'll walk through an example of that, using an open recipe database compiled from various sources on the Web.\n",
    "Our goal will be to parse the recipe data into ingredient lists, so we can quickly find a recipe based on some ingredients we have on hand.\n",
    "\n",
    "The scripts used to compile this can be found at https://github.com/fictivekin/openrecipes, and the link to the current version of the database is found there as well.\n",
    "\n",
    "As of Spring 2016, this database is about 30 MB, and can be downloaded and unzipped with these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !curl -O http://openrecipes.s3.amazonaws.com/recipeitems-latest.json.gz\n",
    "# !gunzip recipeitems-latest.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database is in JSON format, so we will try ``pd.read_json`` to read it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: Trailing data\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    recipes = pd.read_json('recipeitems-latest.json')\n",
    "except ValueError as e:\n",
    "    print(\"ValueError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! We get a ``ValueError`` mentioning that there is \"trailing data.\"\n",
    "Searching for the text of this error on the Internet, it seems that it's due to using a file in which *each line* is itself a valid JSON, but the full file is not.\n",
    "Let's check if this interpretation is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('recipeitems-latest.json') as f:\n",
    "    line = f.readline()\n",
    "pd.read_json(line).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, apparently each line is a valid JSON, so we'll need to string them together.\n",
    "One way we can do this is to actually construct a string representation containing all these JSON entries, and then load the whole thing with ``pd.read_json``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the entire file into a Python array\n",
    "with open('recipeitems-latest.json', 'r') as f:\n",
    "    # Extract each line\n",
    "    data = (line.strip() for line in f)\n",
    "    # Reformat so each line is the element of a list\n",
    "    data_json = \"[{0}]\".format(','.join(data))\n",
    "# read the result as a JSON\n",
    "recipes = pd.read_json(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173278, 17)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are nearly 200,000 recipes, and 17 columns.\n",
    "Let's take a look at one row to see what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_id                                {'$oid': '5160756b96cc62079cc2db15'}\n",
       "cookTime                                                          PT30M\n",
       "creator                                                             NaN\n",
       "dateModified                                                        NaN\n",
       "datePublished                                                2013-03-11\n",
       "description           Late Saturday afternoon, after Marlboro Man ha...\n",
       "image                 http://static.thepioneerwoman.com/cooking/file...\n",
       "ingredients           Biscuits\\n3 cups All-purpose Flour\\n2 Tablespo...\n",
       "name                                    Drop Biscuits and Sausage Gravy\n",
       "prepTime                                                          PT10M\n",
       "recipeCategory                                                      NaN\n",
       "recipeInstructions                                                  NaN\n",
       "recipeYield                                                          12\n",
       "source                                                  thepioneerwoman\n",
       "totalTime                                                           NaN\n",
       "ts                                             {'$date': 1365276011104}\n",
       "url                   http://thepioneerwoman.com/cooking/2013/03/dro...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information there, but much of it is in a very messy form, as is typical of data scraped from the Web.\n",
    "In particular, the ingredient list is in string format; we're going to have to carefully extract the information we're interested in.\n",
    "Let's start by taking a closer look at the ingredients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    173278.000000\n",
       "mean        244.617926\n",
       "std         146.705285\n",
       "min           0.000000\n",
       "25%         147.000000\n",
       "50%         221.000000\n",
       "75%         314.000000\n",
       "max        9067.000000\n",
       "Name: ingredients, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.ingredients.str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ingredient lists average 250 characters long, with a minimum of 0 and a maximum of nearly 10,000 characters!\n",
    "\n",
    "Just out of curiousity, let's see which recipe has the longest ingredient list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carrot Pineapple Spice &amp; Brownie Layer Cake with Whipped Cream &amp; Cream Cheese Frosting and Marzipan Carrots'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.name[np.argmax(recipes.ingredients.str.len())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That certainly looks like an involved recipe.\n",
    "\n",
    "We can do other aggregate explorations; for example, let's see how many of the recipes are for breakfast food:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3524"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.description.str.contains('[Bb]reakfast').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or how many of the recipes list cinnamon as an ingredient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10526"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.ingredients.str.contains('[Cc]innamon').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could even look to see whether any recipes misspell the ingredient as \"cinamon\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.ingredients.str.contains('[Cc]inamon').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the type of essential data exploration that is possible with Pandas string tools.\n",
    "It is data munging like this that Python really excels at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple recipe recommender\n",
    "\n",
    "Let's go a bit further, and start working on a simple recipe recommendation system: given a list of ingredients, find a recipe that uses all those ingredients.\n",
    "While conceptually straightforward, the task is complicated by the heterogeneity of the data: there is no easy operation, for example, to extract a clean list of ingredients from each row.\n",
    "So we will cheat a bit: we'll start with a list of common ingredients, and simply search to see whether they are in each recipe's ingredient list.\n",
    "For simplicity, let's just stick with herbs and spices for the time being:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spice_list = ['salt', 'pepper', 'oregano', 'sage', 'parsley',\n",
    "              'rosemary', 'tarragon', 'thyme', 'paprika', 'cumin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then build a Boolean ``DataFrame`` consisting of True and False values, indicating whether this ingredient appears in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cumin</th>\n",
       "      <th>oregano</th>\n",
       "      <th>paprika</th>\n",
       "      <th>parsley</th>\n",
       "      <th>pepper</th>\n",
       "      <th>rosemary</th>\n",
       "      <th>sage</th>\n",
       "      <th>salt</th>\n",
       "      <th>tarragon</th>\n",
       "      <th>thyme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cumin oregano paprika parsley pepper rosemary   sage   salt tarragon  thyme\n",
       "0  False   False   False   False  False    False   True  False    False  False\n",
       "1  False   False   False   False  False    False  False  False    False  False\n",
       "2   True   False   False   False   True    False  False   True    False  False\n",
       "3  False   False   False   False  False    False  False  False    False  False\n",
       "4  False   False   False   False  False    False  False  False    False  False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "spice_df = pd.DataFrame(dict((spice, recipes.ingredients.str.contains(spice, re.IGNORECASE))\n",
    "                             for spice in spice_list))\n",
    "spice_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as an example, let's say we'd like to find a recipe that uses parsley, paprika, and tarragon.\n",
    "We can compute this very quickly using the ``query()`` method of ``DataFrame``s, discussed in [High-Performance Pandas: ``eval()`` and ``query()``](03.12-Performance-Eval-and-Query.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection = spice_df.query('parsley & paprika & tarragon')\n",
    "len(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find only 10 recipes with this combination; let's use the index returned by this selection to discover the names of the recipes that have this combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2069      All cremat with a Little Gem, dandelion and wa...\n",
       "74964                         Lobster with Thermidor butter\n",
       "93768      Burton's Southern Fried Chicken with White Gravy\n",
       "113926                     Mijo's Slow Cooker Shredded Beef\n",
       "137686                     Asparagus Soup with Poached Eggs\n",
       "140530                                 Fried Oyster Po’boys\n",
       "158475                Lamb shank tagine with herb tabbouleh\n",
       "158486                 Southern fried chicken in buttermilk\n",
       "163175            Fried Chicken Sliders with Pickles + Slaw\n",
       "165243                        Bar Tartine Cauliflower Salad\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.name[selection.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have narrowed down our recipe selection by a factor of almost 20,000, we are in a position to make a more informed decision about what we'd like to cook for dinner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further with recipes\n",
    "\n",
    "Hopefully this example has given you a bit of a flavor (ba-dum!) for the types of data cleaning operations that are efficiently enabled by Pandas string methods.\n",
    "Of course, building a very robust recipe recommendation system would require a *lot* more work!\n",
    "Extracting full ingredient lists from each recipe would be an important piece of the task; unfortunately, the wide variety of formats used makes this a relatively time-consuming process.\n",
    "This points to the truism that in data science, cleaning and munging of real-world data often comprises the majority of the work, and Pandas provides the tools that can help you do this efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "< [Pivot Tables](03.09-Pivot-Tables.ipynb) | [Contents](Index.ipynb) | [Working with Time Series](03.11-Working-with-Time-Series.ipynb) >\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.10-Working-With-Strings.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
80/1:
import numpy as np
x = np.array([2, 3, 5, 7, 11, 13])
x * 2
80/2:
data = ['peter', 'Paul', 'MARY', 'gUIDO']
[s.capitalize() for s in data]
80/3:
data = ['peter', 'Paul', None, 'MARY', 'gUIDO']
[s.capitalize() for s in data]
80/4:
import pandas as pd
names = pd.Series(data)
names
80/5: names.str.capitalize()
81/1:
monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',
                   'Eric Idle', 'Terry Jones', 'Michael Palin'])
82/1:
import numpy as np
x = np.array([2, 3, 5, 7, 11, 13])
x * 2
82/2:
data = ['peter', 'Paul', 'MARY', 'gUIDO']
[s.capitalize() for s in data]
82/3:
data = ['peter', 'Paul', None, 'MARY', 'gUIDO']
[s.capitalize() for s in data]
82/4:
monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',
                   'Eric Idle', 'Terry Jones', 'Michael Palin'])
82/5:
import pandas as pd
monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',
                   'Eric Idle', 'Terry Jones', 'Michael Palin'])
82/6: monte.str.lower()
82/7: monte.str.len()
82/8: monte.str.startswith('T')
82/9: monte.str.split()
82/10: monte.str.extract('([A-Za-z]+)', expand=False)
82/11: monte.str.findall(r'^[^AEIOU].*[^aeiou]$')
82/12: monte.str[0:3]
82/13: monte.str.split().str.get(-1)
82/14:
full_monte = pd.DataFrame({'name': monte,
                           'info': ['B|C|D', 'B|D', 'A|C',
                                    'B|D', 'B|C', 'B|C|D']})
full_monte
82/15: full_monte['info'].str.get_dummies('|')
82/16:
try:
    recipes = pd.read_json('recipeitems-latest.json')
except ValueError as e:
    print("ValueError:", e)
82/17:
!curl -O http://openrecipes.s3.amazonaws.com/recipeitems-latest.json.gz
!gunzip recipeitems-latest.json.gz
82/18:
try:
    recipes = pd.read_json('recipeitems-latest.json')
except ValueError as e:
    print("ValueError:", e)
82/19:
with open('recipeitems-latest.json') as f:
    line = f.readline()
pd.read_json(line).shape
82/20:
# read the entire file into a Python array
with open('recipeitems-latest.json', 'r') as f:
    # Extract each line
    data = (line.strip() for line in f)
    # Reformat so each line is the element of a list
    data_json = "[{0}]".format(','.join(data))
# read the result as a JSON
recipes = pd.read_json(data_json)
82/21: recipes.shape
82/22:
!curl -O http://openrecipes.s3.amazonaws.com/recipeitems-latest.json.gz
!gunzip recipeitems-latest.json.gz
83/1:
import numpy as np
x = np.array([2, 3, 5, 7, 11, 13])
x * 2
83/2:
data = ['peter', 'Paul', 'MARY', 'gUIDO']
[s.capitalize() for s in data]
83/3:
data = ['peter', 'Paul', None, 'MARY', 'gUIDO']
[s.capitalize() for s in data]
83/4:
!curl -O http://openrecipes.s3.amazonaws.com/recipeitems-latest.json.gz
!gunzip recipeitems-latest.json.gz
84/1:
import numpy as np
x = np.array([2, 3, 5, 7, 11, 13])
x * 2
84/2:
data = ['peter', 'Paul', 'MARY', 'gUIDO']
[s.capitalize() for s in data]
84/3:
data = ['peter', 'Paul', None, 'MARY', 'gUIDO']
[s.capitalize() for s in data]
84/4:
try:
    recipes = pd.read_json('recipeitems-latest.json')
except ValueError as e:
    print("ValueError:", e)
84/5:
import pandas as pd
monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',
                   'Eric Idle', 'Terry Jones', 'Michael Palin'])
84/6:
try:
    recipes = pd.read_json('recipeitems-latest.json')
except ValueError as e:
    print("ValueError:", e)
84/7:
with open('recipeitems-latest.json') as f:
    line = f.readline()
pd.read_json(line).shape
84/8:
# read the entire file into a Python array
with open('recipeitems-latest.json', 'r') as f:
    # Extract each line
    data = (line.strip() for line in f)
    # Reformat so each line is the element of a list
    data_json = "[{0}]".format(','.join(data))
# read the result as a JSON
recipes = pd.read_json(data_json)
84/9: recipes.shape
84/10: recipes.iloc[0]
84/11: recipes.ingredients.str.len().describe()
84/12:
!curl -O http://openrecipes.s3.amazonaws.com/recipeitems-latest.json.gz
!gunzip recipeitems-latest.json.gz
84/13:
try:
    recipes = pd.read_json('recipeitems-latest.json')
except ValueError as e:
    print("ValueError:", e)
84/14:
try:
    recipes = pd.read_json('recipeitems-latest.json')
except ValueError as e:
    print("ValueError:", e)
84/15:
with open('recipeitems-latest.json') as f:
    line = f.readline()
pd.read_json(line).shape
84/16:
# read the entire file into a Python array
with open('recipeitems-latest.json', 'r') as f:
    # Extract each line
    data = (line.strip() for line in f)
    # Reformat so each line is the element of a list
    data_json = "[{0}]".format(','.join(data))
# read the result as a JSON
recipes = pd.read_json(data_json)
84/17: recipes.shape
84/18: !head recipeitems-latest.json -n1
84/19: !head recipeitems-latest.json
84/20: !head recipeitems-latest.json
84/21: !rm recipeitems-latest.json
84/22:
#!curl -O http://openrecipes.s3.amazonaws.com/recipeitems-latest.json.gz
!gunzip recipeitems-latest.json.gz
84/23: !head recipeitems-latest.json
84/24: !rm -rf recipeitems-latest.json
84/25: ls
84/26:
!curl -O http://openrecipes.s3.amazonaws.com/recipeitems-latest.json.gz
!gunzip recipeitems-latest.json.gz
84/27:
try:
    recipes = pd.read_json('recipeitems-latest.json')
except ValueError as e:
    print("ValueError:", e)
84/28:
try:
    recipes = pd.read_json('recipeitems-latest.json')
except ValueError as e:
    print("ValueError:", e)
84/29:
with open('recipeitems-latest.json') as f:
    line = f.readline()
pd.read_json(line).shape
84/30:
# read the entire file into a Python array
with open('recipeitems-latest.json', 'r') as f:
    # Extract each line
    data = (line.strip() for line in f)
    # Reformat so each line is the element of a list
    data_json = "[{0}]".format(','.join(data))
# read the result as a JSON
recipes = pd.read_json(data_json)
84/31: recipes.shape
85/1:
#Import pandas, matplotlib.pyplot, and seaborn
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
85/2:
#Import pandas, matplotlib.pyplot, and seaborn
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
85/3: os.getcwd()
85/4: os.getcwd()
85/5: os.listdir()
85/6: path= '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2'
85/7: os.getcwd()
85/8: os.getcwd()
85/9:
path= '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2'
os.chdir(path)
85/10: os.getcwd()
85/11: os.listdir()
85/12:
path= '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data'
os.chdir(path)
85/13: os.getcwd()
85/14: os.listdir()
85/15:
path= '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/raw'
os.chdir(path)
85/16: os.getcwd()
85/17: os.listdir()
85/18:
path= '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/raw/fredgraph'
os.chdir(path)
85/19: os.getcwd()
85/20: os.listdir()
85/21: os.listdir()
85/22:
# load FRED data - current US national data
ski_data = pd.read_csv('Annual.csv')
85/23:
# load FRED data - current US national data
df = pd.read_csv('Annual.csv')
85/24: df.head()
85/25: df.info()
85/26:
# load FRED data - current US national data
df_year = pd.read_csv('Annual.csv')
85/27:
# load FRED data - current US national data
df_yr = pd.read_csv('Annual.csv')
df_m = pd.read_csv('Monthly.csv')
df_q = pd.read_csv('Quarterly.csv')
85/28: df_m.info()
85/29: df_m.head()
85/30: df_m.tail()
85/31: df_yr.tail()
85/32: df_q.tail()
85/33: df_m.tail()
85/34: df_yr.tail()
85/35:
#see a summary of the data
df_yr.info()
85/36:
#see a summary of the data
df_yr.info()
85/37: df_m.info()
85/38: df_m.info()
85/39: df_q.info()
85/40: df_q.info()
85/41:
#Call the tail method on FRED data
df_yr.head()
85/42:
#Call the tail method on FRED data
df_yr.tail()
85/43:
#Call the tail method on FRED data
df_yr.head()
85/44:
#Call the tail method on FRED data
df_yr.head()
df_yr.tail()
85/45:
#Call the tail method on FRED data
df_yr.head()
85/46:
#Call the head method on FRED data
df_yr.head()
85/47: df_m.head()
85/48: df_m.tail()
85/49: df_m.tail()
85/50: df_m.tail()
85/51: df_q.head()
85/52: df_q.tail()
85/53: df_q.head()
85/54: df_q.tail()
85/55: df_m.head()
85/56: df_m.tail()
85/57: #convert DATE to datetime object
85/58: #merge/join/concat the 3 dataframes
85/59: #convert dtype objects to floats as needed
85/60:
#WHICH ONE FIRST?
#merge/join/concat the 3 dataframes
#convert DATE to datetime object
df_q['DATE']= pd.to_datetime(df_q['DATE'])
df_q.tail()
85/61:
#WHICH ONE FIRST?
#merge/join/concat the 3 dataframes
#convert DATE to datetime object
df_q['DATE']= pd.to_datetime(df_q['DATE'])
df_q.dtype()
85/62:
#WHICH ONE FIRST?
#merge/join/concat the 3 dataframes
#convert DATE to datetime object
df_q['DATE']= pd.to_datetime(df_q['DATE'])
df_q.dtypes
85/63:
df_m['DATE']= pd.to_datetime(df_m['DATE'])
df_m.tail()
#df_q.dtypes
85/64:
df_m['DATE']= pd.to_datetime(df_m['DATE'])
df_m.dtypes
85/65:
df_m['DATE']= pd.to_datetime(df_m['DATE'])
df_m.dtypes
85/66:
df_yr['DATE']= pd.to_datetime(df_yr['DATE'])
df_y.head()
85/67:
df_yr['DATE']= pd.to_datetime(df_yr['DATE'])
df_yr.head()
85/68: #scale data to commpare (maybe prepreprocessing?)
85/69:
df_yr['DATE']= pd.to_datetime(df_yr['DATE'])
df_yr.dtypes
85/70:
#merge/join/concat the 3 dataframes
#preserve month and drop the outlier years
#df2 = merge yr w/ q, then df3 = merge df2 w/ m
85/71: #then df3 = merge df2 w/ m
85/72: df_q#.tail()
85/73: df_m#.tail()
85/74:
#Call the head/tail method to explore FRED data
df_yr#.head()
85/75:
#Call the head/tail method to explore FRED data
#SPPOPGROWUSA has data from 1961-01-01 to 2019-01-01
#MEHOINUSA672N has data from 1984-01-01 to 2018-01-01
df_yr.head()
85/76:
#df_m.tail()
df_m['UNRATE']
85/77: df_m.tail()
85/78: #change column headers to make more user friendly
85/79:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill") #left_by="group"
df2
85/80:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill") #left_by="group"
df2.T
85/81:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill") #left_by="group"
df2
85/82:
#Call the head/tail method to explore FRED data
#SPPOPGROWUSA has data from 1961-01-01 to 2019-01-01
#MEHOINUSA672N has data from 1984-01-01 to 2018-01-01
df_yr
85/83:
#see an info summary of the data
#df_yr.info()
85/84: #df_m.info()
85/85: #df_q.info()
85/86:
#see an info summary of the data
df_yr.info()
85/87: df_m.info()
85/88: df_q.info()
85/89:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill") #left_by="group"
df2.loc[1984-01-01: , :]
85/90:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill") #left_by="group"
df2.loc['1984-01-01': , :]
85/91:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill") #left_by="group"
df2.loc['1984': , :]
85/92:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill") #left_by="group"
df2.loc['1984':]
85/93:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill") #left_by="group"
df2.loc[:, '1984':]
85/94:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill") #left_by="group"
df2.loc['1984':]
85/95:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill") #left_by="group"
df2.loc['1984']
85/96:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill") #left_by="group"
df2.loc['1984':]
85/97:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill") #left_by="group"
df2
85/98:
#merge/join/concat the 3 dataframes

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill")
df2
85/99:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
85/100:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3
85/101:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3['1987':]
85/102:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3['1987':, :]
85/103:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc['1987':, :]
85/104:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc['1987':]
85/105:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc['1987-01-01':,:]
85/106:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3
85/107:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3['1987']
85/108:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc['1987']
85/109:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc['1987',]
85/110:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc['1987':]
85/111:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc['2000':]
86/1:
#Import pandas, matplotlib.pyplot, and seaborn
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
86/2:
#change directory to get FRED data
path= '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/raw/fredgraph'
os.chdir(path)
86/3: os.getcwd()
86/4: os.listdir()
86/5:
# load FRED data - current US national data
df_yr = pd.read_csv('Annual.csv')
df_m = pd.read_csv('Monthly.csv')
df_q = pd.read_csv('Quarterly.csv')
86/6:
#see an info summary of the data
df_yr.info()
86/7: df_m.info()
86/8: df_q.info()
86/9:
#Call the head/tail method to explore FRED data
#SPPOPGROWUSA has data from 1961-01-01 to 2019-01-01
#MEHOINUSA672N has data from 1984-01-01 to 2018-01-01
df_yr
86/10:
'''
UNRATE data from 1948-01-01 to 2020-07-01
INTDSRUSM193N from 1950-01-01 to 2020-07-01
CUUR0000SEHA from 1914-12-01 to 2020-07-01
CSUSHPINSA from 1987-01-01 to 2020-06-01
HOUST from 1959-01-01 to 2020-07-01
WPUIP2311001 from 1986-06-01 to 2020-07-01
TLRESCONS from 2002-01-01 to 2020-06-01
'''
df_m.tail()
86/11:
#RRVRUSQ156N is from 1956-01-01 to 2020-04-01
df_q.tail()
86/12:
#convert DATE columns to datetime object
df_q['DATE']= pd.to_datetime(df_q['DATE'])
df_q.dtypes
86/13:
df_m['DATE']= pd.to_datetime(df_m['DATE'])
df_m.dtypes
86/14:
df_yr['DATE']= pd.to_datetime(df_yr['DATE'])
df_yr.dtypes
86/15:
#merge yearly df with quarterly: df2

df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill")
df2
86/16:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc['2000':]
86/17: #preserve month and drop the outlier years
86/18: #convert dtype objects to floats as needed
86/19: #scale data to commpare (maybe prepreprocessing?)
86/20: #change column headers to make more user friendly
86/21:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc['1990':]
86/22:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc['1990-01-01':]
86/23:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc['1990-01-01':, :]
86/24:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc[datetime(1990-01-01):, :]
86/25:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.loc[datetime(1990):, :]
86/26:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3
86/27:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.iloc[500:]
86/28:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.iloc[700:]
86/29:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.iloc[900:]
86/30:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.iloc[800:]
86/31:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.iloc[850:]
86/32:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.iloc[853:]
86/33:
#drop the outlier years (pre 1987)
df3_87 = df3[df3['DATE'] >= 1987]
df3_87
86/34:
#drop the outlier years (pre 1987)
df3_87 = df3[df3['DATE'] >= '1987']
df3_87
86/35: ## Data Joining
86/36:
#see an info summary of the data
df_yr.info()
86/37: df_yr.describe()
86/38: df_yr.describe()
86/39:
#see an info summary of the data
df_yr.info()
86/40: df_yr.describe()
86/41: df_yr.describe().T
86/42: df_yr.describe()
86/43: df_yr.describe()
86/44: df_yr.value_counts().head()
86/45: df_yr['DATE'].value_counts().head()
86/46: df_yr.nunique()
86/47: df_yr.nunique()
86/48: df_yr.counts()
86/49: df_yr.value_counts()
86/50: df_yr.nunique()
86/51: df_yr.nunique()
86/52: df_yr['MEHOINUSA672N'].value_counts(normalize=True)*100
86/53: df_yr['MEHOINUSA672N'].value_counts(normalize=True)*100.head()
86/54: df_yr['MEHOINUSA672N'].value_counts(normalize=True)*100
86/55: df_m.info()
86/56: df_m.describe()
86/57: df_m.describe()
86/58: df_m.value_counts(normalize=True)*100
86/59: df_m['UNRATE'.value_counts(normalize=True)*100
86/60: df_m['UNRATE'].value_counts(normalize=True)*100
86/61: df_m['INTDSRUSM193N'].value_counts(normalize=True)*100
86/62: df_m['UNRATE'].value_counts(normalize=True)*100
86/63: df_m['INTDSRUSM193N'].value_counts(normalize=True)*100
86/64: df_m['CUUR0000SEHA'].value_counts(normalize=True)*100
86/65: df_m['CSUSHPINSA'].value_counts(normalize=True)*100
86/66: df_m['HOUST'].value_counts(normalize=True)*100
86/67: df_m['WPUIP2311001'].value_counts(normalize=True)*100
86/68: df_m['TLRESCONS'].value_counts(normalize=True)*100
86/69: df_q.info()
86/70: df_q.describe()
86/71: df_q.describe()
86/72: df_q.value_counts(normalize=True)*100
86/73: df_q['RRVRUSQ156N'].value_counts(normalize=True)*100
86/74: df_q.info()
86/75:
#RRVRUSQ156N is from 1956-01-01 to 2020-04-01
df_q.tail()
86/76:
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
df3.DATE.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
ax.set_title('Date')
#Label the xaxis 'Count'
ax.set_xlabel('Count')
86/77:
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
df3.DATE.value_counts().plot(kind='barh', ax=ax[0])
#Give the plot a helpful title of 'Region'
plot.set_title('Date')
#Label the xaxis 'Count'
plot.set_xlabel('Count')
plt.show()
86/78:
#Specify a horizontal barplot ('barh') as kind of plot (kind=)
df3.DATE.value_counts().plot(kind='barh')
#Give the plot a helpful title of 'Region'
plot.set_title('Date')
#Label the xaxis 'Count'
plot.set_xlabel('Count')
plt.show()
86/79: #WANT TO CREATE BAR PLOT OF DATA PER YEAR TO HELP CHOOSE TIME BOUNDS FOR DATA, AND SEABORN BOXPLOT OF ALL FEATURES
86/80: #WANT TO CREATE BAR PLOT OF DATA PER YEAR TO HELP CHOOSE TIME BOUNDS FOR DATA, AND SEABORN BOXPLOT OF ALL FEATURES
86/81:
#Call df3's `hist` method to plot histograms of each of the numeric features
df3.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
86/82: #convert dtype objects to floats as needed
86/83: df3.dtypes
86/84: df3.dtypes
86/85: df3.describe()
86/86: df3.dtypes
86/87: #convert dtype objects to floats as needed
86/88:
#convert dtype objects to floats as needed
df3 = df3.astype({"MEHOINUSA672N": float, "UNRATE": float, "INTDSRUSM193N", "CUUR0000SEHA", "CSUSHPINSA", "HOUST", ""})df3 = df3.astype({"MEHOINUSA672N": float, "UNRATE": float, "INTDSRUSM193N", "CUUR0000SEHA", "CSUSHPINSA", "HOUST", ""})df3 = df3.astype({"MEHOINUSA672N": float, "UNRATE": float, "INTDSRUSM193N": float, "CUUR0000SEHA": float, "CSUSHPINSA": float, "HOUST": float, "WPUIP2311001": float, "TLRESCONS": float})
86/89:
#convert dtype objects to floats as needed
df3 = df3.astype({"MEHOINUSA672N": float, "UNRATE": float, "INTDSRUSM193N": float, "CUUR0000SEHA": float, "CSUSHPINSA": float, "HOUST": float, "WPUIP2311001": float, "TLRESCONS": float})
86/90: df3.dtypes
86/91:
#replace '.'s with NaN
df3.replace(".", np.nan)
86/92:
#Import pandas, matplotlib.pyplot, and seaborn
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import numpy as np
86/93:
#replace '.'s with NaN
df3.replace(".", np.nan)
86/94:
#convert dtype objects to floats as needed
df3 = df3.astype({"MEHOINUSA672N": float, "UNRATE": float, "INTDSRUSM193N": float, "CUUR0000SEHA": float, "CSUSHPINSA": float, "HOUST": float, "WPUIP2311001": float, "TLRESCONS": float})
86/95:
#convert dtype objects to floats as needed
df3 = df3.astype({"MEHOINUSA672N": float, "UNRATE": float, "INTDSRUSM193N": float, "CUUR0000SEHA": float, "CSUSHPINSA": float, "HOUST": float, "WPUIP2311001": float, "TLRESCONS": float})
86/96:
#replace '.'s with NaN
df3 = df3.replace(".", np.nan)
86/97:
#convert dtype objects to floats as needed
df3 = df3.astype({"MEHOINUSA672N": float, "UNRATE": float, "INTDSRUSM193N": float, "CUUR0000SEHA": float, "CSUSHPINSA": float, "HOUST": float, "WPUIP2311001": float, "TLRESCONS": float})
86/98:
#convert dtype objects to floats as needed
df3 = df3.astype({"MEHOINUSA672N": float, "UNRATE": float, "INTDSRUSM193N": float, "CUUR0000SEHA": float, "CSUSHPINSA": float, "HOUST": float, "WPUIP2311001": float, "TLRESCONS": float})
86/99: df3
86/100: df3.dtypes
86/101: #change column headers to make more user friendly
86/102: #deal with NAN
86/103: df3.info()
86/104: df3.info()
86/105: df3.info()
86/106: df3.describe()
86/107:
#explore combined datas in df3
df3.info()
86/108: df3.describe()
86/109: df3.describe()
86/110: df3.nunique()
86/111: df3.nunique()
86/112: df3['UNRATE']value_counts(normalize=True)*100
86/113: df3['UNRATE'].value_counts(normalize=True)*100
86/114: df3['UNRATE'].value_counts(normalize=True)*100
86/115: df3.value_counts
86/116: df3.value_counts()
86/117: df3['SPPOPGROWUSA'].value_counts(normalize=True)*100
86/118: df3['MEHOINUSA672N'].value_counts(normalize=True)*100
86/119: df3['INTDSRUSM193N'].value_counts(normalize=True)*100
86/120: df3['CUUR0000SEHA'].value_counts(normalize=True)*100
86/121: df3['CSUSHPINSA'].value_counts(normalize=True)*100
86/122: df3['HOUST'].value_counts(normalize=True)*100
86/123: df3['WPUIP2311001'].value_counts(normalize=True)*100
86/124: df3['TLRESCONS'].value_counts(normalize=True)*100
86/125: df3['RRVRUSQ156N'].value_counts(normalize=True)*100
86/126:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'USPOP_GROWTH', 'MEHOINUSA672N': 'MED_HINCOME', 'UNRATE': 'UNEMPLT_RATE', 'INTDSRUSM193N': 'INT_RATE', 'CUUR0000SEHA': 'CPI_RENT', 'CSUSHPINSA': 'HOMEPRICE_INDEX', 'HOUST': 'NEWHOUSE_STARTS', 'WPUIP2311001': 'PPI_RESCONSTRUCT'}, inplace = True)
86/127:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'USPOP_GROWTH', 'MEHOINUSA672N': 'MED_HINCOME', 'UNRATE': 'UNEMPLT_RATE', 'INTDSRUSM193N': 'INT_RATE', 'CUUR0000SEHA': 'CPI_RENT', 'CSUSHPINSA': 'HOMEPRICE_INDEX', 'HOUST': 'NEWHOUSE_STARTS', 'WPUIP2311001': 'PPI_RESCONSTRUCT'}, inplace = True)
df3
86/128:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'USPOP_GROWTH', 'MEHOINUSA672N': 'MED_HINCOME', 'UNRATE': 'UNEMPLT_RATE', 'INTDSRUSM193N': 'INT_RATE', 'CUUR0000SEHA': 'CPI_RENT', 'CSUSHPINSA': 'HOMEPRICE_INDEX', 'HOUST': 'NEWHOUSE_STARTS', 'WPUIP2311001': 'PPI_RESCONSTRUCT', 'RRVRUSQ156N': 'RENTL_VACNYRATE'}, inplace = True)
df3
86/129:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'USPOP_GROWTH', 'MEHOINUSA672N': 'MED_HINCOME', 'UNRATE': 'UNEMPLT_RATE', 'INTDSRUSM193N': 'INT_RATE', 'CUUR0000SEHA': 'CPI_RENT', 'CSUSHPINSA': 'HOMEPRICE_INDEX', 'HOUST': 'NEWHOUSE_STARTS', 'WPUIP2311001': 'PPI_RES_CONSTRUCT', 'RRVRUSQ156N': 'RENTL_VACNYRATE', 'TLRESCONS': 'RES_CONSTRUCT_SPENDING'}, inplace = True)
df3
86/130:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'USPOP_GROWTH', 'MEHOINUSA672N': 'MED_HINCOME', 'UNRATE': 'UNEMPLT_RATE', 'INTDSRUSM193N': 'INT_RATE', 'CUUR0000SEHA': 'CPI_RENT', 'CSUSHPINSA': 'HOMEPRICE_INDEX', 'HOUST': 'NEWHOUSE_STARTS', 'WPUIP2311001': 'PPI_RES_CONSTRUCT', 'RRVRUSQ156N': 'RENTL_VACNYRATE', 'TLRESCONS': 'RES_CONSTRUCT_SPENDING'}, inplace = True)
df3
86/131:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'USPOP_GROWTH', 'MEHOINUSA672N': 'MED_HINCOME', 'UNRATE': 'UNEMPLT_RATE', 'INTDSRUSM193N': 'INT_RATE', 'CUUR0000SEHA': 'CPI_RENT', 'CSUSHPINSA': 'HOMEPRICE_INDEX', 'HOUST': 'NEWHOUSE_STARTS', 'WPUIP2311001': 'PPI_RES_CONSTRUCT', 'RRVRUSQ156N': 'RENTL_VACNYRATE', 'TLRESCONS': 'RES_CONSTRUCT_SPENDING'}, inplace = True)
df3
86/132:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'USPOP_GROWTH', 'MEHOINUSA672N': 'MED_HINCOME', 'UNRATE': 'UNEMPLT_RATE', 'INTDSRUSM193N': 'INT_RATE', 'CUUR0000SEHA': 'CPI_RENT', 'CSUSHPINSA': 'HOMEPRICE_INDEX', 'HOUST': 'NEWHOUSE_STARTS', 'WPUIP2311001': 'PPI_RES_CONSTRUCT', 'RRVRUSQ156N': 'RENTL_VACNYRATE', 'TLRESCONS': 'RES_CONSTRUCT_SPENDING'}, inplace = True)
86/133:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'USPOP_GROWTH', 'MEHOINUSA672N': 'MED_HINCOME', 'UNRATE': 'UNEMPLT_RATE', 'INTDSRUSM193N': 'INT_RATE', 'CUUR0000SEHA': 'CPI_RENT', 'CSUSHPINSA': 'HOMEPRICE_INDEX', 'HOUST': 'NEWHOUSE_STARTS', 'WPUIP2311001': 'PPI_RES_CONSTRUCT', 'RRVRUSQ156N': 'RENTL_VACNYRATE', 'TLRESCONS': 'RES_CONSTRUCT_SPENDING'}, inplace = True)
df3
86/134:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'USPOP_GROWTH', 'MEHOINUSA672N': 'MED_HINCOME', 'UNRATE': 'UNEMPLT_RATE', 'INTDSRUSM193N': 'INT_RATE', 'CUUR0000SEHA': 'CPI_RENT', 'CSUSHPINSA': 'HOMEPRICE_INDEX', 'HOUST': 'NEWHOUSE_STARTS', 'WPUIP2311001': 'PPI_RESCONSTRUCT', 'RRVRUSQ156N': 'RENTL_VACNYRATE', 'TLRESCONS': 'RES_CONSTRUCT_SPENDING'}, inplace = True)
df3
86/135:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'USPOP_GROWTH', 'MEHOINUSA672N': 'MED_HINCOME', 'UNRATE': 'UNEMPLT_RATE', 'INTDSRUSM193N': 'INT_RATE', 'CUUR0000SEHA': 'CPI_RENT', 'CSUSHPINSA': 'HOMEPRICE_INDEX', 'HOUST': 'NEWHOUSE_STARTS', 'WPUIP2311001': 'PPI_RESCONSTRUCT', 'RRVRUSQ156N': 'RENTL_VACNYRATE', 'TLRESCONS': 'RESCONSTRUCT_SPENDING'}, inplace = True)
df3
86/136:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'USPOP_GROWTH', 'MEHOINUSA672N': 'MED_HINCOME', 'UNRATE': 'UNEMPLT_RATE', 'INTDSRUSM193N': 'INT_RATE', 'CUUR0000SEHA': 'CPI_RENT', 'CSUSHPINSA': 'HOMEPRICE_INDEX', 'HOUST': 'NEWHOUSE_STARTS', 'WPUIP2311001': 'PPI_RESCONSTRUCT', 'RRVRUSQ156N': 'RENTL_VACNYRATE', 'TLRESCONS': 'RES_CONSTRUCT_SPENDING'}, inplace = True)
df3
86/137:
#drop the outlier years (pre 1987), 9/10 features have data 1987 on.
#df3_87 = df3[df3['DATE'] >= '1987']
#df3_87
nonnull_counts = df3.apply(lambda x: x.count(), axis=1)
nonnull_counts
86/138:
#drop the outlier years (pre 1987), 9/10 features have data 1987 on.
#df3_87 = df3[df3['DATE'] >= '1987']
#df3_87
nonnull_counts = df3.apply(lambda x: x.count(), axis=1)
nonnull_counts
86/139: nonull_counts.describe()
86/140: nonnull_counts.describe()
86/141: nonnull_counts.info()
86/142: nonnull_counts.desribe()
86/143: nonnull_counts.describe()
86/144: nonnull_counts.describe()
86/145: nonnull_counts.describe()
86/146: nonnull_counts.plot.hist()
86/147: nonnull_counts.plot()
86/148: nonnull_counts.plot()
86/149: nonnull_counts.plot(king='boxplot')
86/150: nonnull_counts.plot(kind='boxplot')
86/151: nonnull_counts.boxplot()
86/152: nonnull_counts.plt.boxplot()
86/153: nonnull_counts.plt.boxplot
86/154:
#drop the outlier years (pre 1987), 9/10 features have data 1987 on.
#df3_87 = df3[df3['DATE'] >= '1987']
#df3_87
nonnull_counts = df3.apply(lambda x: x.count(), axis=1)
nonnull_counts = pd.DataFrame(nonnull_counts)
nonnull_counts
86/155: nonnull_counts.describe()
86/156: nonnull_counts.plot()
86/157: plt.boxplot(nonnull_counts)
86/158: pd.nonnull_counts.boxplot
86/159: nonnull_counts.boxplot
86/160: nonnull_counts.boxplot()
86/161: nonnull_counts.plot(x='index number', y='counts')
86/162:
nonnull_counts.plot()
plt.xlabel('index number')
plt.ylabel('counts')
plt.show()
86/163: nonnull.iloc[350]
86/164: nonnull_counts.iloc[350]
86/165: nonnull_counts.iloc[350:]
86/166: nonnull_counts.iloc[400:]
86/167: nonnull_counts.iloc[299:]
86/168: nonnull_counts.iloc[399:]
86/169: nonnull_counts.iloc[380:]
86/170: nonnull_counts.iloc[390:]
86/171: nonnull_counts.iloc[385:]
86/172: nonnull_counts.iloc[385:, :1100]
86/173: nonnull_counts.iloc[400:, :1100]
86/174: nonnull_counts.iloc[390:, :1100]
86/175: nonnull_counts.iloc[385:, :1100]
86/176: nonnull_counts.iloc[380:, :1100]
86/177: nonnull_counts.iloc[384:, :1100]
86/178:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts.iloc[384:1100]
86/179:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts.iloc[384:1200]
86/180:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts.iloc[384:1250]
86/181:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts.iloc[384:11275]
86/182:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts.iloc[384:1275]
86/183:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts.iloc[384:1275]
86/184:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts[nonnull_counts.Length >=2]
86/185:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts[nonnull_counts.len >=2]
86/186: nonnull_counts.iloc[384:1275]
86/187:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts[nonnull_counts.0 >=2]
86/188:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts[nonnull_counts[>=2]
86/189:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts[nonnull_counts[>=2]]
86/190:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts[nonnull_counts[0] >=2]]
86/191:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts[nonnull_counts[0] >=2]
86/192:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts[nonnull_counts[0] >=2 and nonnull_counts <=10]
86/193:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts[nonnull_counts[0] >=2 and nonnull_counts[0] <=10]
86/194:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts[[nonnull_counts[0] >=2] and [nonnull_counts[0] <=10]
86/195:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts[[nonnull_counts[0] >=2] and [nonnull_counts[0] <=10]]
86/196: nonnull_counts.iloc[384:1275]
86/197: nonnull_counts.iloc[384:1290]
86/198: nonnull_counts.iloc[384:]
86/199: nonnull_counts.iloc[384:1250]
86/200: nonnull_counts.iloc[384:1260]
86/201: nonnull_counts.iloc[384:1255]
86/202: nonnull_counts.iloc[384:1260]
86/203: nonnull_counts.iloc[384:]
86/204:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts[[nonnull_counts[0] >=2] and [nonnull_counts[0] <=10]]
86/205:
#to get within 25-75% of non null data, need to use iloc384 
nonnull_counts.iloc[384:]
86/206:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[400:]
86/207:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[395:]
86/208:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[390:]
86/209:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[385:]
86/210:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[380:]
86/211:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[382:]
86/212:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[384:]
86/213:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[384:1266]
86/214:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[384:1267]
86/215:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[384:]
86/216:
#drop the outlier years (pre 1987), 9/10 features have data 1987 on.
df3_drop = df3.iloc[385:1267]
df3_drop
86/217:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[450:]
86/218:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[430:]
86/219:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[400:]
86/220:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[390:]
86/221:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[400:]
86/222:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[4390:]
86/223:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[390:]
86/224:
#to get within 25-75% of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[395:]
86/225:
#plot nonnull counts, seems like index 400-1100 is ballpark 25-75% 
nonnull_counts.plot()
plt.xlabel('index number')
plt.ylabel('counts')
plt.show()
86/226: nonnull_counts.plot.hist()
86/227:
#drop the outlier years (pre 1947 and post 6/2020)
df3_drop = df3.iloc[397:1267]
df3_drop
86/228:
nonnull_counts.plot.hist()
plt.xlabel('non-null counts')
86/229:
#plot nonnull counts, seems like index 400-1100 is ballpark 25-75% 
nonnull_counts.plot()
plt.xlabel('index number')
plt.ylabel('non-null counts')
plt.show()
86/230:
#to get within 1 standard deviation of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[410:]
86/231:
#to get within 1 standard deviation of non null data, need to use rows 385-1266 inclusive
nonnull_counts.iloc[420:]
86/232:
#drop the outlier years (pre 1947 and post 6/2020)
df3_drop = df3.iloc[421:1267]
df3_drop
86/233:
#to get within 1 standard deviation of non null data, need to use rows 421-1266 inclusive
nonnull_counts.iloc[420:]
86/234:
#drop the outlier years for NaN values (pre 1950 and post 6/2020)
df3_drop = df3.iloc[421:1267]
df3_drop
86/235: df3_drop.info()
86/236: df3_drop.info()
86/237: df3_drop.info()
86/238: df3.info()
86/239: df3.describe()
86/240: df3_drop.desribe()
86/241: df3_drop.describe()
86/242: df3_drop.describe().T
86/243: df3.describe()/T
86/244: df3.describe().T
86/245: df3.describe().T
86/246: df3.describe().T
86/247:
df3_drop.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
86/248:
df3_drop.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
86/249:
df3.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
86/250:
#examine distribution of values, don't seem to be any extreme outliers
df3_drop.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
86/251:
#drop rows with no rental vacany data
missing_vacany = df3[['RNTL_VACNYRATE']].isnull().sum(axis=1)
missing_vacany.value_counts()/len(missing_price) * 100
86/252:
#drop rows with no rental vacany data
missing_vacany = df3[['RNTL_VACNYRATE']].isnull().sum(axis=1)
missing_vacany.value_counts()/len(missing_vacany) * 100
86/253:
#drop rows with no rental vacany data
missing_vacancy = df3[['RNTL_VACNYRATE']].isnull().sum(axis=1)
missing_vacancy.value_counts()/len(missing_vacancy) * 100
86/254:
#drop rows with no rental vacany data
missing_vacancy = df3[['RNTL_VACNYRATE']].isnull().sum()
missing_vacancy.value_counts()/len(missing_vacancy) * 100
86/255:
#drop rows with no rental vacany data
missing_vacancy = df3[['RNTL_VACNYRATE']].isnull().sum(axis=1)
#missing_vacancy.value_counts()/len(missing_vacancy) * 100
missing_vacancy
86/256:
#drop rows with no rental vacany data
missing_vacancy = df3['RNTL_VACNYRATE'].isnull().sum(axis=1)
#missing_vacancy.value_counts()/len(missing_vacancy) * 100
missing_vacancy
86/257:
#drop rows with no rental vacany data
missing_vacancy = df3['RENTL_VACNYRATE'].isnull().sum(axis=1)
#missing_vacancy.value_counts()/len(missing_vacancy) * 100
missing_vacancy
86/258:
#drop rows with no rental vacany data
missing_vacancy = df3['RENTL_VACNYRATE'].isnull().sum()
#missing_vacancy.value_counts()/len(missing_vacancy) * 100
missing_vacancy
86/259:
#drop rows with no rental vacany data
missing_vacancy = df3['RENTL_VACNYRATE'].isnull().sum()
missing_vacancy.value_counts()/len(missing_vacancy) * 100
86/260:
#drop rows with no rental vacany data
missing_vacancy = df3['RENTL_VACNYRATE'].isnull().
#missing_vacancy.value_counts()/len(missing_vacancy) * 100
missing_vacany
86/261:
#drop rows with no rental vacany data
missing_vacancy = df3['RENTL_VACNYRATE'].isnull()
#missing_vacancy.value_counts()/len(missing_vacancy) * 100
missing_vacany
86/262:
#drop rows with no rental vacany data
missing_vacancy = df3['RENTL_VACNYRATE'].isnull()
#missing_vacancy.value_counts()/len(missing_vacancy) * 100
missing_vacancy
86/263:
#drop rows with no rental vacany data
missing_vacancy = df3['RENTL_VACNYRATE'].isnull().sum()
#missing_vacancy.value_counts()/len(missing_vacancy) * 100
missing_vacancy
86/264:
#drop the outlier years for NaN values (pre 1950 and post 6/2020)
df3_drop = df3.iloc[421:1267]
df3_drop
86/265:
#create another data frame that drops rows with no vacany rate data
#df3_56 = df3[df3['DATE'] >= '1956']

missing_vacancy = ski_data['RENTL_VACNYRATE'].isnull()
missing_vacancy
86/266:
#create another data frame that drops rows with no vacany rate data
#df3_56 = df3[df3['DATE'] >= '1956']

missing_vacancy = df3['RENTL_VACNYRATE'].isnull()
missing_vacancy
86/267:
#create another data frame that drops rows with no vacany rate data
#df3_56 = df3[df3['DATE'] >= '1956']

missing_vacancy = df3['RENTL_VACNYRATE'].isnull()
df3_noVacant = df3[missing_vacancy == True]
df3_noVacant
86/268:
#create another data frame that drops rows with no vacany rate data
#df3_56 = df3[df3['DATE'] >= '1956']

missing_vacancy = df3['RENTL_VACNYRATE'].isnull()
df3_noVacant = df3[missing_vacancy == False]
df3_noVacant
86/269:
#drop the outlier years for NaN values (pre 1950 and post 6/2020)
df3.iloc[421:1267]
86/270:
#the years outside 1 standard deviation of NaN values are pre 1950 and post 6/2020
df3.iloc[421:1267]
86/271: df3_noVacant.describe().T
86/272: df3.describe().T
86/273:
#examine distribution of values, don't seem to be any extreme outliers
df3_noVacant.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
86/274: #scale data would need to be done to commpare (maybe prepreprocessing?)
86/275:
savepath= '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/interim'
df3_noVacant.to_csv(savepath, index=False)
86/276:
savepath = '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/interim/'
df3_noVacant.to_csv(savepath, index=False)
86/277: df3_noVacant.to_csv('/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/interim/', index=False)
86/278: df3_noVacant.to_csv(r'/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/interim/', index=False)
86/279: df3_noVacant.to_csv(r'/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/interim/df3_noVacant', index=False)
86/280:
#vacany rate data starts at year 1956, so we can confidently drop rows without rental vacancy rate and get the majority of the data
#create another data frame that drops rows with no vacany rate data
missing_vacancy = df3['RENTL_VACNYRATE'].isnull()
df3_noVacant = df3[missing_vacancy == False]
df3_noVacant
86/281: df3_noVacant[df3_noVacant['RENTL_VACNYRATE' = pd.isnull()]]
86/282: df3_noVacant[df3_noVacant['RENTL_VACNYRATE' == pd.isnull()]]
86/283: df3_noVacant[df3_noVacant['RENTL_VACNYRATE' == isna()]]
86/284: df3_noVacant[df3_noVacant['RENTL_VACNYRATE'] == isna()]
86/285: df3_noVacant[df3_noVacant['RENTL_VACNYRATE'] == df3_noVacant.isna()]
86/286: df3_noVacant['RENTL_VACNYRATE'].isnull().values.any()
86/287: df3_noVacant['RENTL_VACNYRATE'].isnull()
86/288:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'uspop_growth', 'MEHOINUSA672N': 'med_hIncome', 'UNRATE': 'unemplt_rate', 'INTDSRUSM193N': 'int_rate', 'CUUR0000SEHA': 'cpi_rent', 'CSUSHPINSA': 'homePrice_index', 'HOUST': 'newHouse_starts', 'WPUIP2311001': 'ppi_resConstruct', 'RRVRUSQ156N': 'rentl_vacnyRate', 'TLRESCONS': 'resConstruct_spending'}, inplace = True)
df3.T
86/289:
#the years outside 1 standard deviation of NaN values are pre 1950 and post 6/2020
df3.iloc[421:1267].T
87/1:
#Import pandas, matplotlib.pyplot, and seaborn
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import numpy as np
87/2:
#change directory to get FRED data
path= '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/raw/fredgraph'
os.chdir(path)
87/3: os.getcwd()
87/4: os.listdir()
87/5:
# load FRED data - current US national data
df_yr = pd.read_csv('Annual.csv')
df_m = pd.read_csv('Monthly.csv')
df_q = pd.read_csv('Quarterly.csv')
87/6:
#see info/describe/unique value counts to get a summary of the data
df_yr.info()
87/7: df_yr.describe()
87/8: df_yr.nunique()
87/9: df_yr['MEHOINUSA672N'].value_counts(normalize=True)*100
87/10: df_m.info()
87/11: df_m.describe()
87/12: df_m['TLRESCONS'].value_counts(normalize=True)*100
87/13: df_q.info()
87/14: df_q['RRVRUSQ156N'].value_counts(normalize=True)*100
87/15: df_q.describe()
87/16:
#Call the head/tail method to explore FRED data
#SPPOPGROWUSA has data from 1961-01-01 to 2019-01-01
#MEHOINUSA672N has data from 1984-01-01 to 2018-01-01
df_yr
87/17:
'''
UNRATE data from 1948-01-01 to 2020-07-01
INTDSRUSM193N from 1950-01-01 to 2020-07-01
CUUR0000SEHA from 1914-12-01 to 2020-07-01
CSUSHPINSA from 1987-01-01 to 2020-06-01
HOUST from 1959-01-01 to 2020-07-01
WPUIP2311001 from 1986-06-01 to 2020-07-01
TLRESCONS from 2002-01-01 to 2020-06-01
'''
df_m.tail()
87/18:
#RRVRUSQ156N is from 1956-01-01 to 2020-04-01
df_q.tail()
87/19: #WANT TO CREATE BAR PLOT OF DATA PER YEAR TO HELP CHOOSE TIME BOUNDS FOR DATA, AND SEABORN BOXPLOT OF ALL FEATURES
87/20:
#Call df3's `hist` method to plot histograms of each of the numeric features
df3.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
87/21:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'uspop_growth', 'MEHOINUSA672N': 'med_hIncome', 'UNRATE': 'unemplt_rate', 'INTDSRUSM193N': 'int_rate', 'CUUR0000SEHA': 'cpi_rent', 'CSUSHPINSA': 'homePrice_index', 'HOUST': 'newHouse_starts', 'WPUIP2311001': 'ppi_resConstruct', 'RRVRUSQ156N': 'rentl_vacnyRate', 'TLRESCONS': 'resConstruct_spending'}, inplace = True)
df3.T
87/22:
#Import pandas, matplotlib.pyplot, and seaborn
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import numpy as np
87/23:
#change directory to get FRED data
path= '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/raw/fredgraph'
os.chdir(path)
87/24: os.getcwd()
87/25: os.listdir()
87/26:
# load FRED data - current US national data
df_yr = pd.read_csv('Annual.csv')
df_m = pd.read_csv('Monthly.csv')
df_q = pd.read_csv('Quarterly.csv')
87/27:
#see info/describe/unique value counts to get a summary of the data
df_yr.info()
87/28: df_yr.describe()
87/29: df_yr.nunique()
87/30: df_yr['MEHOINUSA672N'].value_counts(normalize=True)*100
87/31: df_m.info()
87/32: df_m.describe()
87/33: df_m['TLRESCONS'].value_counts(normalize=True)*100
87/34: df_q.info()
87/35: df_q['RRVRUSQ156N'].value_counts(normalize=True)*100
87/36: df_q.describe()
87/37:
#Call the head/tail method to explore FRED data
#SPPOPGROWUSA has data from 1961-01-01 to 2019-01-01
#MEHOINUSA672N has data from 1984-01-01 to 2018-01-01
df_yr
87/38:
'''
UNRATE data from 1948-01-01 to 2020-07-01
INTDSRUSM193N from 1950-01-01 to 2020-07-01
CUUR0000SEHA from 1914-12-01 to 2020-07-01
CSUSHPINSA from 1987-01-01 to 2020-06-01
HOUST from 1959-01-01 to 2020-07-01
WPUIP2311001 from 1986-06-01 to 2020-07-01
TLRESCONS from 2002-01-01 to 2020-06-01
'''
df_m.tail()
87/39:
#RRVRUSQ156N is from 1956-01-01 to 2020-04-01
df_q.tail()
87/40: #WANT TO CREATE BAR PLOT OF DATA PER YEAR TO HELP CHOOSE TIME BOUNDS FOR DATA, AND SEABORN BOXPLOT OF ALL FEATURES
87/41:
#Call df3's `hist` method to plot histograms of each of the numeric features
df3.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
87/42:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'uspop_growth', 'MEHOINUSA672N': 'med_hIncome', 'UNRATE': 'unemplt_rate', 'INTDSRUSM193N': 'int_rate', 'CUUR0000SEHA': 'cpi_rent', 'CSUSHPINSA': 'homePrice_index', 'HOUST': 'newHouse_starts', 'WPUIP2311001': 'ppi_resConstruct', 'RRVRUSQ156N': 'rentl_vacnyRate', 'TLRESCONS': 'resConstruct_spending'}, inplace = True)
df3.T
87/43:
#determine the outlier for NaNs
nonnull_counts = df3.apply(lambda x: x.count(), axis=1)
nonnull_counts = pd.DataFrame(nonnull_counts)
nonnull_counts
88/1:
#Import pandas, matplotlib.pyplot, and seaborn
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import numpy as np
88/2:
#change directory to get FRED data
path= '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/raw/fredgraph'
os.chdir(path)
88/3: os.getcwd()
88/4: os.listdir()
88/5:
# load FRED data - current US national data
df_yr = pd.read_csv('Annual.csv')
df_m = pd.read_csv('Monthly.csv')
df_q = pd.read_csv('Quarterly.csv')
88/6:
#see info/describe/unique value counts to get a summary of the data
df_yr.info()
88/7: df_yr.describe()
88/8: df_yr.nunique()
88/9: df_yr['MEHOINUSA672N'].value_counts(normalize=True)*100
88/10: df_m.info()
88/11: df_m.describe()
88/12: df_m['TLRESCONS'].value_counts(normalize=True)*100
88/13: df_q.info()
88/14: df_q['RRVRUSQ156N'].value_counts(normalize=True)*100
88/15: df_q.describe()
88/16:
#Call the head/tail method to explore FRED data
#SPPOPGROWUSA has data from 1961-01-01 to 2019-01-01
#MEHOINUSA672N has data from 1984-01-01 to 2018-01-01
df_yr
88/17:
'''
UNRATE data from 1948-01-01 to 2020-07-01
INTDSRUSM193N from 1950-01-01 to 2020-07-01
CUUR0000SEHA from 1914-12-01 to 2020-07-01
CSUSHPINSA from 1987-01-01 to 2020-06-01
HOUST from 1959-01-01 to 2020-07-01
WPUIP2311001 from 1986-06-01 to 2020-07-01
TLRESCONS from 2002-01-01 to 2020-06-01
'''
df_m.tail()
88/18:
#RRVRUSQ156N is from 1956-01-01 to 2020-04-01
df_q.tail()
88/19:
#convert DATE columns to datetime object
df_q['DATE']= pd.to_datetime(df_q['DATE'])
df_q.dtypes
88/20:
df_m['DATE']= pd.to_datetime(df_m['DATE'])
df_m.dtypes
88/21:
df_yr['DATE']= pd.to_datetime(df_yr['DATE'])
df_yr.dtypes
88/22:
#merge yearly df with quarterly: df2
df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill")
df2
88/23:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.iloc[853:]
88/24: df3.dtypes
88/25:
#replace '.'s with NaN
df3 = df3.replace(".", np.nan)
88/26:
#convert dtype objects to floats as needed
df3 = df3.astype({"MEHOINUSA672N": float, "UNRATE": float, "INTDSRUSM193N": float, "CUUR0000SEHA": float, "CSUSHPINSA": float, "HOUST": float, "WPUIP2311001": float, "TLRESCONS": float})
88/27: df3.dtypes
88/28:
#explore combined datas in df3 - stored in data definition table
df3.info()
88/29: df3.describe()
88/30: df3.nunique()
88/31: df3['RRVRUSQ156N'].value_counts(normalize=True)*100
88/32:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'uspop_growth', 'MEHOINUSA672N': 'med_hIncome', 'UNRATE': 'unemplt_rate', 'INTDSRUSM193N': 'int_rate', 'CUUR0000SEHA': 'cpi_rent', 'CSUSHPINSA': 'homePrice_index', 'HOUST': 'newHouse_starts', 'WPUIP2311001': 'ppi_resConstruct', 'RRVRUSQ156N': 'rentl_vacnyRate', 'TLRESCONS': 'resConstruct_spending'}, inplace = True)
df3.T
88/33: #deal with NAN
88/34:
#determine the outlier for NaNs
nonnull_counts = df3.apply(lambda x: x.count(), axis=1)
nonnull_counts = pd.DataFrame(nonnull_counts)
nonnull_counts
88/35:
#Call df3's `hist` method to plot histograms of each of the numeric features
df3.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
88/36: nonnull_counts.describe()
88/37:
#plot nonnull counts, seems like index 400-1100 is ballpark 25-75% 
nonnull_counts.plot()
plt.xlabel('index number')
plt.ylabel('non-null counts')
plt.show()
88/38:
nonnull_counts.plot.hist()
plt.xlabel('non-null counts')
88/39:
#to get within 1 standard deviation of non null data, need to use rows 421-1266 inclusive
nonnull_counts.iloc[420:]
88/40:
#the years outside 1 standard deviation of NaN values are pre 1950 and post 6/2020
df3.iloc[421:1267].T
88/41:
#vacany rate data starts at year 1956, so we can confidently drop rows without rental vacancy rate and get the majority of the data
#create another data frame that drops rows with no vacany rate data
missing_vacancy = df3['RENTL_VACNYRATE'].isnull()
df3_noVacant = df3[missing_vacancy == False]
df3_noVacant
88/42: df3.describe().T
88/43:
#vacany rate data starts at year 1956, so we can confidently drop rows without rental vacancy rate and get the majority of the data
#create another data frame that drops rows with no vacany rate data
missing_vacancy = df3['rentl_vacnyRate'].isnull()
df3_noVacant = df3[missing_vacancy == False]
df3_noVacant
88/44:
#vacany rate data starts at year 1956, so we can confidently drop rows without rental vacancy rate and get the majority of the data
#create another data frame that drops rows with no vacany rate data
missing_vacancy = df3['rentl_vacnyRate'].isnull()
df3_noVacant = df3[missing_vacancy == False]
df3_noVacant.T
88/45:
#check if there are any any NaN values left in Rental vacany rate. 
df3_noVacant['rentl_vacnyRate'].isnull().values.any()
89/1:
#Import pandas, matplotlib.pyplot, and seaborn
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import numpy as np
89/2:
#change directory to get FRED data
path= '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/raw/fredgraph'
os.chdir(path)
89/3: os.getcwd()
89/4: os.listdir()
89/5:
# load FRED data - current US national data
df_yr = pd.read_csv('Annual.csv')
df_m = pd.read_csv('Monthly.csv')
df_q = pd.read_csv('Quarterly.csv')
89/6:
#see info/describe/unique value counts to get a summary of the data
df_yr.info()
89/7: df_yr.describe()
89/8: df_yr.nunique()
89/9: df_yr['MEHOINUSA672N'].value_counts(normalize=True)*100
89/10: df_m.info()
89/11: df_m.describe()
89/12: df_m['TLRESCONS'].value_counts(normalize=True)*100
89/13: df_q.info()
89/14: df_q['RRVRUSQ156N'].value_counts(normalize=True)*100
89/15: df_q.describe()
89/16:
#Call the head/tail method to explore FRED data
#SPPOPGROWUSA has data from 1961-01-01 to 2019-01-01
#MEHOINUSA672N has data from 1984-01-01 to 2018-01-01
df_yr
89/17:
'''
UNRATE data from 1948-01-01 to 2020-07-01
INTDSRUSM193N from 1950-01-01 to 2020-07-01
CUUR0000SEHA from 1914-12-01 to 2020-07-01
CSUSHPINSA from 1987-01-01 to 2020-06-01
HOUST from 1959-01-01 to 2020-07-01
WPUIP2311001 from 1986-06-01 to 2020-07-01
TLRESCONS from 2002-01-01 to 2020-06-01
'''
df_m.tail()
89/18:
#RRVRUSQ156N is from 1956-01-01 to 2020-04-01
df_q.tail()
89/19:
#convert DATE columns to datetime object
df_q['DATE']= pd.to_datetime(df_q['DATE'])
df_q.dtypes
89/20:
df_m['DATE']= pd.to_datetime(df_m['DATE'])
df_m.dtypes
89/21:
df_yr['DATE']= pd.to_datetime(df_yr['DATE'])
df_yr.dtypes
89/22:
#merge yearly df with quarterly: df2
df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill")
df2
89/23:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.iloc[853:]
89/24: df3.dtypes
89/25:
#replace '.'s with NaN
df3 = df3.replace(".", np.nan)
89/26:
#convert dtype objects to floats as needed
df3 = df3.astype({"MEHOINUSA672N": float, "UNRATE": float, "INTDSRUSM193N": float, "CUUR0000SEHA": float, "CSUSHPINSA": float, "HOUST": float, "WPUIP2311001": float, "TLRESCONS": float})
89/27: df3.dtypes
89/28:
#explore combined datas in df3 - stored in data definition table
df3.info()
89/29: df3.describe().T
89/30: df3.nunique()
89/31: df3['RRVRUSQ156N'].value_counts(normalize=True)*100
89/32:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'uspop_growth', 'MEHOINUSA672N': 'med_hIncome', 'UNRATE': 'unemplt_rate', 'INTDSRUSM193N': 'int_rate', 'CUUR0000SEHA': 'cpi_rent', 'CSUSHPINSA': 'homePrice_index', 'HOUST': 'newHouse_starts', 'WPUIP2311001': 'ppi_resConstruct', 'RRVRUSQ156N': 'rentl_vacnyRate', 'TLRESCONS': 'resConstruct_spending'}, inplace = True)
df3.T
89/33: #deal with NAN
89/34:
#determine the outlier for NaNs
nonnull_counts = df3.apply(lambda x: x.count(), axis=1)
nonnull_counts = pd.DataFrame(nonnull_counts)
nonnull_counts
89/35: nonnull_counts.describe()
89/36:
#plot nonnull counts, seems like index 400-1100 is ballpark 25-75% 
nonnull_counts.plot()
plt.xlabel('index number')
plt.ylabel('non-null counts')
plt.show()
89/37:
nonnull_counts.plot.hist()
plt.xlabel('non-null counts')
89/38:
#to get within 1 standard deviation of non null data, need to use rows 421-1266 inclusive
nonnull_counts.iloc[420:]
89/39:
#the years outside 1 standard deviation of NaN values are pre 1950 and post 6/2020
df3.iloc[421:1267].T
89/40:
#vacany rate data starts at year 1956, so we can confidently drop rows without rental vacancy rate and get the majority of the data
#create another data frame that drops rows with no vacany rate data
missing_vacancy = df3['rentl_vacnyRate'].isnull()
df3_noVacant = df3[missing_vacancy == False]
df3_noVacant.T
89/41:
#check if there are any any NaN values left in Rental vacany rate. 
df3_noVacant['rentl_vacnyRate'].isnull().values.any()
89/42: df3_noVacant.describe().T
89/43: df3.describe().T
89/44:
#examine distribution of values, don't seem to be any extreme outliers
df3_noVacant.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
89/45: df3_noVacant.to_csv(r'/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/interim/df3_noVacant', index=False)
90/1:
#Import pandas, matplotlib.pyplot, and seaborn
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import numpy as np
90/2:
#change directory to get FRED data
path= '/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/raw/fredgraph'
os.chdir(path)
90/3: os.getcwd()
90/4: os.listdir()
90/5:
# load FRED data - current US national data
df_yr = pd.read_csv('Annual.csv')
df_m = pd.read_csv('Monthly.csv')
df_q = pd.read_csv('Quarterly.csv')
90/6:
#see info/describe/unique value counts to get a summary of the data
df_yr.info()
90/7: df_yr.describe()
90/8: df_yr.nunique()
90/9: df_yr['MEHOINUSA672N'].value_counts(normalize=True)*100
90/10: df_m.info()
90/11: df_m.describe()
90/12: df_m['TLRESCONS'].value_counts(normalize=True)*100
90/13: df_q.info()
90/14: df_q['RRVRUSQ156N'].value_counts(normalize=True)*100
90/15: df_q.describe()
90/16:
#Call the head/tail method to explore FRED data
#SPPOPGROWUSA has data from 1961-01-01 to 2019-01-01
#MEHOINUSA672N has data from 1984-01-01 to 2018-01-01
df_yr
90/17:
'''
UNRATE data from 1948-01-01 to 2020-07-01
INTDSRUSM193N from 1950-01-01 to 2020-07-01
CUUR0000SEHA from 1914-12-01 to 2020-07-01
CSUSHPINSA from 1987-01-01 to 2020-06-01
HOUST from 1959-01-01 to 2020-07-01
WPUIP2311001 from 1986-06-01 to 2020-07-01
TLRESCONS from 2002-01-01 to 2020-06-01
'''
df_m.tail()
90/18:
#RRVRUSQ156N is from 1956-01-01 to 2020-04-01
df_q.tail()
90/19:
#convert DATE columns to datetime object
df_q['DATE']= pd.to_datetime(df_q['DATE'])
df_q.dtypes
90/20:
df_m['DATE']= pd.to_datetime(df_m['DATE'])
df_m.dtypes
90/21:
df_yr['DATE']= pd.to_datetime(df_yr['DATE'])
df_yr.dtypes
90/22:
#merge yearly df with quarterly: df2
df2 = pd.merge_ordered(df_yr, df_q, fill_method="ffill")
df2
90/23:
#merge df2 with monthly df: df3
df3 = pd.merge_ordered(df2, df_m, fill_method="ffill")
df3.iloc[853:]
90/24: df3.dtypes
90/25:
#replace '.'s with NaN
df3 = df3.replace(".", np.nan)
90/26:
#convert dtype objects to floats as needed
df3 = df3.astype({"MEHOINUSA672N": float, "UNRATE": float, "INTDSRUSM193N": float, "CUUR0000SEHA": float, "CSUSHPINSA": float, "HOUST": float, "WPUIP2311001": float, "TLRESCONS": float})
90/27: df3.dtypes
90/28:
#explore combined datas in df3 - stored in data definition table
df3.info()
90/29: df3.describe().T
90/30: df3.nunique()
90/31: df3['RRVRUSQ156N'].value_counts(normalize=True)*100
90/32:
#change column headers to make more user friendly
df3.rename(columns = {'SPPOPGROWUSA':'uspop_growth', 'MEHOINUSA672N': 'med_hIncome', 'UNRATE': 'unemplt_rate', 'INTDSRUSM193N': 'int_rate', 'CUUR0000SEHA': 'cpi_rent', 'CSUSHPINSA': 'homePrice_index', 'HOUST': 'newHouse_starts', 'WPUIP2311001': 'ppi_resConstruct', 'RRVRUSQ156N': 'rentl_vacnyRate', 'TLRESCONS': 'resConstruct_spending'}, inplace = True)
df3.T
90/33: #deal with NAN
90/34:
#determine the outlier for NaNs
nonnull_counts = df3.apply(lambda x: x.count(), axis=1)
nonnull_counts = pd.DataFrame(nonnull_counts)
nonnull_counts
90/35: nonnull_counts.describe()
90/36:
#plot nonnull counts, seems like index 400-1100 is ballpark 25-75% 
nonnull_counts.plot()
plt.xlabel('index number')
plt.ylabel('non-null counts')
plt.show()
90/37:
nonnull_counts.plot.hist()
plt.xlabel('non-null counts')
90/38:
#to get within 1 standard deviation of non null data, need to use rows 421-1266 inclusive
nonnull_counts.iloc[420:]
90/39:
#the years outside 1 standard deviation of NaN values are pre 1950 and post 6/2020
df3.iloc[421:1267].T
90/40:
#vacany rate data starts at year 1956, so we can confidently drop rows without rental vacancy rate and get the majority of the data
#create another data frame that drops rows with no vacany rate data
missing_vacancy = df3['rentl_vacnyRate'].isnull()
df3_1956 = df3[missing_vacancy == False]
df3_1956.T
90/41:
#check if there are any any NaN values left in Rental vacany rate. 
df3_1956['rentl_vacnyRate'].isnull().values.any()
90/42: df3_1956.describe().T
90/43: df3.describe().T
90/44:
#examine distribution of values, don't seem to be any extreme outliers
df3_1956.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
90/45: df3_1956.to_csv(r'/Users/josephfrasca/Coding_Stuff/Springboard/Capstone_2/data/interim/df3_1956', index=False)
90/46:
#check if there are any any NaN values left in Rental vacany rate. 
df3_1956['rentl_vacnyRate'].isnull().values.any()
90/47: df3_1956.isna()
90/48: df3_1956.isna().count()
90/49: df3_1956.isnull().count()
90/50: df3_1956.notnull().count()
90/51: df3_1956.len()
90/52: len(df3_1956)
90/53: len(df3_1956)
90/54: pd.notnull(df3_1956)
90/55: pd.notnull(df3_1956).count()
90/56: pd.isnull(df3_1956).count()
90/57: pd.isnull(df3_1956).count()
90/58: pd.isnull(df3_1956).count()
90/59: df3_1956.count()
90/60: df3_1956.count()/len(df3_1956)*100
90/61: sum(df3_1956.count())/len(df3_1956)*100
90/62: sum(df3_1956.count())/sum(len(df3_1956))*100
90/63: df3_1956.count()/len(df3_1956)*100
90/64: sum(df3_1956.count())
90/65: sum(df3_1956.count())
90/66: sum(df3_1956.count())
90/67: df3_1956.isna()
90/68: df3_1956.isna().sum()
90/69: df3_1956.isna().sum()/len(df3_1956)*100
90/70: sum(df3_1956.count())
90/71: sum(df3_1956.count())
90/72: sum(df3_1956.isna().sum())
90/73: sum(df3_1956.isna().sum())/sum(df3_1956.count())
90/74: sum(df3_1956.isna().sum())/sum(df3_1956.count())*100
90/75:
#examine distribution of values, don't seem to be any extreme outliers
df3_1956.hist(figsize=(15,10))
plt.subplots_adjust(hspace=0.5);
90/76:
Q1 = df3_1956.quantile(0.25)
Q3 = df3_1956.quantile(0.75)
IQR = Q3 - Q1
90/77:
Q1 = df3_1956.quantile(0.25)
Q3 = df3_1956.quantile(0.75)
IQR = Q3 - Q1
90/78: Q1
90/79: Q3
90/80: IQR
90/81: ((df3_1956 < (Q1 - 1.5 * IQR)) | (df3_1956 > (Q3 + 1.5 * IQR))).sum()
90/82:
Q1 = df3_1956.quantile(0.25)
Q3 = df3_1956.quantile(0.75)
IQR = Q3 - Q1
90/83: Q1
92/1:
# import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        SELECT *
        FROM FACILITIES
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite\db\pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/2:
# import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        SELECT *
        FROM FACILITIES
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite\db\pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/3: os.cd
92/4: os.getcwd()
92/5:
import os
os.getcwd()
92/6:
# import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        SELECT *
        FROM FACILITIES
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite\db\pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/7:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        SELECT *
        FROM FACILITIES
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite\db\pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/8:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        SELECT *
        FROM Facilities
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite\db\pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/9:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        SELECT *
        FROM Facilities
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite\db\pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/10:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        SELECT *
        FROM 'Facilities'
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite\db\pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/11:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = 
        SELECT *
        FROM 'Facilities'
        
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite\db\pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/12:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = 
        SELECT *
        FROM Facilities
        
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite\db\pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/13:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        SELECT *
        FROM Facilities
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite\db\pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/14:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        SELECT *
        FROM Facilities
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite\db\pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/15:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        SELECT *
        FROM Facilities
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/16:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        WITH full_name AS (
            SELECT
                CONCAT(firstname, ' ', surname)
                FROM Members)

        SELECT *
        FROM full_name
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/17:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        WITH full_name AS (
            SELECT
                (firstname || surname)
                FROM Members)

        SELECT *
        FROM full_name
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/18:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        WITH full_name AS (
            SELECT
                (firstname ||''|| surname)
                FROM Members)

        SELECT *
        FROM full_name
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/19:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        WITH full_name AS (
            SELECT
                (firstname ||''|| surname)
                FROM Members)

        SELECT *
        FROM full_name
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/20:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
        WITH full_name AS (
            SELECT
                (firstname ||' '|| surname)
                FROM Members)

        SELECT *
        FROM full_name
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/21:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g_rev AS (
    SELECT bookid, memid, slots, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0

SELECT *
FROM g_rev;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/22:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g_rev AS (
    SELECT bookid, memid, slots, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0)

SELECT *
FROM g_rev;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/23:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g_rev AS (
    SELECT bookid, memid, slots, guestcost, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0)

SELECT *
FROM g_rev;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/24:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g_rev AS (
    SELECT facid, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0)

SELECT *
FROM g_rev;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/25:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH guest_rev AS (
    SELECT facid, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0)
    
member_rev AS (
    SELECT facid, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0)

SELECT *
FROM member_rev;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/26:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH guest_rev AS (
    SELECT facid, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
member_rev AS (
    SELECT facid, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0
)

SELECT *
FROM member_rev;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/27:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0
)

SELECT *
FROM member_rev;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/28:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0
)

SELECT *
FROM m;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/29:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    facility_name,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/30:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/31:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    g.revenue,
    m.revenue,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/32:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    facid,
    g.revenue,
    m.revenue,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/33:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    facid,
    facility_name,
    g.revenue,
    m.revenue,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/34:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    facid,
    facility_name,
    g.revenue,
    m.revenue,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid, facility_name);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/35:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    bookid,
    facid,
    facility_name,
    g.revenue,
    m.revenue,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid, facility_name);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/36:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    bookid,
    facid,
    facility_name,
    g.revenue,
    m.revenue,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid, facility_name)
GROUP BY bookid;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/37:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    bookid,
    facid,
    facility_name,
    g.revenue,
    m.revenue,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (bookid, facid, facility_name)
GROUP BY bookid;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/38:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT bookid, facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    bookid,
    facid,
    facility_name,
    g.revenue,
    m.revenue,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (bookid, facid, facility_name)
GROUP BY bookid;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/39:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT bookid, facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    bookid,
    facid,
    facility_name,
    g.revenue,
    m.revenue,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid, facility_name)
GROUP BY bookid;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/40:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT bookid, facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    facid,
    facility_name,
    g.revenue,
    m.revenue,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid, facility_name)
GROUP BY bookid;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/41:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    bookid,
    facid,
    facility_name,
    g.revenue,
    m.revenue,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid, facility_name)
GROUP BY bookid;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/42:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    bookid,
    facid,
    facility_name,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid, facility_name)
GROUP BY bookid;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/43:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    bookid,
    facid,
    facility_name,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid, facility_name)
ORDER BY bookid;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/44:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    bookid,
    facid,
    facility_name,
    g.revenue + m.revenue AS total_revenue
    
FROM g
JOIN m
USING (facid, facility_name)
GROUP BY bookid;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/45:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    SUM(total_revenue)
FROM (
    SELECT
        bookid,
        facid,
        facility_name,
        g.revenue + m.revenue AS total_revenue
    
    FROM g
    JOIN m
    USING (facid, facility_name)
    GROUP BY bookid;
)
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/46:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    SUM(total_revenue)
FROM (
    SELECT
        bookid,
        facid,
        facility_name,
        g.revenue + m.revenue AS total_revenue
    
    FROM g
    JOIN m
    USING (facid, facility_name)
    GROUP BY bookid
);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/47:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    facility_name,
    SUM(total_revenue)
FROM (
    SELECT
        bookid,
        facid,
        facility_name,
        g.revenue + m.revenue AS total_revenue
    
    FROM g
    JOIN m
    USING (facid, facility_name)
    GROUP BY bookid
);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/48:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    DISTINCT facility_name,
    SUM(total_revenue)
FROM (
    SELECT
        bookid,
        facid,
        facility_name,
        g.revenue + m.revenue AS total_revenue
    
    FROM g
    JOIN m
    USING (facid, facility_name)
    GROUP BY bookid
);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/49:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    DISTINCT facility_name,
    SUM(total_revenue)
FROM (
    SELECT
        bookid,
        facid,
        facility_name,
        g.revenue + m.revenue AS total_revenue
    
    FROM g
    JOIN m
    USING (facid, facility_name)
    GROUP BY bookid
)
WHERE total_revenue < 1000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/50:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    DISTINCT facility_name,
    SUM(total_revenue)
FROM (
    SELECT
        bookid,
        facid,
        facility_name,
        g.revenue + m.revenue AS total_revenue
    
    FROM g
    JOIN m
    USING (facid, facility_name)
    GROUP BY bookid
)
WHERE total_revenue < 1000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/51:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    DISTINCT facility_name,
    SUM(total_revenue) AS sum
FROM (
    SELECT
        bookid,
        facid,
        facility_name,
        g.revenue + m.revenue AS total_revenue
    
    FROM g
    JOIN m
    USING (facid, facility_name)
    GROUP BY bookid
)
WHERE sum < 1000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/52:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    DISTINCT facility_name,
    SUM(total_revenue) AS sm
FROM (
    SELECT
        bookid,
        facid,
        facility_name,
        g.revenue + m.revenue AS total_revenue
    
    FROM g
    JOIN m
    USING (facid, facility_name)
    GROUP BY bookid
)
WHERE sm < 1000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/53:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    DISTINCT facility_name,
    SUM(total_revenue) AS sm
FROM (
    SELECT
        bookid,
        facid,
        facility_name,
        g.revenue + m.revenue AS total_revenue
    
    FROM g
    JOIN m
    USING (facid, facility_name)
    GROUP BY bookid
)
WHERE sm < 1000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/54:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    DISTINCT facility_name,
    SUM(total_revenue)
FROM (
    SELECT
        bookid,
        facid,
        facility_name,
        g.revenue + m.revenue AS total_revenue
    
    FROM g
    JOIN m
    USING (facid, facility_name)
    GROUP BY bookid
)
WHERE SUM(total_revenue) < 1000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/55:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    DISTINCT facility_name,
    SUM(total_revenue)
FROM (
    SELECT
        bookid,
        facid,
        facility_name,
        g.revenue + m.revenue AS total_revenue
    
    FROM g
    JOIN m
    USING (facid, facility_name)
    GROUP BY bookid
)
WHERE SUM(total_revenue) < 1000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/56:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH g AS (
    SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0),
    
m AS (
    SELECT facid, name AS facility_name, membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
)

SELECT
    DISTINCT facility_name,
    SUM(total_revenue)
FROM (
    SELECT
        bookid,
        facid,
        facility_name,
        g.revenue + m.revenue AS total_revenue
    
    FROM g
    JOIN m
    USING (facid, facility_name)
    GROUP BY bookid
)
--WHERE SUM(total_revenue) < 1000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/57:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT m2.memid, recommendedby
FROM (
    SELECT 
        memid,
        surname,
        firstname
    FROM Members) AS m1
JOIN Members AS m2
ON m1.memid = m2.recommendedby;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/58:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT *
FROM (
    SELECT 
        memid,
        surname,
        firstname
    FROM Members) AS m1
JOIN Members AS m2
ON m1.memid = m2.recommendedby;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/59:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT surname, firstname, recommendedby
FROM (
    SELECT 
        memid,
        surname,
        firstname
    FROM Members) AS m1
JOIN Members AS m2
ON m1.memid = m2.recommendedby;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/60:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT m1.surname, m1.firstname, recommendedby
FROM (
    SELECT 
        memid,
        surname,
        firstname
    FROM Members) AS m1
JOIN Members AS m2
ON m1.memid = m2.recommendedby;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/61:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT m2.surname, m2.firstname, recommendedby
FROM (
    SELECT 
        memid,
        surname,
        firstname
    FROM Members) AS m1
JOIN Members AS m2
ON m1.memid = m2.recommendedby;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/62:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/63:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT bookid, facid, name AS facility_name, SUM(guestcost * slots) AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/64:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT bookid, facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/65:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0
    GROUP BY facility_name;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/66:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/67:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT facid, DISTINCT name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/68:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT facid, name AS facility_name, guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/69:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT facid, name AS facility_name, guestcost, slots
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/70:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT facid, DISTINCT name AS facility_name, guestcost, slots
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/71:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT DISTINCT name AS facility_name, guestcost, slots
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/72:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT DISTINCT facid, name AS facility_name, guestcost, slots
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/73:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT facid, name AS facility_name, guestcost, slots
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/74:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT facid, name AS facility_name, SUM(guestcost), slots
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/75:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT facid, name AS facility_name, SUM(guestcost), SUM(slots)
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/76:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT  name AS facility_name, SUM(guestcost), SUM(slots)
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0
    AND facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/77:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT  name AS facility_name, SUM(guestcost), SUM(slots)
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0
    AND facid = 0 OR facid=1;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/78:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT  name AS facility_name, SUM(guestcost), SUM(slots)
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid = 0
    AND facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/79:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT  name AS facility_name, SUM(membercost), SUM(slots)
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE memid > 0
    AND facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/80:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility_name, 
    CASE WHEN memid = 0 THEN guestcost * slots
    ELSE membercost * slots END AS cost
FROM Bookings
JOIN Facilities
USING (facid);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/81:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility_name, 
    CASE WHEN memid = 0 THEN guestcost * slots
    ELSE membercost * slots END AS cost
FROM Bookings
JOIN Facilities
USING (facid)
WHERE facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/82:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility_name, 
    CASE WHEN memid = 0 THEN guestcost * slots
    ELSE membercost * slots END AS cost,
    SUM(cost)
FROM Bookings
JOIN Facilities
USING (facid)
WHERE facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/83:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT SUM(cost)
FROM (
    SELECT
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS cost
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
WHERE facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/84:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT SUM(cost)
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS cost
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
WHERE facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/85:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(cost)
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS cost
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
WHERE facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/86:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue)
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
WHERE facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/87:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue)
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    );
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/88:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue)
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
WHERE facid=0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/89:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue)
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
WHERE facid=0 OR facid=1;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/90:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue)
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
WHERE facid=1;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/91:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue) AS total_revenue
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
WHERE facid=1;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/92:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue) AS total_revenue
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
HAVING total_revenue > 10000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/93:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue) AS total_revenue
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
GROUP BY facility_name
HAVING total_revenue > 10000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/94:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue) AS total_revenue
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
GROUP BY facility_name
HAVING total_revenue < 1000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/95:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue) AS total_revenue
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
GROUP BY facility_name
HAVING total_revenue < 1000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/96:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue) AS total_revenue
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/97:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue) AS total_revenue
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/98:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue) AS total_revenue
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
GROUP BY facility_name
HAVING total_revenue < 1000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/99:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue) AS total_revenue
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
GROUP BY facility_name
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/100:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue) AS total_revenue
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
GROUP BY facility_name
HAVING total_revenue < 1000;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/101:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT 
    facility_name,
    SUM(revenue) AS total_revenue
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
GROUP BY facility_name
HAVING total_revenue < 1000
ORDER BY total_revenue;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/102:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE facid = 3;
 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/103:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        facid,
        name AS facility_name, 
        SUM(membercost * slots)
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE facid = 3 AND
    WHERE memid > 0;
 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/104:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        facid,
        name AS facility_name, 
        SUM(membercost * slots)
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE facid = 3 AND
    memid > 0;
 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/105:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        facid,
        name AS facility_name, 
        SUM(membercost * slots) AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE facid = 3 AND
    memid > 0;
 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/106:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        facid,
        name AS facility_name, 
        membercost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE facid = 3 AND
    memid > 0;
 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/107:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        facid,
        name AS facility_name, 
        guestcost * slots AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE facid = 3 AND
    memid = 0;
 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/108:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        facid,
        name AS facility_name, 
        SUM(guestcost * slots) AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    WHERE facid = 3 AND
    memid = 0;
 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/109:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT 
    facility_name,
    SUM(revenue) AS total_revenue
FROM (
    SELECT
        facid,
        name AS facility_name, 
        CASE WHEN memid = 0 THEN guestcost * slots
        ELSE membercost * slots END AS revenue
    FROM Bookings
    JOIN Facilities
    USING (facid)
    )
GROUP BY facility_name
HAVING total_revenue < 1000
ORDER BY total_revenue;
 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/110:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


SELECT m2.surname, m2.firstname, recommendedby
FROM (
    SELECT 
        memid,
        surname,
        firstname
    FROM Members) AS m1
JOIN Members AS m2
ON m1.memid = m2.recommendedby;
 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/111:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


SELECT m2.surname, m2.firstname, recommendedby
FROM (
    SELECT 
        memid,
        surname,
        firstname
    FROM Members) AS m1
JOIN Members AS m2
ON m1.memid = m2.recommendedby;
 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/112:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


WITH r AS (
    SELECT
        recommendedby
     FROM Members)

SELECT r.recommendedby, surname, firstname
FROM r
JOIN Members AS m
ON r.recommendedby = m.memid
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/113:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


WITH r AS (
    SELECT
        recommendedby
     FROM Members)

SELECT r.recommendedby, (surname, ' ', firstname) AS recommended_by
FROM r
JOIN Members AS m
ON r.recommendedby = m.memid
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/114:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


WITH r AS (
    SELECT
        recommendedby
     FROM Members)

SELECT r.recommendedby, CONCAT(surname, ' ', firstname) AS recommended_by
FROM r
JOIN Members AS m
ON r.recommendedby = m.memid
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/115:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


WITH r AS (
    SELECT
        recommendedby
     FROM Members)

SELECT r.recommendedby, (surnam||' '|| firstname) AS recommended_by
FROM r
JOIN Members AS m
ON r.recommendedby = m.memid
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/116:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


WITH r AS (
    SELECT
        recommendedby
     FROM Members)

SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
FROM r
JOIN Members AS m
ON r.recommendedby = m.memid
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/117:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT m2.surname, m2.firstname, recommendedby
FROM (
    SELECT 
        memid,
        surname,
        firstname
    FROM Members) AS m1
JOIN Members AS m2
ON m1.memid = m2.recommendedby;

    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/118:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
SELECT m2.surname, m2.firstname, recommendedby
FROM (
    SELECT 
        memid,
        surname,
        firstname
    FROM Members) AS m1
JOIN Members AS m2
ON m1.memid = m2.recommendedby;

        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/119:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
WITH r AS (
    SELECT
        recommendedby
     FROM Members)

SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
FROM r
JOIN Members AS m
ON r.recommendedby = m.memid
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/120:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
SELECT, m2.surname, m2.firstname, recommended_by

FROM (
    SELECT m2.surname, m2.firstname, recommendedby
    FROM (
        SELECT 
            memid,
            surname,
            firstname
        FROM Members) AS m1
    JOIN Members AS m2
    ON m1.memid = m2.recommendedby) AS s1

JOIN (
    WITH r AS (
        SELECT
            recommendedby
         FROM Members)

    SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
    FROM r
    JOIN Members AS m
    ON r.recommendedby = m.memid) AS s2

ON s1.recommendedby = s2. recommended_by 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/121:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
SELECT 
    s1.surname, 
    s1.firstname, 
    s2.recommended_by

FROM (
    SELECT m2.surname, m2.firstname, recommendedby
    FROM (
        SELECT 
            memid,
            surname,
            firstname
        FROM Members) AS m1
    JOIN Members AS m2
    ON m1.memid = m2.recommendedby) AS s1

JOIN (
    WITH r AS (
        SELECT
            recommendedby
         FROM Members)

    SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
    FROM r
    JOIN Members AS m
    ON r.recommendedby = m.memid) AS s2

ON s1.recommendedby = s2. recommended_by 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/122:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
SELECT 
    s1.surname, 
    s1.firstname, 
    s2.recommended_by

FROM (
    SELECT m2.surname, m2.firstname, recommendedby
    FROM (
        SELECT 
            memid,
            surname,
            firstname
        FROM Members) AS m1
    JOIN Members AS m2
    ON m1.memid = m2.recommendedby) AS s1

JOIN (
    WITH r AS (
        SELECT
            recommendedby
         FROM Members)

    SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
    FROM r
    JOIN Members AS m
    ON r.recommendedby = m.memid) AS s2

ON s1.recommendedby = s2. recommended_by 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/123:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
SELECT 
    m2.surname, 
    m2.firstname, 
    recommended_by

FROM (
    SELECT m2.surname, m2.firstname, recommendedby
    FROM (
        SELECT 
            memid,
            surname,
            firstname
        FROM Members) AS m1
    JOIN Members AS m2
    ON m1.memid = m2.recommendedby) AS s1

JOIN (
    WITH r AS (
        SELECT
            recommendedby
         FROM Members)

    SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
    FROM r
    JOIN Members AS m
    ON r.recommendedby = m.memid) AS s2

ON s1.recommendedby = s2. recommended_by 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/124:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
SELECT 
    surname, 
    firstname, 
    recommended_by

FROM (
    SELECT m2.surname, m2.firstname, recommendedby
    FROM (
        SELECT 
            memid,
            surname,
            firstname
        FROM Members) AS m1
    JOIN Members AS m2
    ON m1.memid = m2.recommendedby) AS s1

JOIN (
    WITH r AS (
        SELECT
            recommendedby
         FROM Members)

    SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
    FROM r
    JOIN Members AS m
    ON r.recommendedby = m.memid) AS s2

ON s1.recommendedby = s2. recommended_by 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/125:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
SELECT 
    s1.surname, 
    s1.firstname, 
    s2.recommended_by

FROM (
    SELECT m2.surname, m2.firstname, recommendedby
    FROM (
        SELECT 
            memid,
            surname,
            firstname
        FROM Members) AS m1
    JOIN Members AS m2
    ON m1.memid = m2.recommendedby) AS s1

JOIN (
    WITH r AS (
        SELECT
            recommendedby
         FROM Members)

    SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
    FROM r
    JOIN Members AS m
    ON r.recommendedby = m.memid) AS s2

ON s1.recommendedby = s2. recommended_by 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/126:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
SELECT 
    s1.surname, 
    s1.firstname, 
    s2.recommended_by

FROM (
    SELECT m2.surname, m2.firstname, recommendedby
    FROM (
        SELECT 
            memid,
            surname,
            firstname
        FROM Members) AS m1
    JOIN Members AS m2
    ON m1.memid = m2.recommendedby) AS s1

JOIN (
    WITH r AS (
        SELECT
            recommendedby
         FROM Members)

    SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
    FROM r
    JOIN Members AS m
    ON r.recommendedby = m.memid) AS s2

ON s1.recommendedby = s2. recommended_by; 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/127:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
SELECT 
    surname, 
    firstname, 
    recommended_by

FROM (
    SELECT m2.surname, m2.firstname, recommendedby
    FROM (
        SELECT 
            memid,
            surname,
            firstname
        FROM Members) AS m1
    JOIN Members AS m2
    ON m1.memid = m2.recommendedby) AS s1

JOIN (
    WITH r AS (
        SELECT
            recommendedby
         FROM Members)

    SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
    FROM r
    JOIN Members AS m
    ON r.recommendedby = m.memid) AS s2

ON s1.recommendedby = s2. recommended_by; 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/128:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
SELECT 
    s1.surname, 
    s1.firstname, 
    s2.recommended_by

FROM (
    SELECT m2.surname, m2.firstname, recommendedby
    FROM (
        SELECT 
            memid,
            surname,
            firstname
        FROM Members) AS m1
    JOIN Members AS m2
    ON m1.memid = m2.recommendedby) AS s1

JOIN (
    WITH r AS (
        SELECT
            recommendedby
         FROM Members)

    SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
    FROM r
    JOIN Members AS m
    ON r.recommendedby = m.memid) AS s2

ON s1.recommendedby = s2. recommended_by; 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/129:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """
SELECT 
    s1.surname, 
    s1.firstname, 
    s2.recommended_by

FROM (
    SELECT m2.surname, m2.firstname, recommendedby
    FROM (
        SELECT 
            memid,
            surname,
            firstname
        FROM Members) AS m1
    JOIN Members AS m2
    ON m1.memid = m2.recommendedby) AS s1

JOIN (
    SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
    FROM (
        SELECT
            recommendedby
         FROM Members) AS r
    JOIN Members AS m
    ON r.recommendedby = m.memid) AS s2

ON s1.recommendedby = s2. recommended_by; 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/130:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

    SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
    FROM (
        SELECT
            recommendedby
         FROM Members) AS r
    JOIN Members AS m
    ON r.recommendedby = m.memid) AS s2; 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/131:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

    SELECT r.recommendedby, (surname||' '|| firstname) AS recommended_by
    FROM (
        SELECT
            recommendedby
         FROM Members) AS r
    JOIN Members AS m
    ON r.recommendedby = m.memid; 
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/132:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    SELECT
    facid
    name AS facility
    COUNT(bookid)
FROM Facilities
JOIN Bookings
USING (facid);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/133:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

    SELECT
    facid
    name AS facility
    COUNT(bookid)
FROM Facilities
JOIN Bookings
USING (facid);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/134:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facid,
    name AS facility,
    COUNT(bookid)
FROM Facilities
JOIN Bookings
USING (facid);
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/135:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facid,
    name AS facility,
    COUNT(bookid)
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE b.facid=m.facid;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/136:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facid,
    name AS facility,
    COUNT(bookid)
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE b.facid=f.facid;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/137:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facid,
    name AS facility,
    COUNT(bookid)
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/138:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facid,
    name AS facility,
    COUNT(*)
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/139:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facid,
    name AS facility,
    COUNT(bookid)
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/140:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facid,
    COUNT(*)
JOIN Bookings AS b
WHERE facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/141:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facid,
    COUNT(*)
FROM Bookings AS b
WHERE facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/142:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facid,
    COUNT(*)
FROM Bookings AS b
WHERE facid = 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/143:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facid,
    name AS facility,
    COUNT(bookid)
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
GROUP BY facility;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/144:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facid,
    name AS facility,
    COUNT(bookid)
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0
GROUP BY facility;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/145:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility,
    COUNT(bookid) AS usage_by_member
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0
GROUP BY facility;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/146:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility,
    strftime('%m', starttime) AS "month"
    COUNT(strftime('%m', start.) AS "month") AS usage_by_month
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0
GROUP BY month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/147:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility,
    strftime('%m', starttime) AS "month"
    COUNT(strftime('%m', starttime)) AS usage_by_month
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0
GROUP BY month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/148:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility,
    strftime('%m', starttime) AS "month"
    COUNT(SELECT strftime('%m', starttime) FROM Bookings) AS usage_by_month
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0
GROUP BY month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/149:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility,
    strftime('%m', starttime) AS "month",
    COUNT(SELECT strftime('%m', starttime) FROM Bookings) AS usage_by_month
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0
GROUP BY month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/150:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility,
    strftime('%m', starttime) AS "month",
    COUNT(SELECT strftime('%m', starttime)) AS usage_by_month
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0
GROUP BY month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/151:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility,
    strftime('%m', starttime) AS "month",
    COUNT(strftime('%m', starttime)) AS usage_by_month
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0
GROUP BY month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/152:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility,
    strftime('%m', starttime) AS "month",
    COUNT(strftime('%m', starttime)) AS usage_by_month
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0
GROUP BY facility
ORDER BY month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/153:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility,
    strftime('%m', starttime) AS "month",
    COUNT(strftime('%m', starttime)) AS usage_by_month
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0
GROUP BY facility
ORDER BY month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/154:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility,
    strftime('%m', starttime) AS "month",
    COUNT(strftime('%m', starttime)) AS usage_by_month
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0
GROUP BY month
ORDER BY facility;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/155:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility,
    strftime('%m', starttime) AS "month",
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/156:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    name AS facility,
    strftime('%m', starttime) AS "month"
FROM Facilities AS f
JOIN Bookings AS b
USING (facid)
WHERE memid > 0;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/157:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    faciilty
    month
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS "month"
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/158:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    faciilty
    month
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS "month"
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
    
GROUP BY month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/159:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    faciilty
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS "month"
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/160:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS "month"
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/161:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/162:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS "month"
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/163:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS "month"
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/164:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/165:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/166:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(month)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/167:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(facility)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/168:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY month

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/169:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(*) AS usage_per_month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY 

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/170:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(*) AS usage_per_month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY usage_per_month

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/171:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(*) AS usage_per_month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/172:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(month) AS usage_per_month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/173:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(facility) AS usage_per_month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/174:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    DISTINCT COUNT(facility) AS usage_per_month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/175:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    DISTINCT COUNT(*) AS usage_per_month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/176:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    DISTINCT facility,
    month,
    COUNT(*) AS usage_per_month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/177:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    DISTINCT facility,
    month,
    COUNT(*) AS usage_per_month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/178:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    DISTINCT facility,
    month,
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/179:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    DISTINCT facility,
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/180:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    DISTINCT month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/181:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/182:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub

;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/183:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/184:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/185:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/186:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    month,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/187:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    month,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/188:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    month,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY month
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/189:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    month,
    facility,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY month
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/190:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    month,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY month
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/191:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/192:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    COUNT(*),
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/193:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    COUNT(*),
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility
ORDER BY month
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/194:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    COUNT(*),
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility
ORDER BY month
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/195:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    COUNT(*),
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY month
ORDER BY facility
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/196:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    bookid,
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/197:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    bookid,
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/198:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/199:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    DISTINCT facility,
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/200:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        DISTINCT name AS facility,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/201:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        DISTINCT name AS facility,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0
    ORDER BY facility
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/202:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        (DISTINCT name) AS facility,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0
    ORDER BY facility
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/203:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        DISTINCT name,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0
    ORDER BY facility
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/204:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        DISTINCT name,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0
    ORDER BY name
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/205:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        DISTINCT name,
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0
    ORDER BY name
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/206:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """


    SELECT
        DISTINCT name
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0
    ORDER BY name
;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/207:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility, month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
92/208:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility, month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
93/1:
Use <database>
SELECT execquery.last_execution_time AS [Date Time], execsql.text AS [Script] FROM sys.dm_exec_query_stats AS execquery
CROSS APPLY sys.dm_exec_sql_text(execquery.sql_handle) AS execsql
ORDER BY execquery.last_execution_time DESC
   1:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility, month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
   2:
{
  "kernelspec": {
    "name": "python3",
    "display_name": "Python 3",
    "language": "python"
  },
  "language_info": {
    "name": "python",
    "version": "3.7.6",
    "mimetype": "text/x-python",
    "codemirror_mode": {
      "name": "ipython",
      "version": 3
    },
    "pygments_lexer": "ipython3",
    "nbconvert_exporter": "python",
    "file_extension": ".py"
  }
}
   3:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility, month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
   4:
import sqlite3
from sqlite3 import Error
 
def create_connection(db_file):
    """ create a database connection to the SQLite database
        specified by the db_file
    :param db_file: database file
    :return: Connection object or None
    """
    conn = None
    try:
        conn = sqlite3.connect(db_file)
        print(sqlite3.version)
    except Error as e:
        print(e)
 
    return conn
 
def select_all_tasks(conn):
    """
    Query all rows in the tasks table
    :param conn: the Connection object
    :return:
    """
    cur = conn.cursor()
    
    query1 = """

SELECT
    facility,
    month,
    COUNT(*)
FROM (
    SELECT
        name AS facility,
        strftime('%m', starttime) AS month,
        bookid
    FROM Facilities AS f
    JOIN Bookings AS b
    USING (facid)
    WHERE memid > 0) AS sub
GROUP BY facility, month;
        """
    cur.execute(query1)
 
    rows = cur.fetchall()
 
    for row in rows:
        print(row)


def main():
    database = "sqlite_db_pythonsqlite.db"
 
    # create a database connection
    conn = create_connection(database)
    with conn: 
        print("2. Query all tasks")
        select_all_tasks(conn)
 
 
if __name__ == '__main__':
    main()
   5: %history
   6: %history -g -f filename
